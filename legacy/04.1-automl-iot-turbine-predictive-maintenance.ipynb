{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "fff79d53-904b-42c3-96ef-1e22969cbd3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data Science with Databricks\n",
    "\n",
    "## ML is key to wind turbine farm optimization\n",
    "\n",
    "The current market makes energy even more strategic than before. Being able to ingest and analyze our Wind turbine state is a first step, but this isn't enough to thrive in a very competitive market.\n",
    "\n",
    "We need to go further to optimize our energy production, reduce maintenance cost and reduce downtime. Modern data company achieve this with AI.\n",
    "\n",
    "<style>\n",
    ".right_box{\n",
    "  margin: 30px; box-shadow: 10px -10px #CCC; width:650px;height:300px; background-color: #1b3139ff; box-shadow:  0 0 10px  rgba(0,0,0,0.6);\n",
    "  border-radius:25px;font-size: 35px; float: left; padding: 20px; color: #f9f7f4; }\n",
    ".badge {\n",
    "  clear: left; float: left; height: 30px; width: 30px;  display: table-cell; vertical-align: middle; border-radius: 50%; background: #fcba33ff; text-align: center; color: white; margin-right: 10px}\n",
    ".badge_b { \n",
    "  height: 35px}\n",
    "</style>\n",
    "<link href='https://fonts.googleapis.com/css?family=DM Sans' rel='stylesheet'>\n",
    "<div style=\"font-family: 'DM Sans'; display: flex; align-items: flex-start;\">\n",
    "  <!-- Left Section -->\n",
    "  <div style=\"width: 50%; color: #1b3139; padding-right: 20px;\">\n",
    "    <div style=\"color: #ff5f46; font-size:80px;\">90%</div>\n",
    "    <div style=\"font-size:30px; margin-top: -20px; line-height: 30px;\">\n",
    "      Enterprise applications will be AI-augmented by 2025 —IDC\n",
    "    </div>\n",
    "    <div style=\"color: #ff5f46; font-size:80px;\">$10T+</div>\n",
    "    <div style=\"font-size:30px; margin-top: -20px; line-height: 30px;\">\n",
    "       Projected business value creation by AI in 2030 —PWC\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Right Section -->\n",
    "  <div class=\"right_box\", style=\"width: 50%; color: red; font-size: 30px; line-height: 1.5; padding-left: 20px;\">\n",
    "    But—huge challenges getting ML to work at scale!<br/><br/>\n",
    "    In fact, most ML projects still fail before getting to production\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## Machine learning is data + transforms.\n",
    "\n",
    "ML is hard because delivering value to business lines isn't only about building a Model. <br>\n",
    "The ML lifecycle is made of data pipelines: Data-preprocessing, feature engineering, training, inference, monitoring and retraining...<br>\n",
    "Stepping back, all pipelines are data + code.\n",
    "\n",
    "\n",
    "<img style=\"float: right; margin-top: 10px\" width=\"500px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/team_flow_marc.png\" />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/marc.png\" style=\"float: left;\" width=\"80px\"> \n",
    "<h3 style=\"padding: 10px 0px 0px 5px\">Marc, as a Data Scientist, needs a data + ML platform accelerating all the ML & DS steps:</h3>\n",
    "\n",
    "<div style=\"font-size: 19px; margin-left: 73px; clear: left\">\n",
    "<div class=\"badge_b\"><div class=\"badge\">1</div> Build Data Pipeline supporting real time (with DLT)</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">2</div> Data Exploration</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">3</div> Feature creation</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">4</div> Build & train model</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">5</div> Deploy Model (Batch or serverless realtime)</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">6</div> Monitoring</div>\n",
    "</div>\n",
    "\n",
    "**Marc needs a Data Intelligence Platform**. Let's see how we can deploy a Predictive Maintenance model in production with Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "035b2c73-69a2-4668-af0f-9c61370a10a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Predictive maintenance - Single click deployment with AutoML\n",
    "\n",
    "Let's see how we can now leverage the sensor data to build a model predictive maintenance model.\n",
    "\n",
    "Our first step as Data Scientist is to analyze and build the features we'll use to train our model.\n",
    "\n",
    "The sensor table enriched with turbine data has been saved within our Delta Live Table pipeline. All we have to do is read this information, analyze it and start an Auto-ML run.\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-ds-flow.png\" width=\"1000px\">\n",
    "\n",
    "*Note: Make sure you switched to the \"Machine Learning\" persona on the top left menu.*\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=4003492105941350&notebook=%2F04-Data-Science-ML%2F04.1-automl-iot-turbine-predictive-maintenance&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2F04-Data-Science-ML%2F04.1-automl-iot-turbine-predictive-maintenance&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be02e7b-2438-4986-95e3-2d4c8c68a12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-sdk==0.40.0 databricks-feature-engineering==0.8.0 mlflow==2.22.0\n",
    "# %pip install --quiet databricks-sdk==0.40.0 databricks-feature-engineering==0.8.0 \n",
    "# %pip install --quiet mlflow==3.0\n",
    "%pip install --quiet xgboost\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbca96a8-dd37-43ae-88b3-1202b9bc77a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0410a6-bf3e-41e8-979b-b07425dde7bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.deployments import get_deploy_client\n",
    "import os\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e07437d4-b61d-47c3-95bf-562d376af412",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# display(pd.DataFrame({'Python Version': [sys.version]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c112314-5c82-45a3-9e30-4521b8c15b86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(spark.sql(\"SELECT current_version() AS runtime_version\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20e9ab7-9237-4540-9a03-c321aef07c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import mlflow\n",
    "# display(spark.createDataFrame([(mlflow.__version__,)], [\"mlflow_version\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dd75ca1-2ced-4fd6-ab2a-81ea6ba1854c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data exploration and analysis\n",
    "\n",
    "Let's review our dataset and start analyze the data we have to predict our churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65918238-78c7-47ac-bd81-616fdb4ca07c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Quick data exploration leveraging pandas on spark (Koalas): sensor from 2 wind turbines"
    }
   },
   "outputs": [],
   "source": [
    "def plot(sensor_report):\n",
    "  turbine_id = spark.table('turbine_training_dataset').where(f\"abnormal_sensor = '{sensor_report}' \").limit(1).collect()[0]['turbine_id']\n",
    "  #Let's explore a bit our datasets with pandas on spark.\n",
    "  df = spark.table('sensor_bronze').where(f\"turbine_id == '{turbine_id}' \").orderBy('timestamp').limit(500).pandas_api()\n",
    "  df.plot(x=\"timestamp\", y=[\"sensor_B\"], kind=\"line\", title=f'Sensor report: {sensor_report}').show()\n",
    "plot('ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11453de2-1493-432f-9b3b-7bff69134c65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "plot('sensor_B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d4b1ba4-1635-463f-be1d-e6bbe4f8fe09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "As we can see in these graph, we can clearly see some anomaly on the readings we get from sensor F. Let's continue our exploration and use the std we computed in our main feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "688f5f19-3a76-4172-bbf1-edd50b9c8780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read our churn_features table\n",
    "turbine_dataset = spark.table('turbine_training_dataset').withColumn('damaged', col('abnormal_sensor') != 'ok')\n",
    "display(turbine_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f3e3fa4-06d5-4788-9d91-377db2ebe867",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Damaged sensors clearly have a different distribution"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "g = sns.PairGrid(turbine_dataset.sample(0.01).toPandas()[['std_sensor_A', 'std_sensor_E', 'damaged','avg_energy']], diag_sharey=False, hue=\"damaged\")\n",
    "g.map_lower(sns.kdeplot).map_diag(sns.kdeplot, lw=3).map_upper(sns.regplot).add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2feceee9-c119-4631-8f83-e734216cea8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Further data analysis and preparation using pandas API\n",
    "\n",
    "Because our Data Scientist team is familiar with Pandas, we'll use `pandas on spark` to scale `pandas` code. The Pandas instructions will be converted in the spark engine under the hood and distributed at scale.\n",
    "\n",
    "Typicaly Data Science project would involve more advanced preparation and likely require extra data prep step, including more complex feature preparation. We'll keep it simple for this demo.\n",
    "\n",
    "*Note: Starting from `spark 3.2`, koalas is builtin and we can get an Pandas Dataframe using `pandas_api()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26c575fa-0344-4c34-8173-5fd8185b0e8b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Custom pandas transformation / code on top of your entire dataset (koalas)"
    }
   },
   "outputs": [],
   "source": [
    " # Convert to pandas (koalas)\n",
    "dataset = turbine_dataset.pandas_api()\n",
    "\n",
    "# Select the columns we would like to use as ML Model features. #Note: we removed percentiles_sensor_A/B/C.. feature to make the demo easier\n",
    "columns = [\n",
    "    \"turbine_id\",\n",
    "    \"hourly_timestamp\",\n",
    "    \"avg_energy\",\n",
    "    \"std_sensor_A\",\n",
    "    \"std_sensor_B\",\n",
    "    \"std_sensor_C\",\n",
    "    \"std_sensor_D\",\n",
    "    \"std_sensor_E\",\n",
    "    \"std_sensor_F\",\n",
    "    \"location\",\n",
    "    \"model\",\n",
    "    \"state\",\n",
    "    \"abnormal_sensor\"\n",
    "]\n",
    "dataset = dataset[columns]\n",
    "\n",
    "# Drop missing values\n",
    "dataset = dataset.dropna()   \n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "3c8305a2-5b76-49f1-ae8d-ad13c436e3b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Accelerating Predictive maintenance model creation using MLFlow and Databricks Auto-ML\n",
    "\n",
    "MLflow is an open source project allowing model tracking, packaging and deployment. Everytime your datascientist team work on a model, Databricks will track all the parameter and data used and will save it. This ensure ML traceability and reproductibility, making it easy to know which model was build using which parameters/data.\n",
    "\n",
    "### A glass-box solution that empowers data teams without taking away control\n",
    "\n",
    "While Databricks simplify model deployment and governance (MLOps) with MLFlow, bootstraping new ML projects can still be long and inefficient. \n",
    "\n",
    "Instead of creating the same boilerplate for each new project, Databricks Auto-ML can automatically generate state of the art models for Classifications, regression, and forecast.\n",
    "\n",
    "\n",
    "<img width=\"1000\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/auto-ml-full.png\"/>\n",
    "\n",
    "\n",
    "Models can be directly deployed, or instead leverage generated notebooks to boostrap projects with best-practices, saving you weeks of efforts.\n",
    "\n",
    "<br style=\"clear: both\">\n",
    "\n",
    "<img style=\"float: right\" width=\"600\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/retail/resources/images/churn-auto-ml.png\"/>\n",
    "\n",
    "### Using Databricks Auto ML with our Churn dataset\n",
    "\n",
    "Auto ML is available in the \"Machine Learning\" space. All we have to do is start a new Auto-ML experimentation and select the feature table we just created (`turbine_hourly_features`)\n",
    "\n",
    "Our prediction target is the `abnormal_sensor` column.\n",
    "\n",
    "Click on Start, and Databricks will do the rest.\n",
    "\n",
    "While this is done using the UI, you can also leverage the [python API](https://docs.databricks.com/applications/machine-learning/automl.html#automl-python-api-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f127e71-d271-4c54-93c3-a8487dafe0e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mlflow.set_registry_uri('databricks-uc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3184677c-828e-4d27-9fcd-3cf671d80d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "features_table_name=f'{catalog}.{db}.turbine_hourly_features'\n",
    "\n",
    "# read training dataset from catalog\n",
    "training_dataset = spark.table(features_table_name).drop('turbine_id')\n",
    "\n",
    "\n",
    "# 2. Prepare features and labels\n",
    "X = training_dataset.toPandas()[['avg_energy', 'std_sensor_A', 'std_sensor_B', 'std_sensor_C', 'std_sensor_D', 'std_sensor_E', 'std_sensor_F']]\n",
    "\n",
    "y = training_dataset.toPandas()['abnormal_sensor']\n",
    "\n",
    "# 3. Encode labels\n",
    "y_encoded = pd.factorize(y)[0]\n",
    "\n",
    "\n",
    "# Split the dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y_encoded,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a7adcad-84f7-4cc2-a20e-2c5f13de9f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "input_example = pd.DataFrame(X_train_sc).iloc[[0]]\n",
    "\n",
    "# ML model versions in UC must have a model signature (Or input example). If you want to set a signature on a model that's already logged or saved, the mlflow.models.set_signature() API is available for this purpose.\n",
    "\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Define and train the model\n",
    "    lr_model = LogisticRegression(max_iter=200)\n",
    "    lr_model.fit(X_train_sc, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    predictions = lr_model.predict(X_test_sc)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average=\"macro\")\n",
    "    recall = recall_score(y_test, predictions, average=\"macro\")\n",
    "    f1 = f1_score(y_test, predictions, average=\"macro\")\n",
    "\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"max_iter\", 200)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(lr_model, \n",
    "                             artifact_path=\"logreg_model\",\n",
    "                             input_example=input_example,\n",
    "                            #  registered_model_name=\"main.dbdemos_iot_turbine.dbdemos_turbine_maintenance\"\n",
    "                            # conda_env=\"conda.yaml\",\n",
    "                             )\n",
    "\n",
    "    print(f\"Logged with accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e597cf-c3f2-41d7-8c39-cc3f325f2f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#import knn model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Define and train the model\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model.fit(X_train_sc, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    predictions = knn_model.predict(X_test_sc)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average=\"macro\")\n",
    "    recall = recall_score(y_test, predictions, average=\"macro\")\n",
    "    f1 = f1_score(y_test, predictions, average=\"macro\")\n",
    "\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"model_type\", \"KNN\")\n",
    "    mlflow.log_param(\"n_neighbors\", 5)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    # Log the model\n",
    "    mlflow.sklearn.log_model(knn_model, \n",
    "                             artifact_path=\"knn_model\",\n",
    "                             input_example=input_example,\n",
    "                            #  registered_model_name=\"main.dbdemos_iot_turbine.dbdemos_turbine_maintenance\"\n",
    "                            # metadata={\"owner\": \"kemalcan\", \"purpose\": \"demo\", \"python_version\": \"3.12\"}\n",
    "\n",
    "                             )\n",
    "\n",
    "    print(f\"Logged with accuracy: {acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c19529a-8ddb-43c0-ab3e-bf6bbb031111",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "\n",
    "    # Define and train the model\n",
    "    xgb_model = XGBClassifier(\n",
    "        objective=\"multi:softprob\",\n",
    "        num_class=3,\n",
    "        max_depth=3,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"mlogloss\"\n",
    "    )\n",
    "    xgb_model.fit(X_train_sc, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    predictions = xgb_model.predict(X_test_sc)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average=\"macro\")\n",
    "    recall = recall_score(y_test, predictions, average=\"macro\")\n",
    "    f1 = f1_score(y_test, predictions, average=\"macro\")\n",
    "\n",
    "    # Log model parameters\n",
    "    mlflow.log_param(\"model_type\", \"XGBOOST\")\n",
    "    mlflow.log_param(\"max_depth\", 3)\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"learning_rate\", 0.1)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    mlflow.log_metric(\"f1_score\", f1)\n",
    "\n",
    "    mlflow.xgboost.log_model(xgb_model, \n",
    "                             artifact_path=\"xgb_model\",\n",
    "                             input_example=input_example,\n",
    "                            #  registered_model_name=\"main.dbdemos_iot_turbine.dbdemos_turbine_maintenance\"\n",
    "                            conda_env=\"conda.yaml\"\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cac711a-a32d-474f-919d-6d9f29fcffc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.search_runs(order_by=['metrics.accuracy DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "097ad96e-6a02-4796-946a-81a4581223bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_run = pd.DataFrame(mlflow.search_runs(order_by=['metrics.accuracy DESC']).iloc[0])\n",
    "\n",
    "print(f\"Best accuracy:                   {best_run.loc['metrics.accuracy',0]}\")\n",
    "print(f\"Best model type:                 {best_run.loc['params.model_type',0]}\")\n",
    "\n",
    "best_model_run_id = best_run.loc['run_id',0]\n",
    "\n",
    "print(f\"Best model run id:                 {best_model_run_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb533bbd-8815-4150-bebd-87bebd53af76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "AutoML saved our best model in the MLFlow registry. Open the experiment from the AutoML run to explore its artifact and analyze the parameters used, including traceability to the notebook used for its creation.\n",
    "\n",
    "If we're ready, we can move this model into Production stage in a click, or using the API. Let' register the model to Unity Catalog and move it to production.\n",
    "\n",
    "You can programatically get the last best run from your automl training:\n",
    "```\n",
    "from mlflow import MlflowClient\n",
    "\n",
    "# retrieve best model trial run\n",
    "trial_id = automl_run.best_trial.mlflow_run_id\n",
    "model_uri = \"runs:/{}/model\".format(automl_run.best_trial.mlflow_run_id)\n",
    "#Use Databricks Unity Catalog to save our model\n",
    "latest_model = mlflow.register_model(model_uri, f\"{catalog}.{db}.{model_name}\")\n",
    "# Flag it as Production ready using UC Aliases\n",
    "MlflowClient().set_registered_model_alias(name=f\"{catalog}.{db}.{model_name}\", alias=\"prod\", version=latest_model.version)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8f05f40-211b-4cbf-a584-56d1200f56a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "best_run = mlflow.search_runs(order_by=['metrics.accuracy DESC']).iloc[0]\n",
    "best_model_run_id = best_run['run_id']\n",
    "\n",
    "model_uri = f\"runs:/{best_model_run_id}/logreg_model\" if best_run['params.model_type'] == \"LogisticRegression\" else f\"runs:/{best_model_run_id}/xgb_model\"\n",
    "\n",
    "\n",
    "model_name = \"dbdemos_turbine_maintenance\"\n",
    "\n",
    "latest_model = mlflow.register_model(model_uri, f\"{catalog}.{db}.{model_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19feab71-205d-49e5-9f1e-9e8bdc485fc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "loaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec702667-8ef7-41d2-9df2-3375d32a324d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "loaded_model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44459c45-9d3c-43ca-8131-f95867ebe39c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Flag it as Production ready using UC Aliases\n",
    "MlflowClient().set_registered_model_alias(name=f\"{catalog}.{db}.dbdemos_turbine_maintenance\", \n",
    "                                          alias=\"prod\", \n",
    "                                          version=latest_model.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b2be9f-e88d-411f-ae31-9c7fff6785a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6353292d-2900-4b14-a548-402c23cc0aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # # Creating a User-Defined Function (UDF) with an ML model in Spark allows you to apply the model to data within a Spark DataFrame. This means you can use the model to make predictions directly in your Spark SQL queries or DataFrame operations.\n",
    "# # # By creating a UDF with an ML model, you can seamlessly integrate machine learning predictions into your data processing workflows in Spark.\n",
    "\n",
    "# predict_maintenance = mlflow.pyfunc.spark_udf(spark, \n",
    "#                                               f\"models:/{catalog}.{db}.{model_name}@prod\", \n",
    "#                                               \"integer\", #output\n",
    "#                                               env_manager='virtualenv',\n",
    "#                                               )\n",
    "\n",
    "\n",
    "# #This registers the UDF with Spark SQL, allowing you to use it in SQL queries.\n",
    "# spark.udf.register(\"predict_maintenance\", predict_maintenance)\n",
    "\n",
    "\n",
    "# # # This retrieves the names of the input columns that the model expects.\n",
    "# # columns = predict_maintenance.metadata.get_input_schema().input_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60cbde3b-70a2-4780-8837-a6fc9bb8d972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f898b672-6bfa-44a0-b573-c551f919cec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Create a sample DataFrame with the same schema as the input data\n",
    "# sample_data = [(1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0)]\n",
    "\n",
    "# columns = ['avg_energy', 'std_sensor_A', 'std_sensor_B', 'std_sensor_C', 'std_sensor_D', 'std_sensor_E', 'std_sensor_F']\n",
    "\n",
    "# sample_df = spark.createDataFrame(sample_data, columns)\n",
    "\n",
    "# # Apply the UDF to the sample DataFrame\n",
    "# result_df = sample_df.withColumn(\"prediction\", predict_maintenance(*[col(c) for c in columns]))\n",
    "\n",
    "# # Display the result to verify the function\n",
    "# display(result_df)\n",
    "\n",
    "# create a table in the catalog\n",
    "# result_df.write.mode(\"overwrite\").saveAsTable(\"turbine_hourly_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d71b828d-7f69-4740-9b41-771ab62a438d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This applies the UDF to a Spark DataFrame, adding a new column with the model's predictions.\n",
    "# spark.table('turbine_hourly_features').withColumn(\"dbdemos_turbine_maintenance\", predict_maintenance(*[col(c) for c in columns])).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e9682d-1859-4d4a-9eda-d23e52ae6803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT turbine_id, \n",
    "# predict_maintenance(avg_energy, std_sensor_A, std_sensor_B, std_sensor_C, std_sensor_D, std_sensor_E, std_sensor_F) as prediction \n",
    "\n",
    "# FROM turbine_hourly_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153ad103-9b57-4b9a-be82-c024011b5c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_SERVING_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4aeb401-440d-45f5-ba10-b5d4131940cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "try:\n",
    "    endpoint = client.create_endpoint(\n",
    "        name=MODEL_SERVING_ENDPOINT_NAME,\n",
    "        config={\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"iot-maintenance-serving-endpoint\",\n",
    "                    \"entity_name\": f\"{catalog}.{db}.{model_name}\",\n",
    "                    \"entity_version\": get_last_model_version(f\"{catalog}.{db}.{model_name}\"),\n",
    "                    \"workload_size\": \"Small\",\n",
    "                    \"scale_to_zero_enabled\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"Endpoint {catalog}.{db}.{MODEL_SERVING_ENDPOINT_NAME} already exists. Skipping creation.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "while client.get_endpoint(MODEL_SERVING_ENDPOINT_NAME)['state']['config_update'] == 'IN_PROGRESS':\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b715e36c-9764-4513-bf45-0d8d5429d435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the API endpoint and token for the current notebook context\n",
    "API_ROOT = \"https://dbc-344514e4-b5cd.staging.cloud.databricks.com/\"\n",
    "\n",
    "\n",
    "def create_tf_serving_json(data):\n",
    "    return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "def score_model(dataset):\n",
    "\n",
    "    url = f'{API_ROOT}/serving-endpoints/dbdemos_iot_turbine_prediction_endpoint/invocations'\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {API_TOKEN}', \n",
    "               'Content-Type': 'application/json'}\n",
    "\n",
    "\n",
    "    ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acddaeb-1e01-49f4-aa7b-cad619e5e5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(f'turbine_hourly_features').toPandas()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682cbcb9-2903-4333-8605-6cf50d12b32f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dataset = spark.table(f'turbine_hourly_features').select(*columns).toPandas()[:5]\n",
    "\n",
    "np.array(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992948ad-5108-4e4b-8945-b4bde06da9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# #Deploy your model and uncomment to run your inferences live!\n",
    "score_model(np.array(dataset))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8281987032933773,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04.1-automl-iot-turbine-predictive-maintenance",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
