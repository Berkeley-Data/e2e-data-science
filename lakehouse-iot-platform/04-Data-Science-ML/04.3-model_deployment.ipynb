{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "fff79d53-904b-42c3-96ef-1e22969cbd3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Data Science with Databricks\n",
    "\n",
    "## ML is key to wind turbine farm optimization\n",
    "\n",
    "The current market makes energy even more strategic than before. Being able to ingest and analyze our Wind turbine state is a first step, but this isn't enough to thrive in a very competitive market.\n",
    "\n",
    "We need to go further to optimize our energy production, reduce maintenance cost and reduce downtime. Modern data company achieve this with AI.\n",
    "\n",
    "<style>\n",
    ".right_box{\n",
    "  margin: 30px; box-shadow: 10px -10px #CCC; width:650px;height:300px; background-color: #1b3139ff; box-shadow:  0 0 10px  rgba(0,0,0,0.6);\n",
    "  border-radius:25px;font-size: 35px; float: left; padding: 20px; color: #f9f7f4; }\n",
    ".badge {\n",
    "  clear: left; float: left; height: 30px; width: 30px;  display: table-cell; vertical-align: middle; border-radius: 50%; background: #fcba33ff; text-align: center; color: white; margin-right: 10px}\n",
    ".badge_b { \n",
    "  height: 35px}\n",
    "</style>\n",
    "<link href='https://fonts.googleapis.com/css?family=DM Sans' rel='stylesheet'>\n",
    "<div style=\"font-family: 'DM Sans'; display: flex; align-items: flex-start;\">\n",
    "  <!-- Left Section -->\n",
    "  <div style=\"width: 50%; color: #1b3139; padding-right: 20px;\">\n",
    "    <div style=\"color: #ff5f46; font-size:80px;\">90%</div>\n",
    "    <div style=\"font-size:30px; margin-top: -20px; line-height: 30px;\">\n",
    "      Enterprise applications will be AI-augmented by 2025 —IDC\n",
    "    </div>\n",
    "    <div style=\"color: #ff5f46; font-size:80px;\">$10T+</div>\n",
    "    <div style=\"font-size:30px; margin-top: -20px; line-height: 30px;\">\n",
    "       Projected business value creation by AI in 2030 —PWC\n",
    "    </div>\n",
    "  </div>\n",
    "\n",
    "  <!-- Right Section -->\n",
    "  <div class=\"right_box\", style=\"width: 50%; color: red; font-size: 30px; line-height: 1.5; padding-left: 20px;\">\n",
    "    But—huge challenges getting ML to work at scale!<br/><br/>\n",
    "    In fact, most ML projects still fail before getting to production\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "## Machine learning is data + transforms.\n",
    "\n",
    "ML is hard because delivering value to business lines isn't only about building a Model. <br>\n",
    "The ML lifecycle is made of data pipelines: Data-preprocessing, feature engineering, training, inference, monitoring and retraining...<br>\n",
    "Stepping back, all pipelines are data + code.\n",
    "\n",
    "\n",
    "<img style=\"float: right; margin-top: 10px\" width=\"500px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/team_flow_marc.png\" />\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/marc.png\" style=\"float: left;\" width=\"80px\"> \n",
    "<h3 style=\"padding: 10px 0px 0px 5px\">Marc, as a Data Scientist, needs a data + ML platform accelerating all the ML & DS steps:</h3>\n",
    "\n",
    "<div style=\"font-size: 19px; margin-left: 73px; clear: left\">\n",
    "<div class=\"badge_b\"><div class=\"badge\">1</div> Build Data Pipeline supporting real time (with DLT)</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">2</div> Data Exploration</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">3</div> Feature creation</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">4</div> Build & train model</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">5</div> Deploy Model (Batch or serverless realtime)</div>\n",
    "<div class=\"badge_b\"><div class=\"badge\">6</div> Monitoring</div>\n",
    "</div>\n",
    "\n",
    "**Marc needs a Data Intelligence Platform**. Let's see how we can deploy a Predictive Maintenance model in production with Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "035b2c73-69a2-4668-af0f-9c61370a10a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Predictive maintenance \n",
    "\n",
    "Let's see how we can now leverage the sensor data to build a model predictive maintenance model.\n",
    "\n",
    "Our first step as Data Scientist is to analyze and build the features we'll use to train our model.\n",
    "\n",
    "The sensor table enriched with turbine data has been saved within our Delta Live Table pipeline. \n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-ds-flow.png\" width=\"1000px\">\n",
    "\n",
    "*Note: Make sure you switched to the \"Machine Learning\" persona on the top left menu.*\n",
    "\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=4003492105941350&notebook=%2F04-Data-Science-ML%2F04.1-automl-iot-turbine-predictive-maintenance&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2F04-Data-Science-ML%2F04.1-automl-iot-turbine-predictive-maintenance&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be02e7b-2438-4986-95e3-2d4c8c68a12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-sdk==0.40.0 mlflow==2.22.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbca96a8-dd37-43ae-88b3-1202b9bc77a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0410a6-bf3e-41e8-979b-b07425dde7bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.deployments import get_deploy_client\n",
    "import os\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f127e71-d271-4c54-93c3-a8487dafe0e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri('databricks-uc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6353292d-2900-4b14-a548-402c23cc0aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # # Creating a User-Defined Function (UDF) with an ML model in Spark allows you to apply the model to data within a Spark DataFrame. This means you can use the model to make predictions directly in your Spark SQL queries or DataFrame operations.\n",
    "# # # By creating a UDF with an ML model, you can seamlessly integrate machine learning predictions into your data processing workflows in Spark.\n",
    "\n",
    "predict_maintenance = mlflow.pyfunc.spark_udf(spark, \n",
    "                                              f\"models:/{catalog}.{db}.dbdemos_turbine_maintenance@prod\", \n",
    "                                              \"float\", #output\n",
    "                                              env_manager='virtualenv'\n",
    "\n",
    "                                              )\n",
    "\n",
    "\n",
    "#This registers the UDF with Spark SQL, allowing you to use it in SQL queries.\n",
    "spark.udf.register(\"predict_maintenance\", predict_maintenance)\n",
    "\n",
    "\n",
    "# This retrieves the names of the input columns that the model expects.\n",
    "columns = predict_maintenance.metadata.get_input_schema().input_names()\n",
    "\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44cd82f9-0033-464e-8111-501ea078589b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predict_maintenance.metadata.get_input_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f898b672-6bfa-44a0-b573-c551f919cec8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Create a sample DataFrame with the same schema as the input data\n",
    "# sample_data = [(1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0)]\n",
    "\n",
    "# sample_df = spark.createDataFrame(sample_data, columns)\n",
    "\n",
    "\n",
    "# # # # Apply the UDF to the sample DataFrame\n",
    "# # result_df = sample_df.withColumn(\"prediction\", predict_maintenance(*[col(c) for c in columns]))\n",
    "\n",
    "# # # Display the result to verify the function\n",
    "# # display(result_df)\n",
    "\n",
    "# # # create a table in the catalog\n",
    "# # result_df.write.mode(\"overwrite\").saveAsTable(\"turbine_hourly_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d71b828d-7f69-4740-9b41-771ab62a438d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBUaGlzIGFwcGxpZXMgdGhlIFVERiB0byBhIFNwYXJrIERhdGFGcmFtZSwgYWRkaW5nIGEgbmV3IGNvbHVtbiB3aXRoIHRoZSBtb2RlbCdzIHByZWRpY3Rpb25zLgoKCmJhdGNoX3ByZWRfZGYgPSBzcGFyay50YWJsZSgndHVyYmluZV9ob3VybHlfZmVhdHVyZXMnKS53aXRoQ29sdW1uKCJkYmRlbW9zX3R1cmJpbmVfbWFpbnRlbmFuY2UiLCBwcmVkaWN0X21haW50ZW5hbmNlKCpjb2x1bW5zKSkKCmJhdGNoX3ByZWRfZGYuZGlzcGxheSgpCgojIGNyZWF0ZSBhIHRhYmxlIGluIHRoZSBjYXRhbG9nCmJhdGNoX3ByZWRfZGYud3JpdGUubW9kZSgib3ZlcndyaXRlIikuc2F2ZUFzVGFibGUoInR1cmJpbmVfaG91cmx5X3ByZWRpY3Rpb25zIik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewf2c9da2\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewf2c9da2\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewf2c9da2\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewf2c9da2) ,min_max AS (SELECT `dbdemos_turbine_maintenance`,(SELECT MAX(`dbdemos_turbine_maintenance`) FROM q) `target_column_max`,(SELECT MIN(`dbdemos_turbine_maintenance`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `dbdemos_turbine_maintenance`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 3 `step` FROM min_max) SELECT IF(ISNULL(`dbdemos_turbine_maintenance`),NULL,LEAST(WIDTH_BUCKET(`dbdemos_turbine_maintenance`,`min_value`,`max_value`,3),3)) `dbdemos_turbine_maintenance_BIN`,FIRST(`min_value` + ((IF(ISNULL(`dbdemos_turbine_maintenance`),NULL,LEAST(WIDTH_BUCKET(`dbdemos_turbine_maintenance`,`min_value`,`max_value`,3),3)) - 1) * `step`)) `dbdemos_turbine_maintenance_BIN_LOWER_BOUND`,FIRST(`step`) `dbdemos_turbine_maintenance_BIN_STEP`,COUNT(`dbdemos_turbine_maintenance`) `COUNT` FROM histogram_meta GROUP BY `dbdemos_turbine_maintenance_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewf2c9da2\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "dbdemos_turbine_maintenance",
             "id": "column_fd9faf61377"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 3,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "avg_energy": {
             "type": "histogram",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "1d69ffa9-5fb2-4419-be44-198287be1b7e",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 12.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "dbdemos_turbine_maintenance_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "dbdemos_turbine_maintenance_BIN",
           "args": [
            {
             "column": "dbdemos_turbine_maintenance",
             "type": "column"
            },
            {
             "number": 3,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "dbdemos_turbine_maintenance_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "dbdemos_turbine_maintenance",
             "type": "column"
            },
            {
             "number": 3,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "dbdemos_turbine_maintenance_BIN_STEP",
           "args": [
            {
             "column": "dbdemos_turbine_maintenance",
             "type": "column"
            },
            {
             "number": 3,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "dbdemos_turbine_maintenance",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This applies the UDF to a Spark DataFrame, adding a new column with the model's predictions.\n",
    "\n",
    "\n",
    "batch_pred_df = spark.table('turbine_hourly_features').withColumn(\"dbdemos_turbine_maintenance\", predict_maintenance(*columns))\n",
    "\n",
    "batch_pred_df.display()\n",
    "\n",
    "# create a table in the catalog\n",
    "batch_pred_df.write.mode(\"overwrite\").saveAsTable(\"turbine_hourly_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e9682d-1859-4d4a-9eda-d23e52ae6803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT turbine_id, \n",
    "# predict_maintenance(avg_energy, std_sensor_A, std_sensor_B, std_sensor_C, std_sensor_D, std_sensor_E, std_sensor_F) as prediction \n",
    "\n",
    "# FROM turbine_hourly_features\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153ad103-9b57-4b9a-be82-c024011b5c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_SERVING_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee256bb-1ea6-42c7-959a-ffbd661c7dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "for each in client.list_endpoints():\n",
    "    if each['name'] == MODEL_SERVING_ENDPOINT_NAME:\n",
    "        client.delete_endpoint(MODEL_SERVING_ENDPOINT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4aeb401-440d-45f5-ba10-b5d4131940cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Endpoint creation spins up a container that will run the model for inference. This can take 12+ minutes to complete.\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "try:\n",
    "    endpoint = client.create_endpoint(\n",
    "        name=MODEL_SERVING_ENDPOINT_NAME,\n",
    "        config={\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"iot-maintenance-serving-endpoint\",\n",
    "                    \"entity_name\": f\"{catalog}.{db}.{model_name}\",\n",
    "                    \"entity_version\": get_last_model_version(f\"{catalog}.{db}.{model_name}\"),\n",
    "                    \"workload_size\": \"Small\",\n",
    "                    \"scale_to_zero_enabled\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"Endpoint {catalog}.{db}.{MODEL_SERVING_ENDPOINT_NAME} already exists. Skipping creation.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "while client.get_endpoint(MODEL_SERVING_ENDPOINT_NAME)['state']['config_update'] == 'IN_PROGRESS':\n",
    "    time.sleep(10) \n",
    "\n",
    "if client.get_endpoint(MODEL_SERVING_ENDPOINT_NAME)['state']['ready'] != 'READY':\n",
    "    print(f\"Endpoint {catalog}.{db}.{MODEL_SERVING_ENDPOINT_NAME} creation failed.\")\n",
    "else:\n",
    "    print(f\"Endpoint {catalog}.{db}.{MODEL_SERVING_ENDPOINT_NAME} created successfully.\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c76a18-f2d6-4d7a-8d84-3cbb06af7182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the API endpoint and token for the current notebook context\n",
    "API_ROOT = f\"https://{dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().value()}/\"\n",
    "API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b715e36c-9764-4513-bf45-0d8d5429d435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_tf_serving_json(data):\n",
    "    return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "def score_model(dataset):\n",
    "\n",
    "    url = f'{API_ROOT}/serving-endpoints/{MODEL_SERVING_ENDPOINT_NAME}/invocations'\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {API_TOKEN}', \n",
    "               'Content-Type': 'application/json'}\n",
    "\n",
    "\n",
    "    ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acddaeb-1e01-49f4-aa7b-cad619e5e5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(f'turbine_hourly_features').toPandas()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682cbcb9-2903-4333-8605-6cf50d12b32f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = ['avg_energy', 'std_sensor_A', 'std_sensor_B', 'std_sensor_C', 'std_sensor_D', 'std_sensor_E', 'std_sensor_F']\n",
    "dataset = spark.table(f'turbine_hourly_features').select(*columns).toPandas()[:5]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992948ad-5108-4e4b-8945-b4bde06da9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Deploy your model and uncomment to run your inferences live!\n",
    "score_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c459abc4-6dd8-4e90-afcb-d5c93d0f8775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Try it on another computer\n",
    "\n",
    "\n",
    "curl \\\n",
    "  -u token:$DATABRICKS_TOKEN \\\n",
    "  -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"inputs\": [[0.1889792 , 0.9644652 , 2.65583866, 3.4528106 , 2.48515875,\n",
    "        2.28840325, 4.70213899],\n",
    "       [0.19212258, 1.06818556, 2.38481843, 3.30341204, 2.17225129,\n",
    "        2.34259302, 4.87087542],\n",
    "       [0.17356345, 1.14208877, 2.0627087 , 3.01932966, 2.33955204,\n",
    "        2.73069787, 4.23719664],\n",
    "       [0.10343409, 1.04987272, 2.21921651, 3.24672614, 2.32046658,\n",
    "        2.66270018, 4.28940458],\n",
    "       [0.15481244, 1.03255521, 2.14210166, 2.72984232, 2.35974868,\n",
    "        2.7614664 , 4.58878877]]}' \\\n",
    "  https://dbc-0664a3f5-7bb4.cloud.databricks.com/serving-endpoints/dbdemos_iot_turbine_prediction_endpoint/invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005577c2-7af4-4bb7-80af-4da91ba44d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# %sql\n",
    "# SELECT ai_query('dbdemos_iot_turbine_prediction_endpoint',\n",
    "#     request => {\n",
    "#   \"dataframe_split\": {\n",
    "#     \"data\": [\n",
    "#       [\n",
    "#         0.3343003711119671,\n",
    "#         0.3250868023612564,\n",
    "#         -0.3970504309971035,\n",
    "#         -0.26756059753270023,\n",
    "#         -0.38967895864662727,\n",
    "#         1.606278850727433,\n",
    "#         4.490631184834478\n",
    "#       ]\n",
    "#     ]\n",
    "#   }\n",
    "# })"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4851250551581089,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04.3-model_deployment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
