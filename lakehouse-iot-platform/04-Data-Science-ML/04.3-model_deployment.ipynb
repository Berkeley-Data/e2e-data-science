{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7eb3b3cb-5165-44e2-9b3d-b34a58670ceb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "![](../_resources/images/e2eai-4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "fff79d53-904b-42c3-96ef-1e22969cbd3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# MLOps on Databricks\n",
    "\n",
    "Being successful with ML and AI is about more than just building models.  We need to consider lineage, repeatability, model consumption and ongoing performance.  And all of this needs to be managed using a repeatable and hopefully automated process.  This is the idea behind MLOps.\n",
    "\n",
    "In this notebook, we will walk through model deployment to a live API Endpoint managed by Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be02e7b-2438-4986-95e3-2d4c8c68a12b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install --quiet databricks-sdk==0.40.0 mlflow==2.22.0\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbca96a8-dd37-43ae-88b3-1202b9bc77a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0410a6-bf3e-41e8-979b-b07425dde7bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.deployments import get_deploy_client\n",
    "import os\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f127e71-d271-4c54-93c3-a8487dafe0e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mlflow.set_registry_uri('databricks-uc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3979f476-43a8-4914-b199-110c5f7b1e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a local UDF for our saved model\n",
    "By creating a local UDF with an ML model, you can seamlessly integrate machine learning predictions into your data processing workflows in Spark. This method makes the model available (via the UDF) in your local notebook scope and is useful for batch inference.\n",
    "\n",
    "Databricks spins up a virtual environment for the model to ensure it runs in the same environment it was built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a32f1d58-a211-4ead-a1d2-afef710cf9c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Change the model_name here if you changed it in the prior notebook ####\n",
    "model_name = \"turbine_maintenance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6353292d-2900-4b14-a548-402c23cc0aea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Creating a User-Defined Function (UDF) with an ML model in Spark allows you to apply the model to data within a Spark DataFrame. This means you can use the model to make predictions directly in your Spark SQL queries or DataFrame operations.\n",
    "\n",
    "# spark_udf loads the model in a virtual environment which can take 15+ minutes to build.\n",
    "\n",
    "# This UDF is available in the context of this notebook.\n",
    "\n",
    "predict_maintenance = mlflow.pyfunc.spark_udf(spark, \n",
    "                                              f\"models:/{catalog}.{db}.{model_name}@prod\", \n",
    "                                              \"float\", #output\n",
    "                                              env_manager='virtualenv'\n",
    "                                              )\n",
    "\n",
    "\n",
    "# This registers the UDF with Spark SQL, allowing you to use it in SQL queries.\n",
    "spark.udf.register(\"predict_maintenance\", predict_maintenance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d51b67b-b843-4f32-9378-c1956b3133a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the names of the input columns that the model expects.\n",
    "columns = predict_maintenance.metadata.get_input_schema().input_names()\n",
    "\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44cd82f9-0033-464e-8111-501ea078589b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check the signature / expected input schema\n",
    "predict_maintenance.metadata.get_input_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d71b828d-7f69-4740-9b41-771ab62a438d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"IyBUaGlzIGFwcGxpZXMgdGhlIFVERiB0byBhIFNwYXJrIERhdGFGcmFtZSwgYWRkaW5nIGEgbmV3IGNvbHVtbiB3aXRoIHRoZSBtb2RlbCdzIHByZWRpY3Rpb25zLgoKCmJhdGNoX3ByZWRfZGYgPSBzcGFyay50YWJsZSgndHVyYmluZV9ob3VybHlfZmVhdHVyZXMnKS53aXRoQ29sdW1uKCJkYmRlbW9zX3R1cmJpbmVfbWFpbnRlbmFuY2UiLCBwcmVkaWN0X21haW50ZW5hbmNlKCpjb2x1bW5zKSkKCmJhdGNoX3ByZWRfZGYuZGlzcGxheSgpCgojIGNyZWF0ZSBhIHRhYmxlIGluIHRoZSBjYXRhbG9nCmJhdGNoX3ByZWRfZGYud3JpdGUubW9kZSgib3ZlcndyaXRlIikuc2F2ZUFzVGFibGUoInR1cmJpbmVfaG91cmx5X3ByZWRpY3Rpb25zIik=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView34324db\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView34324db\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView34324db\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView34324db) ,min_max AS (SELECT `dbdemos_turbine_maintenance`,(SELECT MAX(`dbdemos_turbine_maintenance`) FROM q) `target_column_max`,(SELECT MIN(`dbdemos_turbine_maintenance`) FROM q) `target_column_min` FROM q) ,histogram_meta AS (SELECT `dbdemos_turbine_maintenance`,`target_column_min` `min_value`,IF(`target_column_max` = `target_column_min`,`target_column_max` + 1,`target_column_max`) `max_value`,(`target_column_max` - `target_column_min`) / 3 `step` FROM min_max) SELECT IF(ISNULL(`dbdemos_turbine_maintenance`),NULL,LEAST(WIDTH_BUCKET(`dbdemos_turbine_maintenance`,`min_value`,`max_value`,3),3)) `dbdemos_turbine_maintenance_BIN`,FIRST(`min_value` + ((IF(ISNULL(`dbdemos_turbine_maintenance`),NULL,LEAST(WIDTH_BUCKET(`dbdemos_turbine_maintenance`,`min_value`,`max_value`,3),3)) - 1) * `step`)) `dbdemos_turbine_maintenance_BIN_LOWER_BOUND`,FIRST(`step`) `dbdemos_turbine_maintenance_BIN_STEP`,COUNT(`dbdemos_turbine_maintenance`) `COUNT` FROM histogram_meta GROUP BY `dbdemos_turbine_maintenance_BIN`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView34324db\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "dbdemos_turbine_maintenance",
             "id": "column_fd9faf61377"
            }
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "histogram",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numBins": 3,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "avg_energy": {
             "type": "histogram",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestAssumeRoleInfo": null,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "1d69ffa9-5fb2-4419-be44-198287be1b7e",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 12.0,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "dbdemos_turbine_maintenance_BIN",
           "type": "column"
          }
         ],
         "selects": [
          {
           "alias": "dbdemos_turbine_maintenance_BIN",
           "args": [
            {
             "column": "dbdemos_turbine_maintenance",
             "type": "column"
            },
            {
             "number": 3,
             "type": "number"
            }
           ],
           "function": "BIN",
           "type": "function"
          },
          {
           "alias": "dbdemos_turbine_maintenance_BIN_LOWER_BOUND",
           "args": [
            {
             "column": "dbdemos_turbine_maintenance",
             "type": "column"
            },
            {
             "number": 3,
             "type": "number"
            }
           ],
           "function": "BIN_LOWER_BOUND",
           "type": "function"
          },
          {
           "alias": "dbdemos_turbine_maintenance_BIN_STEP",
           "args": [
            {
             "column": "dbdemos_turbine_maintenance",
             "type": "column"
            },
            {
             "number": 3,
             "type": "number"
            }
           ],
           "function": "BIN_STEP",
           "type": "function"
          },
          {
           "alias": "COUNT",
           "args": [
            {
             "column": "dbdemos_turbine_maintenance",
             "type": "column"
            }
           ],
           "function": "COUNT",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the UDF to a Spark DataFrame, adding a new column with the model's predictions.\n",
    "\n",
    "batch_pred_df = spark.table('turbine_hourly_features').withColumn(\"predict_turbine_maintenance\", predict_maintenance(*columns))\n",
    "\n",
    "display(batch_pred_df)\n",
    "\n",
    "# create a table in the catalog\n",
    "batch_pred_df.write.mode(\"overwrite\").saveAsTable(\"turbine_hourly_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8e9682d-1859-4d4a-9eda-d23e52ae6803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- An example of using our UDF in a SQL query\n",
    "SELECT turbine_id, \n",
    "    predict_maintenance(avg_energy, \n",
    "                        std_sensor_A, \n",
    "                        std_sensor_B, \n",
    "                        std_sensor_C, \n",
    "                        std_sensor_D, \n",
    "                        std_sensor_E, \n",
    "                        std_sensor_F) as prediction \n",
    "FROM turbine_hourly_features\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daf8fcd2-5706-4ab3-9eea-95fdb6a216a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create a serving endpoint for our model\n",
    "By creating a serving endpoint for an ML model, you can seamlessly integrate machine learning predictions into external applications or agents.  This method makes the model available (via the endpoint) outside your local notebook scope and is useful for near real-time or on-demand inference.\n",
    "\n",
    "Once again, Databricks spins up a virtual environment for the model to ensure it runs in the same environment it was built.  This virtual environment remains up and running, waiting for inference requests.  To help save cost, Databricks sets the allocated compute to scale to zero by default based on idle time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "153ad103-9b57-4b9a-be82-c024011b5c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "MODEL_SERVING_ENDPOINT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ee256bb-1ea6-42c7-959a-ffbd661c7dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the endpoint if it already exists\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "for each in client.list_endpoints():\n",
    "    if each['name'] == MODEL_SERVING_ENDPOINT_NAME:\n",
    "        client.delete_endpoint(MODEL_SERVING_ENDPOINT_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4aeb401-440d-45f5-ba10-b5d4131940cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Endpoint creation spins up a container that will run the model for inference. This can take 15+ minutes to complete.\n",
    "\n",
    "# The endpoint will be availalbe via API or SDK outside this notebook context.\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "\n",
    "try:\n",
    "    endpoint = client.create_endpoint(\n",
    "        name=MODEL_SERVING_ENDPOINT_NAME,\n",
    "        config={\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"iot-maintenance-serving-endpoint\",\n",
    "                    \"entity_name\": f\"{catalog}.{db}.{model_name}\",\n",
    "                    \"entity_version\": get_last_model_version(f\"{catalog}.{db}.{model_name}\"),\n",
    "                    \"workload_size\": \"Small\",\n",
    "                    \"scale_to_zero_enabled\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"Endpoint {catalog}.{db}.{MODEL_SERVING_ENDPOINT_NAME} already exists. Skipping creation.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "while client.get_endpoint(MODEL_SERVING_ENDPOINT_NAME)['state']['config_update'] == 'IN_PROGRESS':\n",
    "    time.sleep(10) \n",
    "\n",
    "if client.get_endpoint(MODEL_SERVING_ENDPOINT_NAME)['state']['ready'] != 'READY':\n",
    "    print(f\"Endpoint {catalog}.{db}.{MODEL_SERVING_ENDPOINT_NAME} creation failed.\")\n",
    "else:\n",
    "    print(f\"Endpoint {catalog}.{db}.{MODEL_SERVING_ENDPOINT_NAME} created successfully.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0f443a5-a466-492a-b9fc-c65c5859c770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Test our serving endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3acddaeb-1e01-49f4-aa7b-cad619e5e5b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Python API method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36c76a18-f2d6-4d7a-8d84-3cbb06af7182",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Get the API endpoint and token for the current notebook context\n",
    "API_ROOT = f\"https://{dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().value()}/\"\n",
    "API_TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b715e36c-9764-4513-bf45-0d8d5429d435",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_tf_serving_json(data):\n",
    "    return {'inputs': {name: data[name].tolist() for name in data.keys()} if isinstance(data, dict) else data.tolist()}\n",
    "\n",
    "def score_model(dataset):\n",
    "\n",
    "    url = f'{API_ROOT}/serving-endpoints/{MODEL_SERVING_ENDPOINT_NAME}/invocations'\n",
    "\n",
    "    headers = {'Authorization': f'Bearer {API_TOKEN}', \n",
    "               'Content-Type': 'application/json'}\n",
    "\n",
    "\n",
    "    ds_dict = {'dataframe_split': dataset.to_dict(orient='split')} if isinstance(dataset, pd.DataFrame) else create_tf_serving_json(dataset)\n",
    "\n",
    "    data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "\n",
    "    response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682cbcb9-2903-4333-8605-6cf50d12b32f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "columns = ['avg_energy', 'std_sensor_A', 'std_sensor_B', 'std_sensor_C', 'std_sensor_D', 'std_sensor_E', 'std_sensor_F']\n",
    "\n",
    "# Get 5 rows to test with\n",
    "dataset = spark.table(f'turbine_hourly_features').select(*columns).toPandas()[:5]\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992948ad-5108-4e4b-8945-b4bde06da9fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use our function to call the API of our model and get inferences live!\n",
    "score_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8bfd85c7-b8f6-4dda-97a4-9f46491a0a19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Using ai_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005577c2-7af4-4bb7-80af-4da91ba44d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query our endpoint name using ai_query\n",
    "-- ai_query is a powerful way to apply any ML or AI endpoint to a large dataset\n",
    "SELECT ai_query('e2eai_iot_turbine_prediction_endpoint',\n",
    "STRUCT(CAST(avg_energy AS DOUBLE) AS avg_energy, \n",
    "      CAST(std_sensor_A AS DOUBLE) AS std_sensor_A, \n",
    "      CAST(std_sensor_B AS DOUBLE) AS std_sensor_B, \n",
    "      CAST(std_sensor_C AS DOUBLE) AS std_sensor_C, \n",
    "      CAST(std_sensor_D AS DOUBLE) AS std_sensor_D, \n",
    "      CAST(std_sensor_E AS DOUBLE) AS std_sensor_E,\n",
    "      CAST(std_sensor_F AS DOUBLE) AS std_sensor_F), \n",
    "returnType => 'FLOAT') AS prediction\n",
    "FROM turbine_hourly_features\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01b42d16-6da6-4d3d-b189-d30a0cd853ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query our endpoint name using ai_query with values we provide (not from a table)\n",
    "SELECT ai_query('e2eai_iot_turbine_prediction_endpoint',\n",
    "  STRUCT(0.1889 AS avg_energy, \n",
    "        0.9644 AS std_sensor_A, \n",
    "        2.6558 AS std_sensor_B, \n",
    "        3.4528 AS std_sensor_C, \n",
    "        2.4851 AS std_sensor_D, \n",
    "        2.2884 AS std_sensor_E, \n",
    "        4.7021 AS std_sensor_F),\n",
    "  returnType => 'FLOAT') AS prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c459abc4-6dd8-4e90-afcb-d5c93d0f8775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Try viewing your endpoint in the UI by going to Serving, then clicking on your endpoint name. From this page, you can view usage statistics for your endpoint or click `Use` to see code examples of how to call your endpoint in various different ways."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7664950503516276,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "04.3-model_deployment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
