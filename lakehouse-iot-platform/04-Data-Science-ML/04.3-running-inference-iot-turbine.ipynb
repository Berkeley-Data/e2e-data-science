{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00e3a8ff-f119-48e2-9459-87779a932286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Predictive Maintenance Inference - Batch or serverless real-time\n",
    "\n",
    "\n",
    "With AutoML, our best model was automatically saved in our MLFlow registry.\n",
    "\n",
    "All we need to do now is use this model to run Inferences. A simple solution is to share the model name to our Data Engineering team and they'll be able to call this model within the pipeline they maintained. That's what we did in our Delta Live Table pipeline!\n",
    "\n",
    "Alternatively, this can be schedule in a separate job. Here is an example to show you how MLFlow can be directly used to retriver the model and run inferences.\n",
    "\n",
    "*Make sure you run the previous notebook to be able to access the data.*\n",
    "\n",
    "## Environment Recreation\n",
    "The cell below downloads the model artifacts associated with your model in the remote registry, which include `conda.yaml` and `requirements.txt` files. In this notebook, `pip` is used to reinstall dependencies by default.\n",
    "\n",
    "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=4003492105941350&notebook=%2F04-Data-Science-ML%2F04.3-running-inference-iot-turbine&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2F04-Data-Science-ML%2F04.3-running-inference-iot-turbine&version=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e19c2973-6928-45da-9cee-ad992d4f1d45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow==2.22.0 databricks-sdk==0.40.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e396e04-7bb3-48a0-a507-fec6b76adb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "f7641b38-9ef4-460c-b997-07a9401d4cef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Deploying the model for batch inferences\n",
    "\n",
    "<img style=\"float: right; margin-left: 20px\" width=\"800\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/ep_model_serving_creation.gif?raw=true\" />\n",
    "\n",
    "Now that our model is available in the Registry, we can load it to compute our inferences and save them in a table to start building dashboards.\n",
    "\n",
    "We will use MLFlow function to load a pyspark UDF and distribute our inference in the entire cluster. If the data is small, we can also load the model with plain python and use a pandas Dataframe.\n",
    "\n",
    "If you don't know how to start, Databricks can generate a batch inference notebook in just one click from the model registry: Open MLFlow model registry and click the \"User model for inference\" button!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a55412e5-7579-4cb3-bd2d-6e958e15fc62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Scaling inferences using Spark \n",
    "We'll first see how it can be loaded as a spark UDF and called directly in a SQL function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9a8215d-4dc8-4b36-8a4f-e603af08349b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "#                                                                                                    Stage/version  \n",
    "#                                                                                       Model name         |        \n",
    "#                                                                                           |              |        \n",
    "predict_maintenance = mlflow.pyfunc.spark_udf(spark, f\"models:/{catalog}.{db}.dbdemos_turbine_maintenance@prod\", \"string\", env_manager='virtualenv')\n",
    "#We can use the function in SQL\n",
    "spark.udf.register(\"predict_maintenance\", predict_maintenance)\n",
    "columns = predict_maintenance.metadata.get_input_schema().input_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "218d104d-1eee-4945-89fa-528b9e14d9b4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run inferences"
    }
   },
   "outputs": [],
   "source": [
    "columns = predict_maintenance.metadata.get_input_schema().input_names()\n",
    "spark.table('turbine_hourly_features').withColumn(\"dbdemos_turbine_maintenance\", predict_maintenance(*columns)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "422f2349-7269-4d3f-a3fd-3372ac5226b7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Or in SQL directly"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT turbine_id, predict_maintenance(hourly_timestamp, avg_energy, std_sensor_A, std_sensor_B, std_sensor_C, std_sensor_D, std_sensor_E, std_sensor_F, location, model, state) as prediction FROM turbine_hourly_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40fe2d63-309d-4178-8c3e-c4cbf6423447",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Pure pandas inference\n",
    "If we have a small dataset, we can also compute our segment using a single node and pandas API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "693d9228-f27a-43c8-acb8-91e5767b739c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "local_path = ModelsArtifactRepository(f\"models:/{catalog}.{db}.dbdemos_turbine_maintenance@prod\").download_artifacts(\"\") # download model from remote registry\n",
    "\n",
    "requirements_path = os.path.join(local_path, \"requirements.txt\")\n",
    "if not os.path.exists(requirements_path):\n",
    "  dbutils.fs.put(\"file:\" + requirements_path, \"\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b160e77-37e5-4d3d-bc2d-05b6862944df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r $requirements_path\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e0ff8c5-1df2-4433-8d99-ee574c39c6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "898b4854-5644-465e-9e87-4014191caf40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "mlflow.set_registry_uri('databricks-uc')\n",
    "model = mlflow.pyfunc.load_model(f\"models:/{catalog}.{db}.dbdemos_turbine_maintenance@prod\")\n",
    "columns = model.metadata.get_input_schema().input_names()\n",
    "df = spark.table('turbine_hourly_features').select(*columns).limit(10).toPandas()\n",
    "df['churn_prediction'] = model.predict(df)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "3d3fe9e6-7fae-4930-a750-48c9677c8315",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Realtime model serving with Databricks serverless serving\n",
    "\n",
    "<img style=\"float: right; margin-left: 20px\" width=\"800\" src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/retail/lakehouse-churn/ep_model_serving_creation.gif?raw=true\" />\n",
    "\n",
    "Databricks also provides serverless serving.\n",
    "\n",
    "Click on model Serving, enable realtime serverless and your endpoint will be created, providing serving over REST api within a Click.\n",
    "\n",
    "Databricks Serverless offer autoscaling, including downscaling to zero when you don't have any traffic to offer best-in-class TCO while keeping low-latencies model serving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29efda70-7bb8-4265-8304-6f45f4841819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Real time model inference\n",
    "\n",
    "Let's now deploy our model behind a realtime model serving endpoint.\n",
    "\n",
    "We'll then use this endpoint in our GenAI Agentic demo to be able to fetch a turbine status in realtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd564410-314e-44d9-9874-b7eee34457ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")\n",
    "try:\n",
    "    endpoint = client.create_endpoint(\n",
    "        name=MODEL_SERVING_ENDPOINT_NAME,\n",
    "        config={\n",
    "            \"served_entities\": [\n",
    "                {\n",
    "                    \"name\": \"iot-maintenance-serving-endpoint\",\n",
    "                    \"entity_name\": f\"{catalog}.{db}.{model_name}\",\n",
    "                    \"entity_version\": get_last_model_version(f\"{catalog}.{db}.{model_name}\"),\n",
    "                    \"workload_size\": \"Small\",\n",
    "                    \"scale_to_zero_enabled\": True\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "except Exception as e:\n",
    "    if \"already exists\" in str(e):\n",
    "        print(f\"Endpoint {catalog}.{db}.{MODEL_SERVING_ENDPOINT_NAME} already exists. Skipping creation.\")\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "while client.get_endpoint(MODEL_SERVING_ENDPOINT_NAME)['state']['config_update'] == 'IN_PROGRESS':\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "357696ba-d69f-450b-b33c-089dc3fcc8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can now view the status of the Feature Serving Endpoint in the table on the **Serving endpoints** page. Click **Serving** in the sidebar to display the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff2425ea-5937-49d3-acee-3738a33fa6b0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Call the REST API deployed using standard python"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow import deployments\n",
    "\n",
    "\n",
    "def score_model(dataset):\n",
    "  client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "  predictions = client.predict(endpoint=MODEL_SERVING_ENDPOINT_NAME, inputs=dataset.to_dict(orient='split'))\n",
    "\n",
    "dataset = spark.table(f'turbine_hourly_features').select(*columns).limit(3).toPandas()\n",
    "#Deploy your model and uncomment to run your inferences live!\n",
    "#score_model(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41b9792c-6b42-4845-bf7d-5ea65f90baba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Next step: Leverage inferences and automate action to lower cost\n",
    "\n",
    "## Automate action to react on potential turbine failure\n",
    "\n",
    "We now have an end 2 end data pipeline analizing and predicting churn. We can now easily trigger actions to reduce downtime, such as dispatching a team earlier to fix the issue before an actual outage!\n",
    "\n",
    "## Track windturbine failure and impact\n",
    "\n",
    "Of course, this prediction can be re-used in our dashboard to analyse future failure and measure impact. \n",
    "\n",
    "<img width=\"800px\" src=\"https://github.com/databricks-demos/dbdemos-resources/raw/main/images/manufacturing/lakehouse-iot-turbine/lakehouse-manuf-iot-dashboard-2.png\">\n",
    "\n",
    "<a dbdemos-dashboard-id=\"turbine-predictive\" href=\"/sql/dashboardsv3/01f0624023b417aa91a2bab00ec4d4ba\">Open the Predictive Maintenance AI/BI dashboard</a> | [Go back to the introduction]($../00-IOT-wind-turbine-introduction-DI-platform)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "04.3-running-inference-iot-turbine",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
