{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99199586-1c1d-4aca-a9a4-f879dc0ceeae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&nbsp;\n",
    "&nbsp;\n",
    "![](../_resources/images/e2eai-5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "ed8876c9-27ae-4355-96fe-685a57d58395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# Generative AI with Databricks\n",
    "\n",
    "## From Predictive to Prescriptive Maintenance\n",
    "Manufacturers face labor shortages, supply chain disruptions, and rising costs, making efficient maintenance essential. Despite investments in maintenance programs, many struggle to boost asset productivity due to technician shortages and poor knowledge-sharing systems. This leads to knowledge loss and operational inefficiencies.\n",
    "\n",
    "<div style=\"font-family: 'DM Sans';\">\n",
    "  <div style=\"width: 400px; color: #1b3139; margin-left: 50px; margin-right: 50px; float: left;\">\n",
    "    <div style=\"color: #ff5f46; font-size:50px;\">73%</div>\n",
    "    <div style=\"font-size:25px; margin-top: -20px; line-height: 30px;\">\n",
    "      of manufacturers struggle to recruit maintenance technicians — McKinsey (2023)\n",
    "    </div>\n",
    "    <div style=\"color: #ff5f46; font-size:50px;\">55%</div>\n",
    "    <div style=\"font-size:25px; margin-top: -20px; line-height: 30px;\">\n",
    "      of manufacturers lack formal knowledge-sharing systems — McKinsey (2023)\n",
    "    </div>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "Generative AI can transform maintenance by reducing downtime and improving productivity. While predictive maintenance anticipates failures, Generative AI enables prescriptive maintenance. Using historical data, AI systems can identify issues, generate solutions, and assist technicians, allowing junior staff to perform effectively and freeing experts for complex tasks.\n",
    "<br><br>\n",
    "\n",
    "### From Models to Agent Systems\n",
    "Generative AI is moving from standalone models to modular agent systems ([Zaharia et al., 2024](https://bair.berkeley.edu/blog/2024/02/18/compound-ai-systems/)). These systems integrate retrievers, models, prompts, and tools to handle complex tasks. Their modular design allows seamless upgrades (e.g., integrating a new LLM) and adaptation to changing needs.\n",
    "\n",
    "<br>\n",
    "<img style=\"float: right; margin-top: 10px;\" width=\"700px\" src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/team_flow_liza.png\" />\n",
    "\n",
    "<br>\n",
    "<!div style=\"font-size: 19px; margin-left: 0px; clear: left; padding-top: 10px; \">\n",
    "\n",
    "**Databricks empowers users with a Data + AI platform for Prescriptive Maintenance.** \n",
    "Let’s explore how to deploy this in production.\n",
    "<br><br>\n",
    "<div style=\"font-size: 19px; margin-left: 0px; clear: left; padding-top: 10px; \">\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/liza.png\" style=\"width:80px\">\n",
    "<br>\n",
    "<h3 style=\"padding: 10px 0px 0px 5px;\">Liza, a Generative AI engineer, uses the Databricks Intelligence Platform to:</h3>\n",
    "<ul style=\"list-style: none; padding: 0; margin-left: 05%;\">\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">1</div>\n",
    "    Build real-time data pipelines\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">2</div>\n",
    "    Retrieve vectors & features\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">3</div>\n",
    "    Create AI agent tools\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">4</div>\n",
    "    Build & deploy agents\n",
    "  </li>\n",
    "  <li style=\"margin-bottom: 10px; display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">5</div>\n",
    "    Operate in batch or real-time\n",
    "  </li>\n",
    "  <li style=\"display: flex; align-items: center;\">\n",
    "    <div class=\"badge\" style=\"height: 30px; width: 30px; border-radius: 50%; background: #fcba33; color: white; text-align: center; line-height: 30px; font-weight: bold; margin-right: 10px;\">6</div>\n",
    "    Evaluate agent performance\n",
    "  </li>\n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=4003492105941350&notebook=%2F05-Generative-AI%2F05.1-ai-tools-iot-turbine-prescriptive-maintenance&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2F05-Generative-AI%2F05.1-ai-tools-iot-turbine-prescriptive-maintenance&version=1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "59e1e932-8e37-4dfc-94da-1e699932eb5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Building Agent Systems with Databricks Mosaic AI agent framework\n",
    "\n",
    "We will build an Agent System designed to generate prescriptive work orders for wind turbine maintenance technicians. This system integrates multiple interacting components to ensure proactive and efficient maintenance, thereby optimizing the overall equipment effectiveness.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/iot_agent_graph_v2_0.png\" style=\"margin-left: 5px; float: right\"  width=\"1000px;\">\n",
    "\n",
    "Databricks simplifies this by providing a built-in service to:\n",
    "\n",
    "- Create and store your AI tools leveraging UC functions\n",
    "- Execute the AI tools in a safe way\n",
    "- Use agents to reason about the tools you selected and chain them together to properly answer your question. \n",
    "\n",
    "\n",
    "This notebook creates the three Mosaic AI tools and associated Mosaic AI endpoints, which will be composed together into a agent in notebook [05.2-agent-creation-guide]($./05.2-agent-creation-guide).\n",
    "1. **Turbine specifications retriever** which retrieve the turbine specifications based on its id.\n",
    "2. **Turbine maintenance predictor** which uses a Model Serving endpoint to predict turbines at risk of failure.\n",
    "3. **Turbine maintenance guide**  which uses a Vector Search endpoint to retrieve maintenance guide based on the turbines and issues being adressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6760fd38-9a07-4df9-a04d-63e3b975e9ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required external libraries"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: databricks-vectorsearch==0.49 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (0.49)\nRequirement already satisfied: databricks-feature-engineering==0.8.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (0.8.0)\nRequirement already satisfied: databricks-sdk==0.40.0 in /databricks/python3/lib/python3.11/site-packages (0.40.0)\nRequirement already satisfied: mlflow-skinny<3,>=2.11.3 in /databricks/python3/lib/python3.11/site-packages (from databricks-vectorsearch==0.49) (2.11.4)\nRequirement already satisfied: protobuf<6,>=3.12.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from databricks-vectorsearch==0.49) (4.25.8)\nRequirement already satisfied: requests>=2 in /databricks/python3/lib/python3.11/site-packages (from databricks-vectorsearch==0.49) (2.31.0)\nRequirement already satisfied: deprecation>=2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from databricks-vectorsearch==0.49) (2.1.0)\nRequirement already satisfied: pyyaml<7,>=6 in /databricks/python3/lib/python3.11/site-packages (from databricks-feature-engineering==0.8.0) (6.0)\nRequirement already satisfied: boto3<2,>=1.16.7 in /databricks/python3/lib/python3.11/site-packages (from databricks-feature-engineering==0.8.0) (1.34.39)\nRequirement already satisfied: dbl-tempo<1,>=0.1.26 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from databricks-feature-engineering==0.8.0) (0.1.30)\nRequirement already satisfied: azure-cosmos==4.3.1 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from databricks-feature-engineering==0.8.0) (4.3.1)\nRequirement already satisfied: numpy<2,>=1.19.2 in /databricks/python3/lib/python3.11/site-packages (from databricks-feature-engineering==0.8.0) (1.23.5)\nRequirement already satisfied: flask<3,>=1.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from databricks-feature-engineering==0.8.0) (2.3.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-feature-engineering==0.8.0) (0.5.1)\nRequirement already satisfied: google-auth~=2.0 in /databricks/python3/lib/python3.11/site-packages (from databricks-sdk==0.40.0) (2.35.0)\nRequirement already satisfied: azure-core<2.0.0,>=1.23.0 in /databricks/python3/lib/python3.11/site-packages (from azure-cosmos==4.3.1->databricks-feature-engineering==0.8.0) (1.32.0)\nRequirement already satisfied: botocore<1.35.0,>=1.34.39 in /databricks/python3/lib/python3.11/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering==0.8.0) (1.34.39)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /databricks/python3/lib/python3.11/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering==0.8.0) (0.10.0)\nRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /databricks/python3/lib/python3.11/site-packages (from boto3<2,>=1.16.7->databricks-feature-engineering==0.8.0) (0.10.3)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.11/site-packages (from deprecation>=2->databricks-vectorsearch==0.49) (23.2)\nRequirement already satisfied: Werkzeug>=2.3.7 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.0) (3.1.3)\nRequirement already satisfied: Jinja2>=3.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.0) (3.1.6)\nRequirement already satisfied: itsdangerous>=2.1.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.0) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.0) (8.3.0)\nRequirement already satisfied: blinker>=1.6.2 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from flask<3,>=1.1.2->databricks-feature-engineering==0.8.0) (1.9.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.40.0) (5.5.0)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.40.0) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4 in /databricks/python3/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk==0.40.0) (4.9)\nRequirement already satisfied: cloudpickle<4 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.49) (3.0.0)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.49) (0.4)\nRequirement already satisfied: gitpython<4,>=3.1.9 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.49) (3.1.43)\nRequirement already satisfied: pytz<2025 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.49) (2022.7)\nRequirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.49) (6.0.0)\nRequirement already satisfied: azure-storage-file-datalake>12 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (12.14.0)\nRequirement already satisfied: google-cloud-storage>=1.30.0 in /databricks/python3/lib/python3.11/site-packages (from mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (2.18.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.11/site-packages (from requests>=2->databricks-vectorsearch==0.49) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.11/site-packages (from requests>=2->databricks-vectorsearch==0.49) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.11/site-packages (from requests>=2->databricks-vectorsearch==0.49) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.11/site-packages (from requests>=2->databricks-vectorsearch==0.49) (2023.7.22)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core<2.0.0,>=1.23.0->azure-cosmos==4.3.1->databricks-feature-engineering==0.8.0) (1.16.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /databricks/python3/lib/python3.11/site-packages (from azure-core<2.0.0,>=1.23.0->azure-cosmos==4.3.1->databricks-feature-engineering==0.8.0) (4.10.0)\nRequirement already satisfied: azure-storage-blob<13.0.0,>=12.19.0 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (12.19.1)\nRequirement already satisfied: isodate>=0.6.1 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (0.7.2)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /databricks/python3/lib/python3.11/site-packages (from botocore<1.35.0,>=1.34.39->boto3<2,>=1.16.7->databricks-feature-engineering==0.8.0) (2.8.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.49) (4.0.11)\nRequirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (2.18.0)\nRequirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (2.4.1)\nRequirement already satisfied: google-resumable-media>=2.7.2 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (2.7.2)\nRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /databricks/python3/lib/python3.11/site-packages (from google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (1.6.0)\nRequirement already satisfied: zipp>=0.5 in /databricks/python3/lib/python3.11/site-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.49) (3.11.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /local_disk0/.ephemeral_nfs/envs/pythonEnv-d6d1c74a-5aa2-4d69-9bee-7d585b78f12c/lib/python3.11/site-packages (from Jinja2>=3.1.2->flask<3,>=1.1.2->databricks-feature-engineering==0.8.0) (3.0.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /databricks/python3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk==0.40.0) (0.4.8)\nRequirement already satisfied: cryptography>=2.1.4 in /databricks/python3/lib/python3.11/site-packages (from azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (41.0.3)\nRequirement already satisfied: smmap<6,>=3.0.1 in /databricks/python3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=2.11.3->databricks-vectorsearch==0.49) (5.0.1)\nRequirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /databricks/python3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (1.65.0)\nRequirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /databricks/python3/lib/python3.11/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage>=1.30.0->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (1.25.0)\nRequirement already satisfied: cffi>=1.12 in /databricks/python3/lib/python3.11/site-packages (from cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (1.15.1)\nRequirement already satisfied: pycparser in /databricks/python3/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob<13.0.0,>=12.19.0->azure-storage-file-datalake>12->mlflow-skinny[databricks]<3,>=2.11.0->databricks-feature-engineering==0.8.0) (2.21)\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install databricks-vectorsearch==0.49 databricks-feature-engineering==0.8.0 databricks-sdk==0.40.0 \n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8edbbc63-4184-4f4f-9a32-631fc0387c0e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initializing the Application"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "## Configuration file\n",
       "\n",
       "Please change your catalog and schema here to run the demo on a different catalog.\n",
       "\n",
       " \n",
       "<!-- Collect usage data (view). Remove it to disable collection. View README for more details.  -->\n",
       "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=lakehouse&org_id=4003492105941350&notebook=%2Fconfig&demo_name=lakehouse-iot-platform&event=VIEW&path=%2F_dbdemos%2Flakehouse%2Flakehouse-iot-platform%2Fconfig&version=1\">"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": [
       "\n",
       "# Technical Setup notebook. Hide this cell results\n",
       "Initialize dataset to the current user and cleanup data when reset_all_data is set to true\n",
       "\n",
       "Do not edit"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USE CATALOG `main`\nusing catalog.database `main`.`e2eai_iot_turbine`\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data already existing. Run with reset_all_data=true to force a data cleanup for your local demo.\n"
     ]
    }
   ],
   "source": [
    "%run ../_resources/00-setup $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3229a89d-eb03-452a-8774-cfd94fb142fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 1: Create the Turbine Specification Retriever as a tool to return sensor readings for a turbine\n",
    "\n",
    "Edit the FROM table if you changed from the default catalog/schema in your config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d1bd0a1-b9aa-49ba-ad74-5054fd20170b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP FUNCTION IF EXISTS turbine_specifications_retriever;\n",
    "\n",
    "--turbine_specifications_retriever to get the current status of a turbine\n",
    "--This function is used to retrieve the turbine specifications based on its id\n",
    "\n",
    "CREATE OR REPLACE FUNCTION \n",
    "turbine_specifications_retriever(turbine_id STRING COMMENT 'ID of the wind turbine to look up')\n",
    "RETURNS TABLE (\n",
    "  avg_energy DOUBLE COMMENT 'Average energy reading',\n",
    "  std_sensor_A DOUBLE COMMENT 'Sensor A reading',\n",
    "  std_sensor_B DOUBLE COMMENT 'Sensor B reading',\n",
    "  std_sensor_C DOUBLE COMMENT 'Sensor C reading',\n",
    "  std_sensor_D DOUBLE COMMENT 'Sensor D reading',\n",
    "  std_sensor_E DOUBLE COMMENT 'Sensor E reading',\n",
    "  std_sensor_F DOUBLE COMMENT 'Sensor F reading'\n",
    ")\n",
    "LANGUAGE SQL\n",
    "COMMENT 'This function retrieves the turbine sensor readings / specifications based on the turbine_id'\n",
    "RETURN\n",
    "(\n",
    "SELECT \n",
    "\n",
    "avg_energy, std_sensor_A, std_sensor_B, std_sensor_C, std_sensor_D, std_sensor_E, std_sensor_F\n",
    "FROM main.e2eai_iot_turbine.turbine_current_features\n",
    "WHERE turbine_id = turbine_specifications_retriever.turbine_id\n",
    "SORT BY hourly_timestamp DESC\n",
    "limit 1\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51a1fd0b-7e54-4a09-800d-d0dedfd376b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, test our tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1832d7d-cf2b-4531-92ba-60f0046f91a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>avg_energy</th><th>std_sensor_A</th><th>std_sensor_B</th><th>std_sensor_C</th><th>std_sensor_D</th><th>std_sensor_E</th><th>std_sensor_F</th></tr></thead><tbody><tr><td>0.074818609038791</td><td>1.058048335093487</td><td>2.4852932716249665</td><td>2.8927160852893152</td><td>2.1567050955955853</td><td>2.2120358529793696</td><td>5.614526027139428</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         0.074818609038791,
         1.058048335093487,
         2.4852932716249665,
         2.8927160852893152,
         2.1567050955955853,
         2.2120358529793696,
         5.614526027139428
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "avg_energy",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "std_sensor_A",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "std_sensor_B",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "std_sensor_C",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "std_sensor_D",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "std_sensor_E",
            "nullable": true,
            "type": "double"
           },
           {
            "metadata": {},
            "name": "std_sensor_F",
            "nullable": true,
            "type": "double"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 14
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "avg_energy",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_A",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_B",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_C",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_D",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_E",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_F",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM turbine_specifications_retriever('004a641f-e9e5-9fff-d421-1bf88319420b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "fd9c59e1-2893-43cd-83cd-d4599c908242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 2: Create the Turbine Predictor as a tool to predict turbine failure\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/iot_agent_graph_v2_1.png\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "To enable our Agent System to predict turbine failtures based on industrial IoT sensor readings, we will rely on the model we deployed previously in the  [./04.3-running-inference-iot-turbine]($./04.3-running-inference-iot-turbine) notebook. \n",
    "\n",
    "**Make sure you run this ML notebook to create the model serving endpoint!**\n",
    "\n",
    "\n",
    "### Using the Model Serving as tool to predict faulty turbines\n",
    "Let's define the turbine predictor tool function our LLM agent will be able to execute. \n",
    "\n",
    "AI agents use [AI Agent Tools](https://docs.databricks.com/en/generative-ai/create-log-agent.html#create-ai-agent-tools) to perform actions besides language generation, for example to retrieve structured or unstructured data, execute code, or talk to remote services (e.g. send an email or Slack message). \n",
    "\n",
    "These functions can contain any logic, from simple SQL to advanced python. Below we wrap the model serving endpoint in a SQL function using '[ai_query](https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html)' function, as we tested in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecf76199-664f-4c8a-9503-801eda2c0f81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7947322644040229>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDROP FUNCTION IF EXISTS turbine_maintenance_predictor;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m--Use turbine_maintenance_predictor to get a prediction of whether or not a turbine sensor is faulty to facilitate proactive maintenance\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m--This function is used to predict turbine maintenance based on energy and sensor readings\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCREATE OR REPLACE FUNCTION \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mturbine_maintenance_predictor(avg_energy DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_A DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_B DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_C DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_D DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_E DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_F DOUBLE\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mRETURNS STRING\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mLANGUAGE SQL\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCOMMENT \u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mThis tool predicts whether or not a turbine is faulty to facilitate proactive maintenance. It expects an array of 7 double values (energy and sensor readings) as input and returns a string indicating which sensor is predicted to be faulty or if all sensors are ok.\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mRETURN\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    SELECT \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        -- The xgboost model returns a float; translate it back to a string\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        CASE WHEN float_prediction=0 THEN \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            WHEN float_prediction=1 THEN \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mok\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            WHEN float_prediction=2 THEN \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            WHEN float_prediction=3 THEN \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        ELSE \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfaulty\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m END AS prediction    \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    FROM (\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    SELECT ai_query(\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124me2eai_iot_turbine_prediction_endpoint\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        STRUCT(avg_energy AS avg_energy,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_A AS std_sensor_A,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_B AS std_sensor_B,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_C AS std_sensor_C,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_D AS std_sensor_D,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_E AS std_sensor_E,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_F AS std_sensor_F\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        ),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        \u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mFLOAT\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    ) AS float_prediction)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m);\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:152\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m    149\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    150\u001B[0m     )\n",
       "\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_py4j:\n",
       "\u001B[0;32m--> 152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_py4j(cell)\n",
       "\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_sql_comm_handler(cell)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:237\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n",
       "\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    236\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
       "\u001B[0;32m--> 237\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    238\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:233\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n",
       "\u001B[1;32m    231\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuting subquery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stdout__)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
       "\u001B[1;32m    232\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n",
       "\u001B[0;32m--> 233\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    236\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:257\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n",
       "\u001B[1;32m    254\u001B[0m query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n",
       "\u001B[1;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(\n",
       "\u001B[1;32m    256\u001B[0m     query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mescapeWidgetValues(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()))\n",
       "\u001B[0;32m--> 257\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_query_request_result(query)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# E.g. \"create or replace temp view abc as select 1\" gets us here and we should not return a result in this case.\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:129\u001B[0m, in \u001B[0;36mSqlMagic.get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n",
       "\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m    128\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 129\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n",
       "\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:736\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    733\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n",
       "\u001B[1;32m    735\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001B[0;32m--> 736\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n",
       "\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m    738\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1305\u001B[0m )\n",
       "\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1763\u001B[0m     ):\n",
       "\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n",
       "\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2133\u001B[0m                 info,\n",
       "\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mSparkException\u001B[0m: [REMOTE_FUNCTION_HTTP_FAILED_ERROR] The remote HTTP request failed with code 403, and error message 'HTTP request failed with status: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"This feature is either not supported or it has been disabled explicitly for ai_query. Check requirements in https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html. Please fix the problem indicated in the error message and retry.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"a60028a7-a23e-41e7-9e0c-5767bd9ba4f8\",\"serving_data\":\"\"}]}' SQLSTATE: 57012\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.SparkException\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.remoteHttpFailedError(QueryExecutionErrors.scala:2180)\n",
       "\tat com.databricks.sql.util.RemoteHttpClient$.postRawResponse(RemoteHttpClient.scala:124)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getEndpointAPI(AIFunctionsUtils.scala:817)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getModelServingEndpoint(AIFunctionsUtils.scala:842)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider$lzycompute(AIQuery.scala:1111)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider(AIQuery.scala:1099)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl(AIQuery.scala:270)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl$(AIQuery.scala:270)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl$lzycompute(AIQuery.scala:1084)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl(AIQuery.scala:1084)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes(AIQuery.scala:325)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes$(AIQuery.scala:282)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.checkInputDataTypes(AIQuery.scala:1084)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n",
       "\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n",
       "\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n",
       "\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n",
       "\tat scala.collection.immutable.List.map(List.scala:247)\n",
       "\tat scala.collection.immutable.List.map(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2423)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3286)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:454)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3286)\n",
       "\tat com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3285)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3343)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3320)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:191)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n",
       "\tat scala.collection.immutable.List.map(List.scala:247)\n",
       "\tat scala.collection.immutable.List.map(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:191)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:162)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:277)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:275)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:496)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:275)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3320)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3394)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3384)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3384)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3215)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n",
       "\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$7(CreateSQLFunctionCommand.scala:174)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$3(CreateSQLFunctionCommand.scala:174)\n",
       "\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.recordAnalysisError(CreateSQLFunctionCommand.scala:329)\n",
       "\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.run(CreateSQLFunctionCommand.scala:107)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:505)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:504)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$17(SQLExecution.scala:583)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:496)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:898)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:418)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:418)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:933)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:417)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:240)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:851)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:500)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:496)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:438)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:494)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:597)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:589)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:506)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:589)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:589)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:394)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:399)\n",
       "\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\n",
       "Caused by: com.databricks.sql.util.UnexpectedHttpStatus: HTTP request failed with status: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"This feature is either not supported or it has been disabled explicitly for ai_query. Check requirements in https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html. Please fix the problem indicated in the error message and retry.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"a60028a7-a23e-41e7-9e0c-5767bd9ba4f8\",\"serving_data\":\"\"}]}\n",
       "\tat com.databricks.sql.util.RemoteHttpClient.$anonfun$sendRequestInternal$6(RemoteHttpClient.scala:411)\n",
       "\tat com.databricks.sql.util.RemoteHttpClient.$anonfun$sendRequestInternal$6$adapted(RemoteHttpClient.scala:395)\n",
       "\tat com.databricks.sql.util.RetryUtils$.$anonfun$runWithExponentialBackoffRetryWithCount$1(RetryUtils.scala:182)\n",
       "\tat com.databricks.sql.util.RetryUtils$.$anonfun$runWithExponentialBackoffRetryWithCount$1$adapted(RetryUtils.scala:181)\n",
       "\tat com.databricks.sql.util.RetryWorkerImpl.$anonfun$runWithExponentialBackoffRetry$2(RetryUtils.scala:90)\n",
       "\tat com.databricks.sql.util.RetryWorkerImpl.$anonfun$runWithExponentialBackoffRetry$2$adapted(RetryUtils.scala:89)\n",
       "\tat com.databricks.sql.util.RetryWorkerImpl.runWithExponentialBackoffRetryInternal(RetryUtils.scala:103)\n",
       "\tat com.databricks.sql.util.RetryWorkerImpl.runWithExponentialBackoffRetry(RetryUtils.scala:89)\n",
       "\tat com.databricks.sql.util.RetryUtils$.runWithExponentialBackoffRetryWithCount(RetryUtils.scala:181)\n",
       "\tat com.databricks.sql.util.RemoteHttpClient.sendRequestInternal(RemoteHttpClient.scala:395)\n",
       "\tat com.databricks.sql.util.RemoteHttpClient.sendRequest(RemoteHttpClient.scala:325)\n",
       "\tat com.databricks.sql.util.RemoteHttpClient$.postRawResponse(RemoteHttpClient.scala:112)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getEndpointAPI(AIFunctionsUtils.scala:817)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getModelServingEndpoint(AIFunctionsUtils.scala:842)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider$lzycompute(AIQuery.scala:1111)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider(AIQuery.scala:1099)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl(AIQuery.scala:270)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl$(AIQuery.scala:270)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl$lzycompute(AIQuery.scala:1084)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl(AIQuery.scala:1084)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes(AIQuery.scala:325)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes$(AIQuery.scala:282)\n",
       "\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.checkInputDataTypes(AIQuery.scala:1084)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n",
       "\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n",
       "\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n",
       "\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n",
       "\tat scala.collection.immutable.List.map(List.scala:247)\n",
       "\tat scala.collection.immutable.List.map(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2423)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3286)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:454)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3286)\n",
       "\tat com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3285)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3343)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3320)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:191)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n",
       "\tat scala.collection.immutable.List.map(List.scala:247)\n",
       "\tat scala.collection.immutable.List.map(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:191)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:162)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:277)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:275)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:496)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:275)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3320)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3394)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3384)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3384)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3215)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n",
       "\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$7(CreateSQLFunctionCommand.scala:174)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$3(CreateSQLFunctionCommand.scala:174)\n",
       "\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.recordAnalysisError(CreateSQLFunctionCommand.scala:329)\n",
       "\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.run(CreateSQLFunctionCommand.scala:107)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:505)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:505)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:504)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$17(SQLExecution.scala:583)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:496)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:898)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:418)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:418)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:933)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:417)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:240)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:851)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:500)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:496)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:438)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:494)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:597)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:589)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:506)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:589)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:589)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:394)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:399)\n",
       "\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n",
       "\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n",
       "\tat scala.util.Using$.resource(Using.scala:296)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "SparkException",
        "evalue": "[REMOTE_FUNCTION_HTTP_FAILED_ERROR] The remote HTTP request failed with code 403, and error message 'HTTP request failed with status: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"This feature is either not supported or it has been disabled explicitly for ai_query. Check requirements in https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html. Please fix the problem indicated in the error message and retry.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"a60028a7-a23e-41e7-9e0c-5767bd9ba4f8\",\"serving_data\":\"\"}]}' SQLSTATE: 57012\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.remoteHttpFailedError(QueryExecutionErrors.scala:2180)\n\tat com.databricks.sql.util.RemoteHttpClient$.postRawResponse(RemoteHttpClient.scala:124)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getEndpointAPI(AIFunctionsUtils.scala:817)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getModelServingEndpoint(AIFunctionsUtils.scala:842)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider$lzycompute(AIQuery.scala:1111)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider(AIQuery.scala:1099)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl$(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl$lzycompute(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes(AIQuery.scala:325)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes$(AIQuery.scala:282)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.checkInputDataTypes(AIQuery.scala:1084)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2423)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:454)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3286)\n\tat com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3285)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3343)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:162)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:496)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3394)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3215)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$7(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$3(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.recordAnalysisError(CreateSQLFunctionCommand.scala:329)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.run(CreateSQLFunctionCommand.scala:107)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:505)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:505)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:504)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$17(SQLExecution.scala:583)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:496)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:898)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:418)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:418)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:933)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:417)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:240)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:851)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:500)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:438)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:597)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:506)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:589)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:394)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:399)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nCaused by: com.databricks.sql.util.UnexpectedHttpStatus: HTTP request failed with status: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"This feature is either not supported or it has been disabled explicitly for ai_query. Check requirements in https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html. Please fix the problem indicated in the error message and retry.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"a60028a7-a23e-41e7-9e0c-5767bd9ba4f8\",\"serving_data\":\"\"}]}\n\tat com.databricks.sql.util.RemoteHttpClient.$anonfun$sendRequestInternal$6(RemoteHttpClient.scala:411)\n\tat com.databricks.sql.util.RemoteHttpClient.$anonfun$sendRequestInternal$6$adapted(RemoteHttpClient.scala:395)\n\tat com.databricks.sql.util.RetryUtils$.$anonfun$runWithExponentialBackoffRetryWithCount$1(RetryUtils.scala:182)\n\tat com.databricks.sql.util.RetryUtils$.$anonfun$runWithExponentialBackoffRetryWithCount$1$adapted(RetryUtils.scala:181)\n\tat com.databricks.sql.util.RetryWorkerImpl.$anonfun$runWithExponentialBackoffRetry$2(RetryUtils.scala:90)\n\tat com.databricks.sql.util.RetryWorkerImpl.$anonfun$runWithExponentialBackoffRetry$2$adapted(RetryUtils.scala:89)\n\tat com.databricks.sql.util.RetryWorkerImpl.runWithExponentialBackoffRetryInternal(RetryUtils.scala:103)\n\tat com.databricks.sql.util.RetryWorkerImpl.runWithExponentialBackoffRetry(RetryUtils.scala:89)\n\tat com.databricks.sql.util.RetryUtils$.runWithExponentialBackoffRetryWithCount(RetryUtils.scala:181)\n\tat com.databricks.sql.util.RemoteHttpClient.sendRequestInternal(RemoteHttpClient.scala:395)\n\tat com.databricks.sql.util.RemoteHttpClient.sendRequest(RemoteHttpClient.scala:325)\n\tat com.databricks.sql.util.RemoteHttpClient$.postRawResponse(RemoteHttpClient.scala:112)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getEndpointAPI(AIFunctionsUtils.scala:817)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getModelServingEndpoint(AIFunctionsUtils.scala:842)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider$lzycompute(AIQuery.scala:1111)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider(AIQuery.scala:1099)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl$(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl$lzycompute(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes(AIQuery.scala:325)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes$(AIQuery.scala:282)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.checkInputDataTypes(AIQuery.scala:1084)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2423)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:454)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3286)\n\tat com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3285)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3343)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:162)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:496)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3394)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3215)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$7(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$3(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.recordAnalysisError(CreateSQLFunctionCommand.scala:329)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.run(CreateSQLFunctionCommand.scala:107)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:505)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:505)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:504)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$17(SQLExecution.scala:583)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:496)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:898)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:418)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:418)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:933)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:417)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:240)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:851)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:500)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:438)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:597)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:506)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:589)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:394)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:399)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
       },
       "metadata": {
        "errorSummary": "[REMOTE_FUNCTION_HTTP_FAILED_ERROR] The remote HTTP request failed with code 403, and error message 'HTTP request failed with status: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"This feature is either not supported or it has been disabled explicitly for ai_query. Check requirements in https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html. Please fix the problem indicated in the error message and retry.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"a60028a7-a23e-41e7-9e0c-5767bd9ba4f8\",\"serving_data\":\"\"}]}' SQLSTATE: 57012"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "REMOTE_FUNCTION_HTTP_FAILED_ERROR",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "57012",
        "stackTrace": "org.apache.spark.SparkException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.remoteHttpFailedError(QueryExecutionErrors.scala:2180)\n\tat com.databricks.sql.util.RemoteHttpClient$.postRawResponse(RemoteHttpClient.scala:124)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getEndpointAPI(AIFunctionsUtils.scala:817)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getModelServingEndpoint(AIFunctionsUtils.scala:842)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider$lzycompute(AIQuery.scala:1111)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider(AIQuery.scala:1099)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl$(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl$lzycompute(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes(AIQuery.scala:325)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes$(AIQuery.scala:282)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.checkInputDataTypes(AIQuery.scala:1084)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2423)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:454)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3286)\n\tat com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3285)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3343)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:162)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:496)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3394)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3215)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$7(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$3(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.recordAnalysisError(CreateSQLFunctionCommand.scala:329)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.run(CreateSQLFunctionCommand.scala:107)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:505)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:505)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:504)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$17(SQLExecution.scala:583)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:496)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:898)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:418)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:418)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:933)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:417)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:240)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:851)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:500)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:438)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:597)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:506)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:589)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:394)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:399)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nCaused by: com.databricks.sql.util.UnexpectedHttpStatus: HTTP request failed with status: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"This feature is either not supported or it has been disabled explicitly for ai_query. Check requirements in https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html. Please fix the problem indicated in the error message and retry.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"a60028a7-a23e-41e7-9e0c-5767bd9ba4f8\",\"serving_data\":\"\"}]}\n\tat com.databricks.sql.util.RemoteHttpClient.$anonfun$sendRequestInternal$6(RemoteHttpClient.scala:411)\n\tat com.databricks.sql.util.RemoteHttpClient.$anonfun$sendRequestInternal$6$adapted(RemoteHttpClient.scala:395)\n\tat com.databricks.sql.util.RetryUtils$.$anonfun$runWithExponentialBackoffRetryWithCount$1(RetryUtils.scala:182)\n\tat com.databricks.sql.util.RetryUtils$.$anonfun$runWithExponentialBackoffRetryWithCount$1$adapted(RetryUtils.scala:181)\n\tat com.databricks.sql.util.RetryWorkerImpl.$anonfun$runWithExponentialBackoffRetry$2(RetryUtils.scala:90)\n\tat com.databricks.sql.util.RetryWorkerImpl.$anonfun$runWithExponentialBackoffRetry$2$adapted(RetryUtils.scala:89)\n\tat com.databricks.sql.util.RetryWorkerImpl.runWithExponentialBackoffRetryInternal(RetryUtils.scala:103)\n\tat com.databricks.sql.util.RetryWorkerImpl.runWithExponentialBackoffRetry(RetryUtils.scala:89)\n\tat com.databricks.sql.util.RetryUtils$.runWithExponentialBackoffRetryWithCount(RetryUtils.scala:181)\n\tat com.databricks.sql.util.RemoteHttpClient.sendRequestInternal(RemoteHttpClient.scala:395)\n\tat com.databricks.sql.util.RemoteHttpClient.sendRequest(RemoteHttpClient.scala:325)\n\tat com.databricks.sql.util.RemoteHttpClient$.postRawResponse(RemoteHttpClient.scala:112)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getEndpointAPI(AIFunctionsUtils.scala:817)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getModelServingEndpoint(AIFunctionsUtils.scala:842)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider$lzycompute(AIQuery.scala:1111)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider(AIQuery.scala:1099)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl$(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl$lzycompute(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes(AIQuery.scala:325)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes$(AIQuery.scala:282)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.checkInputDataTypes(AIQuery.scala:1084)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2423)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:454)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3286)\n\tat com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3285)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3343)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:162)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:496)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3394)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3215)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spar",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mSparkException\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-7947322644040229>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mDROP FUNCTION IF EXISTS turbine_maintenance_predictor;\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m--Use turbine_maintenance_predictor to get a prediction of whether or not a turbine sensor is faulty to facilitate proactive maintenance\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m--This function is used to predict turbine maintenance based on energy and sensor readings\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCREATE OR REPLACE FUNCTION \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mturbine_maintenance_predictor(avg_energy DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_A DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_B DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_C DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_D DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_E DOUBLE, \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m                              std_sensor_F DOUBLE\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mRETURNS STRING\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mLANGUAGE SQL\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mCOMMENT \u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mThis tool predicts whether or not a turbine is faulty to facilitate proactive maintenance. It expects an array of 7 double values (energy and sensor readings) as input and returns a string indicating which sensor is predicted to be faulty or if all sensors are ok.\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mRETURN\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    SELECT \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        -- The xgboost model returns a float; translate it back to a string\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        CASE WHEN float_prediction=0 THEN \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            WHEN float_prediction=1 THEN \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mok\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            WHEN float_prediction=2 THEN \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mB\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            WHEN float_prediction=3 THEN \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mD\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        ELSE \u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfaulty\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m END AS prediction    \u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    FROM (\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    SELECT ai_query(\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124me2eai_iot_turbine_prediction_endpoint\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124m,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        STRUCT(avg_energy AS avg_energy,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_A AS std_sensor_A,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_B AS std_sensor_B,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_C AS std_sensor_C,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_D AS std_sensor_D,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_E AS std_sensor_E,\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m            std_sensor_F AS std_sensor_F\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        ),\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m        \u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;124mFLOAT\u001B[39m\u001B[38;5;130;01m\\'\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    ) AS float_prediction)\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m);\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:152\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m    149\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    150\u001B[0m     )\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_py4j:\n\u001B[0;32m--> 152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_py4j(cell)\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_sql_comm_handler(cell)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:237\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    236\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n\u001B[0;32m--> 237\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:233\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n\u001B[1;32m    231\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuting subquery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stdout__)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    232\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n\u001B[0;32m--> 233\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    236\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:257\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n\u001B[1;32m    254\u001B[0m query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(\n\u001B[1;32m    256\u001B[0m     query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mescapeWidgetValues(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()))\n\u001B[0;32m--> 257\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_query_request_result(query)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# E.g. \"create or replace temp view abc as select 1\" gets us here and we should not return a result in this case.\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:129\u001B[0m, in \u001B[0;36mSqlMagic.get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 129\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:736\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    733\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    735\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 736\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    738\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1305\u001B[0m )\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1763\u001B[0m     ):\n\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2133\u001B[0m                 info,\n\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mSparkException\u001B[0m: [REMOTE_FUNCTION_HTTP_FAILED_ERROR] The remote HTTP request failed with code 403, and error message 'HTTP request failed with status: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"This feature is either not supported or it has been disabled explicitly for ai_query. Check requirements in https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html. Please fix the problem indicated in the error message and retry.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"a60028a7-a23e-41e7-9e0c-5767bd9ba4f8\",\"serving_data\":\"\"}]}' SQLSTATE: 57012\n\nJVM stacktrace:\norg.apache.spark.SparkException\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.remoteHttpFailedError(QueryExecutionErrors.scala:2180)\n\tat com.databricks.sql.util.RemoteHttpClient$.postRawResponse(RemoteHttpClient.scala:124)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getEndpointAPI(AIFunctionsUtils.scala:817)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getModelServingEndpoint(AIFunctionsUtils.scala:842)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider$lzycompute(AIQuery.scala:1111)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider(AIQuery.scala:1099)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl$(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl$lzycompute(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes(AIQuery.scala:325)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes$(AIQuery.scala:282)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.checkInputDataTypes(AIQuery.scala:1084)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2423)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:454)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3286)\n\tat com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3285)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3343)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:162)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:496)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3394)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3215)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$7(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$3(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.recordAnalysisError(CreateSQLFunctionCommand.scala:329)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.run(CreateSQLFunctionCommand.scala:107)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:505)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:505)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:504)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$17(SQLExecution.scala:583)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:496)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:898)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:418)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:418)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:933)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:417)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:240)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:851)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:500)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:438)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:597)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:506)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:589)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:394)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:399)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)\nCaused by: com.databricks.sql.util.UnexpectedHttpStatus: HTTP request failed with status: {\"error_code\":\"PERMISSION_DENIED\",\"message\":\"This feature is either not supported or it has been disabled explicitly for ai_query. Check requirements in https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html. Please fix the problem indicated in the error message and retry.\",\"details\":[{\"@type\":\"type.googleapis.com/google.rpc.RequestInfo\",\"request_id\":\"a60028a7-a23e-41e7-9e0c-5767bd9ba4f8\",\"serving_data\":\"\"}]}\n\tat com.databricks.sql.util.RemoteHttpClient.$anonfun$sendRequestInternal$6(RemoteHttpClient.scala:411)\n\tat com.databricks.sql.util.RemoteHttpClient.$anonfun$sendRequestInternal$6$adapted(RemoteHttpClient.scala:395)\n\tat com.databricks.sql.util.RetryUtils$.$anonfun$runWithExponentialBackoffRetryWithCount$1(RetryUtils.scala:182)\n\tat com.databricks.sql.util.RetryUtils$.$anonfun$runWithExponentialBackoffRetryWithCount$1$adapted(RetryUtils.scala:181)\n\tat com.databricks.sql.util.RetryWorkerImpl.$anonfun$runWithExponentialBackoffRetry$2(RetryUtils.scala:90)\n\tat com.databricks.sql.util.RetryWorkerImpl.$anonfun$runWithExponentialBackoffRetry$2$adapted(RetryUtils.scala:89)\n\tat com.databricks.sql.util.RetryWorkerImpl.runWithExponentialBackoffRetryInternal(RetryUtils.scala:103)\n\tat com.databricks.sql.util.RetryWorkerImpl.runWithExponentialBackoffRetry(RetryUtils.scala:89)\n\tat com.databricks.sql.util.RetryUtils$.runWithExponentialBackoffRetryWithCount(RetryUtils.scala:181)\n\tat com.databricks.sql.util.RemoteHttpClient.sendRequestInternal(RemoteHttpClient.scala:395)\n\tat com.databricks.sql.util.RemoteHttpClient.sendRequest(RemoteHttpClient.scala:325)\n\tat com.databricks.sql.util.RemoteHttpClient$.postRawResponse(RemoteHttpClient.scala:112)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getEndpointAPI(AIFunctionsUtils.scala:817)\n\tat com.databricks.sql.catalyst.expressions.ai.AIFunctionsUtils$.getModelServingEndpoint(AIFunctionsUtils.scala:842)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider$lzycompute(AIQuery.scala:1111)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.modelProvider(AIQuery.scala:1099)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.endpointUrl$(AIQuery.scala:270)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl$lzycompute(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.endpointUrl(AIQuery.scala:1084)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes(AIQuery.scala:325)\n\tat com.databricks.sql.catalyst.expressions.ai.AIQueryBase.checkInputDataTypes$(AIQuery.scala:282)\n\tat com.databricks.sql.catalyst.expressions.ai.UnresolvedAIQuery.checkInputDataTypes(AIQuery.scala:1084)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.SubqueryAlias.mapChildren(basicLogicalOperators.scala:2423)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.plans.logical.Project.mapChildren(basicLogicalOperators.scala:94)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$2(Analyzer.scala:3286)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withOuterPlan(Analyzer.scala:454)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.$anonfun$resolveSubQuery$1(Analyzer.scala:3286)\n\tat com.databricks.sql.analyzer.SQLFunctionContext$.withNewContext(SQLFunctionExpression.scala:115)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQuery(Analyzer.scala:3285)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3343)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries$1.applyOrElse(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.expressions.UnaryExpression.mapChildren(Expression.scala:759)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:535)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:191)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:162)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:277)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressionsWithPruning$1.applyOrElse(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:496)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressionsWithPruning(QueryPlan.scala:275)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning(AnalysisHelper.scala:386)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformAllExpressionsWithPruning$(AnalysisHelper.scala:381)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformAllExpressionsWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveSubquery$$resolveSubQueries(Analyzer.scala:3320)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3394)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$$anonfun$apply$25.applyOrElse(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3384)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveSubquery$.apply(Analyzer.scala:3215)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$7(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.$anonfun$run$3(CreateSQLFunctionCommand.scala:174)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.recordAnalysisError(CreateSQLFunctionCommand.scala:329)\n\tat com.databricks.sql.execution.command.CreateSQLFunctionCommand.run(CreateSQLFunctionCommand.scala:107)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:195)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:87)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:84)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:83)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:97)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$5(QueryExecution.scala:505)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$4(QueryExecution.scala:505)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$3(QueryExecution.scala:504)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$17(SQLExecution.scala:583)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$15(SQLExecution.scala:496)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:898)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$14(SQLExecution.scala:418)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$13(SQLExecution.scala:418)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:933)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:417)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:240)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:851)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:500)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:496)\n\tat org.apache.spark.sql.execution.QueryExecution.withMVTagsIfNecessary(QueryExecution.scala:438)\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:494)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:597)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$8$1.applyOrElse(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:530)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:361)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:357)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:506)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$8(QueryExecution.scala:589)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:589)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:394)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:399)\n\tat org.apache.spark.sql.classic.Dataset.<init>(Dataset.scala:432)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:155)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:280)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:237)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$execute$1(ExecuteThreadRunner.scala:140)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries(UtilizationMetrics.scala:43)\n\tat com.databricks.spark.connect.service.UtilizationMetrics.recordActiveQueries$(UtilizationMetrics.scala:40)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.recordActiveQueries(ExecuteThreadRunner.scala:52)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:138)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:584)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:109)\n\tat scala.util.Using$.resource(Using.scala:296)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:108)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:584)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP FUNCTION IF EXISTS turbine_maintenance_predictor;\n",
    "\n",
    "--Use turbine_maintenance_predictor to get a prediction of whether or not a turbine sensor is faulty to facilitate proactive maintenance\n",
    "--This function is used to predict turbine maintenance based on energy and sensor readings\n",
    "\n",
    "CREATE OR REPLACE FUNCTION \n",
    "turbine_maintenance_predictor(avg_energy DOUBLE, \n",
    "                              std_sensor_A DOUBLE, \n",
    "                              std_sensor_B DOUBLE, \n",
    "                              std_sensor_C DOUBLE, \n",
    "                              std_sensor_D DOUBLE, \n",
    "                              std_sensor_E DOUBLE, \n",
    "                              std_sensor_F DOUBLE\n",
    ")\n",
    "RETURNS STRING\n",
    "LANGUAGE SQL\n",
    "COMMENT 'This tool predicts whether or not a turbine is faulty to facilitate proactive maintenance. It expects an array of 7 double values (energy and sensor readings) as input and returns a string indicating which sensor is predicted to be faulty or if all sensors are ok.'\n",
    "RETURN\n",
    "(\n",
    "    SELECT \n",
    "        -- The xgboost model returns a float; translate it back to a string\n",
    "        CASE WHEN float_prediction=0 THEN \"F\"\n",
    "            WHEN float_prediction=1 THEN \"ok\"\n",
    "            WHEN float_prediction=2 THEN \"B\"\n",
    "            WHEN float_prediction=3 THEN \"D\"\n",
    "        ELSE \"faulty\" END AS prediction    \n",
    "    FROM (\n",
    "    SELECT ai_query('e2eai_iot_turbine_prediction_endpoint',\n",
    "        STRUCT(avg_energy AS avg_energy,\n",
    "            std_sensor_A AS std_sensor_A,\n",
    "            std_sensor_B AS std_sensor_B,\n",
    "            std_sensor_C AS std_sensor_C,\n",
    "            std_sensor_D AS std_sensor_D,\n",
    "            std_sensor_E AS std_sensor_E,\n",
    "            std_sensor_F AS std_sensor_F\n",
    "        ),\n",
    "        'FLOAT'\n",
    "    ) AS float_prediction)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db16ee02-01bc-43ad-85b6-a2d1b79644b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now, test our tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9aab5a60-628d-423a-8cff-2daecb10d7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-7947322644040231>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSELECT turbine_maintenance_predictor(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    0.9000803742589635,                           -- avg_energy\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.2081154200781867,                           -- std_sensor_A\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.6012126574143823,                           -- std_sensor_B\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.1075958066966423,                           -- std_sensor_C\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.2081154200781867,                           -- std_sensor_D\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.6012126574143823,                           -- std_sensor_E\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.1075958066966423                            -- std_sensor_F\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m) AS prediction\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n",
       "\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n",
       "\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n",
       "\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
       "\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n",
       "\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n",
       "\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n",
       "\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:152\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n",
       "\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n",
       "\u001B[1;32m    149\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m    150\u001B[0m     )\n",
       "\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_py4j:\n",
       "\u001B[0;32m--> 152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_py4j(cell)\n",
       "\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_sql_comm_handler(cell)\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:237\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n",
       "\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    236\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
       "\u001B[0;32m--> 237\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
       "\u001B[1;32m    238\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:233\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n",
       "\u001B[1;32m    231\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuting subquery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stdout__)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n",
       "\u001B[1;32m    232\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n",
       "\u001B[0;32m--> 233\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n",
       "\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
       "\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n",
       "\u001B[1;32m    236\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:257\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n",
       "\u001B[1;32m    254\u001B[0m query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n",
       "\u001B[1;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(\n",
       "\u001B[1;32m    256\u001B[0m     query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mescapeWidgetValues(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()))\n",
       "\u001B[0;32m--> 257\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_query_request_result(query)\n",
       "\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# E.g. \"create or replace temp view abc as select 1\" gets us here and we should not return a result in this case.\u001B[39;00m\n",
       "\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\n",
       "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:129\u001B[0m, in \u001B[0;36mSqlMagic.get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n",
       "\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\u001B[1;32m    128\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m--> 129\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n",
       "\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:736\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    733\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n",
       "\u001B[1;32m    735\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001B[0;32m--> 736\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n",
       "\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m    738\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1305\u001B[0m )\n",
       "\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1763\u001B[0m     ):\n",
       "\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n",
       "\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2133\u001B[0m                 info,\n",
       "\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [WRONG_NUM_ARGS.WITHOUT_SUGGESTION] The `main`.`e2eai_iot_turbine`.`turbine_maintenance_predictor` requires 1 parameters but the actual number is 7. Please, refer to 'https://spark.apache.org/docs/latest/sql-ref-functions.html' for a fix. SQLSTATE: 42605\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.AnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.wrongNumArgsError(QueryCompilationErrors.scala:1329)\n",
       "\tat com.databricks.sql.analyzer.ExternalUDFExpression.checkFunctionArgumentCount(ExternalUDFExpression.scala:95)\n",
       "\tat com.databricks.sql.analyzer.ExternalUDFExpression.checkInputDataTypes(ExternalUDFExpression.scala:66)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n",
       "\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n",
       "\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n",
       "\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n",
       "\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n",
       "\tat scala.collection.immutable.List.map(List.scala:247)\n",
       "\tat scala.collection.immutable.List.map(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n",
       "\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.ParameterizedQuery.mapChildren(parameters.scala:65)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n",
       "\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
       "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
       "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n",
       "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:579)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:411)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:636)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:636)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:628)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:675)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n",
       "\tat scala.Option.flatMap(Option.scala:283)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n",
       "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[WRONG_NUM_ARGS.WITHOUT_SUGGESTION] The `main`.`e2eai_iot_turbine`.`turbine_maintenance_predictor` requires 1 parameters but the actual number is 7. Please, refer to 'https://spark.apache.org/docs/latest/sql-ref-functions.html' for a fix. SQLSTATE: 42605\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.wrongNumArgsError(QueryCompilationErrors.scala:1329)\n\tat com.databricks.sql.analyzer.ExternalUDFExpression.checkFunctionArgumentCount(ExternalUDFExpression.scala:95)\n\tat com.databricks.sql.analyzer.ExternalUDFExpression.checkInputDataTypes(ExternalUDFExpression.scala:66)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.analysis.ParameterizedQuery.mapChildren(parameters.scala:65)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:579)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:411)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:636)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:636)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:628)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:675)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
       },
       "metadata": {
        "errorSummary": "[WRONG_NUM_ARGS.WITHOUT_SUGGESTION] The `main`.`e2eai_iot_turbine`.`turbine_maintenance_predictor` requires 1 parameters but the actual number is 7. Please, refer to 'https://spark.apache.org/docs/latest/sql-ref-functions.html' for a fix. SQLSTATE: 42605"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "WRONG_NUM_ARGS.WITHOUT_SUGGESTION",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": "42605",
        "stackTrace": "org.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.wrongNumArgsError(QueryCompilationErrors.scala:1329)\n\tat com.databricks.sql.analyzer.ExternalUDFExpression.checkFunctionArgumentCount(ExternalUDFExpression.scala:95)\n\tat com.databricks.sql.analyzer.ExternalUDFExpression.checkInputDataTypes(ExternalUDFExpression.scala:66)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.analysis.ParameterizedQuery.mapChildren(parameters.scala:65)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:579)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:411)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:636)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:636)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:628)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:675)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-7947322644040231>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m get_ipython()\u001B[38;5;241m.\u001B[39mrun_cell_magic(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msql\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSELECT turbine_maintenance_predictor(\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    0.9000803742589635,                           -- avg_energy\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.2081154200781867,                           -- std_sensor_A\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.6012126574143823,                           -- std_sensor_B\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.1075958066966423,                           -- std_sensor_C\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.2081154200781867,                           -- std_sensor_D\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.6012126574143823,                           -- std_sensor_E\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m    2.1075958066966423                            -- std_sensor_F\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m) AS prediction\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2541\u001B[0m, in \u001B[0;36mInteractiveShell.run_cell_magic\u001B[0;34m(self, magic_name, line, cell)\u001B[0m\n\u001B[1;32m   2539\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m   2540\u001B[0m     args \u001B[38;5;241m=\u001B[39m (magic_arg_s, cell)\n\u001B[0;32m-> 2541\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   2543\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[1;32m   2544\u001B[0m \u001B[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001B[39;00m\n\u001B[1;32m   2545\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:152\u001B[0m, in \u001B[0;36mSqlMagic.sql\u001B[0;34m(self, line, cell)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\n\u001B[1;32m    149\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot run \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124mql command because spark connect initialization failed and no stacktrace was available.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    150\u001B[0m     )\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_py4j:\n\u001B[0;32m--> 152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_py4j(cell)\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexecute_via_sql_comm_handler(cell)\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:237\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    236\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n\u001B[0;32m--> 237\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_SUCCEEDED\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:233\u001B[0m, in \u001B[0;36mSqlMagic.execute_via_py4j\u001B[0;34m(self, cell)\u001B[0m\n\u001B[1;32m    231\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExecuting subquery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery_text\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, file\u001B[38;5;241m=\u001B[39msys\u001B[38;5;241m.\u001B[39m__stdout__)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    232\u001B[0m         sql_directive \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mgetSqlDirective(query_text)\n\u001B[0;32m--> 233\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle_sql_directive(sql_directive, i \u001B[38;5;241m==\u001B[39m number_of_sub_queries \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    234\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    235\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSQL_MAGIC_PY4J_FAILED\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    236\u001B[0m                                                        json\u001B[38;5;241m.\u001B[39mdumps(get_debug_info(e)))\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:257\u001B[0m, in \u001B[0;36mSqlMagic.handle_sql_directive\u001B[0;34m(self, sql_directive, is_last_query)\u001B[0m\n\u001B[1;32m    254\u001B[0m query \u001B[38;5;241m=\u001B[39m sql_directive\u001B[38;5;241m.\u001B[39msql()\n\u001B[1;32m    255\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_register_udf_if_needed(\n\u001B[1;32m    256\u001B[0m     query, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mentry_point\u001B[38;5;241m.\u001B[39mescapeWidgetValues(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()))\n\u001B[0;32m--> 257\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_query_request_result(query)\n\u001B[1;32m    258\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m df \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(df\u001B[38;5;241m.\u001B[39mschema) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    259\u001B[0m     \u001B[38;5;66;03m# E.g. \"create or replace temp view abc as select 1\" gets us here and we should not return a result in this case.\u001B[39;00m\n\u001B[1;32m    260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
        "File \u001B[0;32m/databricks/python_shell/lib/dbruntime/sql_magic/sql_magic.py:129\u001B[0m, in \u001B[0;36mSqlMagic.get_query_request_result\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(widget_bindings \u001B[38;5;241m:=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdbutils\u001B[38;5;241m.\u001B[39mwidgets\u001B[38;5;241m.\u001B[39mgetAll()) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    128\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdriver_activity_logger\u001B[38;5;241m.\u001B[39mlogExecuteCommandEvent(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPARAM_SYNTAX_USAGE\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 129\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39masserting_spark\u001B[38;5;241m.\u001B[39msql(query, widget_bindings)\n\u001B[1;32m    130\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/session.py:736\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    733\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    735\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 736\u001B[0m data, properties, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    737\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    738\u001B[0m     df \u001B[38;5;241m=\u001B[39m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1303\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1301\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1302\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1303\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1304\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1305\u001B[0m )\n\u001B[1;32m   1306\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1307\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1761\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1758\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1760\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1761\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1762\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1763\u001B[0m     ):\n\u001B[1;32m   1764\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1765\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1737\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1735\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1736\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1737\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2052\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2053\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2054\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2055\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2132\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2128\u001B[0m             d\u001B[38;5;241m.\u001B[39mUnpack(info)\n\u001B[1;32m   2130\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(status, info)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2132\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2133\u001B[0m                 info,\n\u001B[1;32m   2134\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2135\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2136\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2137\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2140\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2141\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mSparkConnectGrpcException\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2142\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2143\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [WRONG_NUM_ARGS.WITHOUT_SUGGESTION] The `main`.`e2eai_iot_turbine`.`turbine_maintenance_predictor` requires 1 parameters but the actual number is 7. Please, refer to 'https://spark.apache.org/docs/latest/sql-ref-functions.html' for a fix. SQLSTATE: 42605\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.wrongNumArgsError(QueryCompilationErrors.scala:1329)\n\tat com.databricks.sql.analyzer.ExternalUDFExpression.checkFunctionArgumentCount(ExternalUDFExpression.scala:95)\n\tat com.databricks.sql.analyzer.ExternalUDFExpression.checkInputDataTypes(ExternalUDFExpression.scala:66)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:356)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$childrenResolved$1$adapted(Expression.scala:368)\n\tat scala.collection.IterableOnceOps.forall(IterableOnce.scala:633)\n\tat scala.collection.IterableOnceOps.forall$(IterableOnce.scala:630)\n\tat scala.collection.AbstractIterable.forall(Iterable.scala:935)\n\tat org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:368)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3092)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20$$anonfun$applyOrElse$169.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:587)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUpWithPruning$1(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:232)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:244)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:250)\n\tat scala.collection.immutable.List.map(List.scala:247)\n\tat scala.collection.immutable.List.map(List.scala:79)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:250)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$5(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:372)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:255)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUpWithPruning(QueryPlan.scala:220)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:3074)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$20.applyOrElse(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:141)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1340)\n\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1339)\n\tat org.apache.spark.sql.catalyst.analysis.ParameterizedQuery.mapChildren(parameters.scala:65)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:418)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:137)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:133)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:45)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2906)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$.apply(Analyzer.scala:2901)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$17(RuleExecutor.scala:503)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule(RuleExecutor.scala:657)\n\tat org.apache.spark.sql.catalyst.rules.RecoverableRuleExecutionHelper.processRule$(RuleExecutor.scala:641)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.processRule(RuleExecutor.scala:137)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$16(RuleExecutor.scala:503)\n\tat com.databricks.spark.util.MemoryTracker$.withThreadAllocatedBytes(MemoryTracker.scala:51)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.measureRule(QueryPlanningTracker.scala:352)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$15(RuleExecutor.scala:501)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$14(RuleExecutor.scala:500)\n\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$13(RuleExecutor.scala:492)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeBatch$1(RuleExecutor.scala:466)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23(RuleExecutor.scala:613)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$23$adapted(RuleExecutor.scala:613)\n\tat scala.collection.immutable.List.foreach(List.scala:334)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:613)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:359)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.super$execute(Analyzer.scala:665)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeSameContext$1(Analyzer.scala:665)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeSameContext(Analyzer.scala:664)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:439)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:656)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:579)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:267)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:347)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:411)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:636)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:636)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:628)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:347)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:675)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:157)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:115)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:138)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:832)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1478)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:825)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:822)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:821)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:809)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:820)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:819)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:329)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:328)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1687)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:389)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:302)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$3(Dataset.scala:154)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1077)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1077)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:146)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$sql$5(SparkSession.scala:856)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.sql(SparkSession.scala:819)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:3540)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:3378)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3255)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:383)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1748)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:753)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:186)\n\tat scala.Option.flatMap(Option.scala:283)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:186)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:832)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:779)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:249)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:237)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:29)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:215)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:215)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:130)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:130)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:116)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:112)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1575)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:112)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT turbine_maintenance_predictor(\n",
    "    0.9000803742589635,                           -- avg_energy\n",
    "    2.2081154200781867,                           -- std_sensor_A\n",
    "    2.6012126574143823,                           -- std_sensor_B\n",
    "    2.1075958066966423,                           -- std_sensor_C\n",
    "    2.2081154200781867,                           -- std_sensor_D\n",
    "    2.6012126574143823,                           -- std_sensor_E\n",
    "    2.1075958066966423                            -- std_sensor_F\n",
    ") AS prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79ba0239-bc3e-462a-86b8-d1c01141dbec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now build an alternate tool using the python code we just tested. \n",
    "\n",
    "**You will need to add an API TOKEN and API ROOT before running this code.** The notebook API_TOKEN that we used for testing the python code above will not work.  Instead, create a [Personal Access Token](https://docs.databricks.com/aws/en/dev-tools/auth/pat)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d30a7f-d402-4452-95fd-5403ec19a86c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE FUNCTION turbine_maintenance_predictor(sensor_values ARRAY<DOUBLE>)\n",
    "RETURNS STRING\n",
    "LANGUAGE PYTHON\n",
    "COMMENT 'This tool predicts whether or not a turbine is faulty to facilitate proactive maintenance. It expects an array of 7 double values (energy and sensor readings) as input and returns a string indicating if a particular sensor is predicted to be faulty or if all sensors are ok.'\n",
    "AS \n",
    "$$\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json \n",
    "import requests\n",
    "\n",
    "#API TOKEN AND URL HERE\n",
    "\n",
    "api_token = \"\"\n",
    "api_root = \"\"\n",
    "\n",
    "model_serving_endpoint_name = 'e2eai_iot_turbine_prediction_endpoint'\n",
    "\n",
    "columns = ['avg_energy', 'std_sensor_A', 'std_sensor_B', 'std_sensor_C', 'std_sensor_D', 'std_sensor_E', 'std_sensor_F']\n",
    "\n",
    "samp_ar = np.array([sensor_values])\n",
    "\n",
    "data = pd.DataFrame(samp_ar, columns=columns)\n",
    "\n",
    "url = f'{api_root}/serving-endpoints/{model_serving_endpoint_name}/invocations'\n",
    "\n",
    "headers = {'Authorization': f'Bearer {api_token}', \n",
    "            'Content-Type': 'application/json'}\n",
    "\n",
    "\n",
    "ds_dict = {'dataframe_split': data.to_dict(orient='split')} if isinstance(data, pd.DataFrame) else tf_serving_json\n",
    "\n",
    "data_json = json.dumps(ds_dict, allow_nan=True)\n",
    "\n",
    "response = requests.request(method='POST', headers=headers, url=url, data=data_json)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n",
    "\n",
    "if response.json()['predictions'][0] == 0:\n",
    "    return 'Sensor F fault'\n",
    "elif response.json()['predictions'][0] == 1:\n",
    "    return 'ok'\n",
    "elif response.json()['predictions'][0] == 2:\n",
    "    return 'Sensor B fault'\n",
    "elif response.json()['predictions'][0] == 3:\n",
    "    return 'Sensor D fault'\n",
    "else:\n",
    "    return 'faulty'\n",
    "\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba25ed66-a823-42c2-b9d3-49d79fecdd5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Test the python/SQL tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "667c40d4-e285-48db-b6ef-19444f2c17a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>prediction</th></tr></thead><tbody><tr><td>Sensor F fault</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Sensor F fault"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "prediction",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "prediction",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT turbine_maintenance_predictor(array(0.1889792, \n",
    "                                           0.9644652, \n",
    "                                           2.65583866, \n",
    "                                           3.4528106, \n",
    "                                           2.48515875,\n",
    "                                           2.28840325, \n",
    "                                           4.70213899)) as prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a11dc8a-f5df-45ee-90c5-dfc0b82275d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Our agent should call turbine_specifications_retriever() to get sensor readings, then call turbine_maintenance_predictor() to get a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "8ca12188-e739-4226-bddc-3f1a4b6485f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Part 3: Add a tool to access our maintenance guide content and provide support to the operator during maintenance operation\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/databricks-demos/dbdemos-resources/refs/heads/main/images/manufacturing/lakehouse-iot-turbine/iot_agent_graph_v2_3.png\" style=\"float: right; width: 600px; margin-left: 10px\">\n",
    "\n",
    "\n",
    "We were provided with PDF guide containing all the error code and maintenance steps for the critical components of our wind turbine. The're saved as pdf file in our volume.\n",
    "\n",
    "Let's parse them and index them so that we can properly retrieve them. We'll save them in a Vector Search endpoint and leverage it to guide the operators with the maintenance step and recommendations.\n",
    "\n",
    "We'll use a Managed embedding index to make it simple. In this section we will:\n",
    "\n",
    "1. Parse and save our PDF text in a Delta Table using Databricks AI Query `ai_parse_document`\n",
    "2. Create a `Vector Search endpoint` (required to host your vector search index)\n",
    "3. Create a `Vector Search Direct Index`  (the actual index)\n",
    "4. Create a `Tool (UC function)` using our vector search "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1732888c-4571-469b-925b-88a471a5cb84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 2.1. Parse and save our PDF text\n",
    "Let's start by parsing the maintenance guide documents, saved as pdf in our volume:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0914421b-8289-4917-8a78-418e27cbc52b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing subquery: DROP FUNCTION IF EXISTS turbine_specifications_retriever.\nExecuting subquery: --turbine_specifications_retriever to get the current status of a turbine\n--This function is used to retrieve the turbine specifications based on its id\n\nCREATE OR REPLACE FUNCTION \nturbine_specifications_retriever(turbine_id STRING COMMENT 'ID of the wind turbine to look up')\nRETURNS TABLE (\n  avg_energy DOUBLE COMMENT 'Average energy reading',\n  std_sensor_A DOUBLE COMMENT 'Sensor A reading',\n  std_sensor_B DOUBLE COMMENT 'Sensor B reading',\n  std_sensor_C DOUBLE COMMENT 'Sensor C reading',\n  std_sensor_D DOUBLE COMMENT 'Sensor D reading',\n  std_sensor_E DOUBLE COMMENT 'Sensor E reading',\n  std_sensor_F DOUBLE COMMENT 'Sensor F reading'\n)\nLANGUAGE SQL\nCOMMENT 'This function retrieves the turbine sensor readings / specifications based on the turbine_id'\nRETURN\n(\nSELECT \n\navg_energy, std_sensor_A, std_sensor_B, std_sensor_C, std_sensor_D, std_sensor_E, std_sensor_F\nFROM main.e2eai_iot_turbine.turbine_current_features\nWHERE turbine_id = turbine_specifications_retriever.turbine_id\nSORT BY hourly_timestamp DESC\nlimit 1\n).\nExecuting subquery: SELECT * FROM turbine_specifications_retriever('004a641f-e9e5-9fff-d421-1bf88319420b').\nExecuting subquery: CREATE OR REPLACE FUNCTION turbine_maintenance_predictor_py(sensor_values ARRAY<DOUBLE>)\nRETURNS STRING\nLANGUAGE PYTHON\nCOMMENT 'This tool predicts whether or not a turbine is faulty to facilitate proactive maintenance. It expects an array of 7 double values (energy and sensor readings) as input and returns a string indicating if a particular sensor is predicted to be faulty or if all sensors are ok.'\nAS \n$$\n\nimport numpy as np\nimport pandas as pd\nimport json \nimport requests\n\n#API TOKEN AND URL HERE\napi_token = '<your-api-token-here>'\napi_root = 'https://<your-api-root-here>.cloud.databricks.com'\n\nmodel_serving_endpoint_name = 'e2eai_iot_turbine_prediction_endpoint'\n\ncolumns = ['avg_energy', 'std_sensor_A', 'std_sensor_B', 'std_sensor_C', 'std_sensor_D', 'std_sensor_E', 'std_sensor_F']\n\nsamp_ar = np.array([sensor_values])\n\ndata = pd.DataFrame(samp_ar, columns=columns)\n\nurl = f'{api_root}/serving-endpoints/{model_serving_endpoint_name}/invocations'\n\nheaders = {'Authorization': f'Bearer {api_token}', \n            'Content-Type': 'application/json'}\n\n\nds_dict = {'dataframe_split': data.to_dict(orient='split')} if isinstance(data, pd.DataFrame) else tf_serving_json\n\ndata_json = json.dumps(ds_dict, allow_nan=True)\n\nresponse = requests.request(method='POST', headers=headers, url=url, data=data_json)\n\nif response.status_code != 200:\n    raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n\nif response.json()['predictions'][0] == 0:\n    return 'Sensor F fault'\nelif response.json()['predictions'][0] == 1:\n    return 'ok'\nelif response.json()['predictions'][0] == 2:\n    return 'Sensor B fault'\nelif response.json()['predictions'][0] == 3:\n    return 'Sensor D fault'\nelse:\n    return 'faulty'\n\n$$.\nExecuting subquery: SELECT turbine_maintenance_predictor(array(0.1889792, \n                                           0.9644652, \n                                           2.65583866, \n                                           3.4528106, \n                                           2.48515875,\n                                           2.28840325, \n                                           4.70213899)) as prediction.\nExecuting subquery: CREATE OR REPLACE FUNCTION turbine_maintenance_predictor_py(sensor_values ARRAY<DOUBLE>)\nRETURNS STRING\nLANGUAGE PYTHON\nCOMMENT 'This tool predicts whether or not a turbine is faulty to facilitate proactive maintenance. It expects an array of 7 double values (energy and sensor readings) as input and returns a string indicating if a particular sensor is predicted to be faulty or if all sensors are ok.'\nAS \n$$\n\nimport numpy as np\nimport pandas as pd\nimport json \nimport requests\n\n#API TOKEN AND URL HERE\n\n\napi_token = f\"https://{dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().value()}/\"\napi_root = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n\nmodel_serving_endpoint_name = 'e2eai_iot_turbine_prediction_endpoint'\n\ncolumns = ['avg_energy', 'std_sensor_A', 'std_sensor_B', 'std_sensor_C', 'std_sensor_D', 'std_sensor_E', 'std_sensor_F']\n\nsamp_ar = np.array([sensor_values])\n\ndata = pd.DataFrame(samp_ar, columns=columns)\n\nurl = f'{api_root}/serving-endpoints/{model_serving_endpoint_name}/invocations'\n\nheaders = {'Authorization': f'Bearer {api_token}', \n            'Content-Type': 'application/json'}\n\n\nds_dict = {'dataframe_split': data.to_dict(orient='split')} if isinstance(data, pd.DataFrame) else tf_serving_json\n\ndata_json = json.dumps(ds_dict, allow_nan=True)\n\nresponse = requests.request(method='POST', headers=headers, url=url, data=data_json)\n\nif response.status_code != 200:\n    raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n\nif response.json()['predictions'][0] == 0:\n    return 'Sensor F fault'\nelif response.json()['predictions'][0] == 1:\n    return 'ok'\nelif response.json()['predictions'][0] == 2:\n    return 'Sensor B fault'\nelif response.json()['predictions'][0] == 3:\n    return 'Sensor D fault'\nelse:\n    return 'faulty'\n\n$$.\nExecuting subquery: SELECT turbine_maintenance_predictor(array(0.1889792, \n                                           0.9644652, \n                                           2.65583866, \n                                           3.4528106, \n                                           2.48515875,\n                                           2.28840325, \n                                           4.70213899)) as prediction.\nExecuting subquery: CREATE OR REPLACE FUNCTION turbine_maintenance_predictor(sensor_values ARRAY<DOUBLE>)\nRETURNS STRING\nLANGUAGE PYTHON\nCOMMENT 'This tool predicts whether or not a turbine is faulty to facilitate proactive maintenance. It expects an array of 7 double values (energy and sensor readings) as input and returns a string indicating if a particular sensor is predicted to be faulty or if all sensors are ok.'\nAS \n$$\n\nimport numpy as np\nimport pandas as pd\nimport json \nimport requests\n\n#API TOKEN AND URL HERE\n\n\napi_token = f\"https://{dbutils.notebook.entry_point.getDbutils().notebook().getContext().browserHostName().value()}/\"\napi_root = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n\nmodel_serving_endpoint_name = 'e2eai_iot_turbine_prediction_endpoint'\n\ncolumns = ['avg_energy', 'std_sensor_A', 'std_sensor_B', 'std_sensor_C', 'std_sensor_D', 'std_sensor_E', 'std_sensor_F']\n\nsamp_ar = np.array([sensor_values])\n\ndata = pd.DataFrame(samp_ar, columns=columns)\n\nurl = f'{api_root}/serving-endpoints/{model_serving_endpoint_name}/invocations'\n\nheaders = {'Authorization': f'Bearer {api_token}', \n            'Content-Type': 'application/json'}\n\n\nds_dict = {'dataframe_split': data.to_dict(orient='split')} if isinstance(data, pd.DataFrame) else tf_serving_json\n\ndata_json = json.dumps(ds_dict, allow_nan=True)\n\nresponse = requests.request(method='POST', headers=headers, url=url, data=data_json)\n\nif response.status_code != 200:\n    raise Exception(f'Request failed with status {response.status_code}, {response.text}')\n\nif response.json()['predictions'][0] == 0:\n    return 'Sensor F fault'\nelif response.json()['predictions'][0] == 1:\n    return 'ok'\nelif response.json()['predictions'][0] == 2:\n    return 'Sensor B fault'\nelif response.json()['predictions'][0] == 3:\n    return 'Sensor D fault'\nelse:\n    return 'faulty'\n\n$$.\nExecuting subquery: SELECT turbine_maintenance_predictor(array(0.1889792, \n                                           0.9644652, \n                                           2.65583866, \n                                           3.4528106, \n                                           2.48515875,\n                                           2.28840325, \n                                           4.70213899)) as prediction.\nExecuting subquery: DROP TABLE IF EXISTS turbine_maintenance_guide.\nExecuting subquery: CREATE TABLE turbine_maintenance_guide (\n  id BIGINT GENERATED ALWAYS AS IDENTITY,\n  EAN STRING,\n  weight STRING,\n  component_type STRING,\n  component_name STRING,\n  full_guide STRING)\n  TBLPROPERTIES (delta.enableChangeDataFeed = true)."
     ]
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS turbine_maintenance_guide;\n",
    "\n",
    "CREATE TABLE turbine_maintenance_guide (\n",
    "  id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "  EAN STRING,\n",
    "  weight STRING,\n",
    "  component_type STRING,\n",
    "  component_name STRING,\n",
    "  full_guide STRING)\n",
    "  TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f6e8aca-7a46-43f8-a5c2-d740646492f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Extract specific features from each maintenance manual.\n",
    "\n",
    "Edit the FROM table if you changed from the default catalog/schema in your config file.\n",
    "\n",
    "This query will take 10+ minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe427989-e829-40da-863b-0eead80b2043",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1753475700219}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>18</td><td>18</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         18,
         18
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "num_affected_rows",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "num_inserted_rows",
            "nullable": true,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 51
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "INSERT INTO turbine_maintenance_guide (EAN, weight, component_type, component_name, full_guide)\n",
    "\n",
    "SELECT ai_query(\n",
    "    'databricks-gemma-3-12b',\n",
    "    CONCAT(\"Extract the EAN from the following text. Return only the EAN. \\n\\nText:\", full_guide)    -- Placeholder for the prompt and input\n",
    "    ) AS EAN  -- Placeholder for the output column\n",
    "    ,ai_query(\n",
    "    'databricks-gemma-3-12b',\n",
    "    CONCAT(\"Extract the component weight from the following text. Return only the component weight. \\n\\nText:\", full_guide)    -- Placeholder for the prompt and input\n",
    "    ) AS weight  -- Placeholder for the output column\n",
    "    ,ai_query(\n",
    "    'databricks-gemma-3-12b',\n",
    "    CONCAT(\"Extract the component type from the following text. Return only the component type. \\n\\nText:\", full_guide)    -- Placeholder for the prompt and input\n",
    "    ) AS component_type  -- Placeholder for the output column \n",
    "    ,ai_query(\n",
    "    'databricks-gemma-3-12b',\n",
    "    CONCAT(\"Extract the component name from the following text. Return only the component name. \\n\\nText:\", full_guide)    -- Placeholder for the prompt and input\n",
    "    ) AS component_name  -- Placeholder for the output column\n",
    "    ,full_guide \n",
    "FROM (\n",
    "    -- Combine the content of all pages into a single string separated by new lines\n",
    "    SELECT array_join(\n",
    "\n",
    "            -- Transform each page struct in the array to just its 'content' field (extract text from each page)\n",
    "            transform(\n",
    "              -- parsed_document:document.pages::ARRAY<STRUCT<content:STRING>>, -- Array of page structs from the parsed document\n",
    "              parsed_document:document.elements::ARRAY<STRUCT<content:STRING>>,\n",
    "\n",
    "              x -> x.content -- For each struct (page), extract the 'content' string\n",
    "             ), \n",
    "             \n",
    "             '\\n' -- Join all extracted page contents with a newline character as the separator\n",
    "             ) \n",
    "             \n",
    "             AS full_guide\n",
    "    FROM (\n",
    "      -- Parse the document content\n",
    "      SELECT ai_parse_document(content) AS parsed_document\n",
    "      FROM READ_FILES(\"/Volumes/main/e2eai_iot_turbine/e2eai_turbine_raw_landing/maintenance_guide\", format => 'binaryFile')\n",
    "    )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fd0be6b-95d8-4225-b168-02ae4905085c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The [`ai_parse_document()`](https://docs.databricks.com/aws/en/sql/language-manual/functions/ai_parse_document) function invokes a state-of-the-art generative AI model from Databricks Foundation Model APIs to extract structured content from unstructured documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7900580-31a6-4051-ad10-3ac394f7cb9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>EAN</th><th>weight</th><th>component_type</th><th>component_name</th><th>full_guide</th></tr></thead><tbody><tr><td>19</td><td>96356234</td><td>4014g</td><td>Anemometer</td><td>AeroSense VTX-220</td><td>AeroSense VTX-220 Precision Anemometer\n",
       "Professional Maintenance Manual - Wind Turbine Component\n",
       "Part Type: Anemometer\n",
       "EAN: 96356234\n",
       "Compatible Turbine: Skylance XR550 Coastal Turbine Platform\n",
       "Sensors Used: sensor D, sensor C\n",
       "Dimensions: 364mm - 286mm\n",
       "Weight: 4014g\n",
       "Stock Zone: America/New_York\n",
       "Component Overview\n",
       "The AeroSense VTX-220 is a precision optical-cup anemometer designed to operate in harsh marine and coastal environments.\n",
       "It delivers high-frequency wind speed measurements to the turbine-s main control unit via sensor_D and communicates backup diagnostics via sensor_C.\n",
       "Equipped with a UV-resistant ABS housing and dual-bearing stainless steel shaft, it maintains sub-0.1s response latency in gusts up to 180 km/h.\n",
       "Recognizing Issues\n",
       "Operators may notice discrepancies between actual weather data and turbine logs, delayed SCADA wind alerts, or sudden drops in power generation.\n",
       "Typical warning signs include inconsistent wind-speed readings or visible physical obstruction of the rotor cups.\n",
       "Error Codes & Troubleshooting\n",
       "ANM-100\n",
       "Description: Low RPM detected despite wind presence. Likely mechanical blockage or bearing failure.\n",
       "Resolution: Inspect the cup rotor for salt deposits or bird interference. Apply approved cleaning fluid and rotate manually to check bearing friction.\n",
       "ANM-201\n",
       "Description: Intermittent signal loss on sensor_D channel. Data irregularities exceed 15% over 10 minutes.\n",
       "Resolution: Replace connector or test with alternate input pin. Ensure waterproof sealing and corrosion-free contact surface.\n",
       "ANM-310\n",
       "Description: Excessive vibration detected beyond 2.0g, Shaft imbalance or mounting flange fault suspected.\n",
       "Resolution: Use vibration sensor diagnostics to assess severity. Tighten mounting base, and recalibrate pitch compensation in firmware settings.\n",
       "Recommended Maintenance Schedule\n",
       "Inspect every 2,500 operational hours. Replace every 12,000 hours or if more than two error events occur in a 90-day period.\n",
       "Preventative cleaning every 6 months in salt-air environments is strongly advised.\n",
       "Certified Maintenance Procedure\n",
       "1. Engage turbine maintenance mode from SCADA dashboard. Confirm rotor lock and system isolation.\n",
       "2. Access the rooftop nacelle via secured ladder or lift. Wear high-visibility PPE and fall arrest gear rated for 100kg minimum.\n",
       "3. Locate the anemometer mast on the rear left corner of the nacelle shell. Document physical condition with photos before removal.\n",
       "4. Disconnect the twin sensor cable junction box under the mast. Use an IP67-rated cap to cover open leads during maintenance.\n",
       "5. Using a torque wrench, loosen the 3 mounting bolts (13mm) securing the anemometer base. Hold the unit from above to prevent fall.\n",
       "6. Gently lift the AeroSense VTX-220 unit upward and inspect the shaft coupling area for corrosion or fatigue.\n",
       "7. Clean the mounting flange area with alcohol-based surface cleaner and dry thoroughly using lint-free cloth.\n",
       "8. Prepare the replacement unit by verifying serial number, firmware revision, and alignment pin compatibility.\n",
       "9. Insert the VTX-220 anemometer onto the flange, align the guiding notch, and secure all bolts to 28 Nm torque.\n",
       "10. Reconnect sensor D and sensor C to the color-coded terminal blocks in the junction box. Use dielectric grease on terminals.\n",
       "11. Run a full sensor check using the Skylance Diagnostic Utility. Validate RPM, signal variance, and latency under simulated gust input.\n",
       "12. Recalibrate the wind-speed baseline against local weather station data. Accept results only if variance < 1.2%.\n",
       "13. Document replacement with timestamp, technician ID, and attach photos to maintenance log in the turbine management system.</td></tr><tr><td>20</td><td>50311982</td><td>9200g</td><td>Spring-applied hydraulic brake caliper</td><td>SBC-1300</td><td>BrakeTech SBC-1300 Spring-Applied Brake Caliper\n",
       "Professional Maintenance Manual - Rotor Braking System\n",
       "Component Type: Spring-applied hydraulic brake caliper\n",
       "EAN: 50311982\n",
       "Compatible Turbine Model: AeroVolt AV4000 High-Speed Wind Turbine\n",
       "Dimensions: 280mm × 240mm\n",
       "Weight: 9200g\n",
       "Sensor Interfaces: sensor_B, sensor_T, sensor_R\n",
       "Stock Location: Italy/Torino\n",
       "Component Overview\n",
       "The BrakeTech SBC-1300 is a heavy-duty spring-applied, hydraulically released brake caliper used to halt the turbine rotor during maintenance, overspeed events, or emergency stops.\n",
       "It applies immense clamping force through hardened steel pads onto the turbine disc. The caliper uses redundant hydraulic releases and preload springs,\n",
       "ensuring default engagement during pressure loss. It is integrated with Sensor_B (brake status), Sensor_T (temperature), and Sensor_R (release pressure feedback).\n",
       "Symptoms of Brake System Degradation\n",
       "- Delayed or ineffective braking in SCADA logs\n",
       "- Elevated brake temperatures during standard operation\n",
       "- Visible pad wear or oil leakage around caliper base\n",
       "- Abnormal noise or rotor drag during idle state\n",
       "Brake Fault Codes and Service Instructions\n",
       "SBC-007\n",
       "Description: Sensor_B reports brake engaged but rotor speed remains >5 RPM.\n",
       "Resolution: Check pad wear or oil contamination. Confirm hydraulic release valve integrity and spring tension.\n",
       "SBC-021\n",
       "Description: Sensor R pressure below 80 bar while in release mode.\n",
       "Resolution: Recharge hydraulic accumulator. Inspect for line blockage or degraded fluid viscosity.\n",
       "SBC-039\n",
       "Description: Brake temperature exceeds 95°C during braking cycle.\n",
       "Resolution: Allow cooldown. Check friction material and rotor disc for glazing. Replace if warped.\n",
       "SBC-044\n",
       "Description: Pad wear sensor triggered - thickness <3 mm.\n",
       "Resolution: Replace pads immediately. Inspect caliper piston travel distance and reset wear indicator.\n",
       "SBC-066\n",
       "Description: Hydraulic release response delay >1.5s.\n",
       "Resolution: Bleed hydraulic line and inspect solenoid valve function. Replace if sluggish.\n",
       "SBC-082\n",
       "Description: Caliper does not return to full open position.\n",
       "Resolution: Inspect spring assembly for binding. Lubricate guide pins and test manual release.\n",
       "SBC-097\n",
       "Description: Sensor _T drift between paired calipers >12°C.\n",
       "Resolution: Recalibrate Sensor_Ts. If discrepancy persists, replace affected sensor. Verify thermal coupling.\n",
       "Maintenance Frequency and Brake Lifecycle\n",
       "Inspect every 2,000 turbine hours or after every emergency stop. Replace brake caliper after 12,000 hours or 3 pad replacements.\n",
       "Caliper Removal and Safety Replacement Steps\n",
       "1. Park turbine, engage rotor lock, and confirm SCADA brake command is disengaged.\n",
       "2. Deactivate hydraulic pressure supply and drain residual line pressure safely.\n",
       "3. Access brake caliper zone at main shaft rear. Use platform lift or nacelle access hatch.\n",
       "4. Inspect for oil leaks, burn marks, or scoring on brake disc. Document any visible anomalies\n",
       "5. Disconnect sensor wiring for B, T, and R with ESD protection. Label connectors clearly.\n",
       "6. Unbolt caliper from its mount using heavy-duty tools. Use hoist or pulley system to support caliper weight.\n",
       "7. Inspect spring pack, return guides, and pistons for corrosion or misalignment. Replace if damaged.\n",
       "8. Mount new SBC-1300 caliper in exact orientation. Torque bolts to 85 Nm using cross pattern.\n",
       "9. Reconnect hydraulic line and perform leak test at 120 bar. Reattach sensor wiring and shield cable ends.\n",
       "10. Initiate SCADA brake release test. Confirm full retraction and zero drag on brake disc.\n",
       "11. Perform emergency brake test at low rotor RPM. Verify response time <1.2s and full stop within specs.\n",
       "12. Check pad contact area for uniform wear. Log disco temperature with Sensor_T\n",
       "13. Log all actions, torque values, and pressure readings. Photograph installation site and serial plate.\n",
       "14. Clear brake fault codes in SCADA and monitor first 3 hours of rotor activity for anomalies.</td></tr><tr><td>21</td><td>74182930</td><td>8900g</td><td>Turbine nacelle cooling fan array</td><td>ClimaRotor CFA-9000</td><td>ClimaRotor CFA-9000 Nacelle Cooling Fan Array\n",
       "Professional Maintenance Manual - Thermal Regulation System\n",
       "Component Type: Turbine nacelle cooling fan array\n",
       "EAN: 74182930\n",
       "Compatible Turbine Model: Solaris T680 Low-Noise Turbine\n",
       "Dimensions: 880mm - 270mm\n",
       "Weight: 8900g\n",
       "Sensor Interfaces: sensor_Q, sensor_T, sensor_E\n",
       "Stock Location: Spain/Valencia\n",
       "Component Overview\n",
       "The ClimaRotor CFA-9000 is an industrial-grade nacelle cooling fan array, composed of four variable-speed axial fans mounted in a stainless-steel frame.\n",
       "It ensures thermal regulation of inverter cabinets, hydraulic controllers, and sensor blocks.\n",
       "The unit operates under adaptive thermal control via SCADA and reads ambient and component temperatures from sensor_Q, sensor_T, and sensor_E.\n",
       "It features hot-swap fans, integrated anti-condensation heaters, low-noise impellers, and IP56-certified enclosures.\n",
       "The CFA-9000 prevents thermal derating and electronic component degradation in high-temperature environments and is critical to maintaining optimal turbine performance.\n",
       "Thermal Fault Symptoms and Behavior\n",
       "- Excessive inverter cabinet temperatures\n",
       "- SCADA alarms related to Sensor Q, Sensor T, or Sensor E\n",
       "- Sudden fan speed drop or loud vibration during operation\n",
       "- Moisture buildup or fog inside nacelle electrical compartments\n",
       "- Error messages from fan controller or PWM signal anomalies\n",
       "Critical Error Codes and Technical Resolutions\n",
       "CFA-012\n",
       "Description: Fan #2 RPM < 400 during operation at 80% PWM duty cycle.\n",
       "Resolution: Verify power connection and fan controller output. If fan does not spin freely by hand, replace motor bearing assembly.\n",
       "CFA-035\n",
       "Description: Sensor_Q temperature exceeds 55-C, airflow nominal.\n",
       "Resolution: Check for external air blockage, clogged intake filter, or sensor calibration drift. Replace filter and recalibrate sensor_Q if needed.\n",
       "CFA-074\n",
       "Description: Condensation heater fault - coil resistance above 500-.\n",
       "Resolution: Check heater coil continuity and relay response. Replace heater module if damaged.\n",
       "Confirm ambient humidity > 60% before retest.\n",
       "CFA-109\n",
       "Description: Sensor T reports thermal runaway - >20-C rise in under 2 minutes.\n",
       "Resolution: Inspect inverter cabinet ventilation. Ensure internal fans are operational. If airflow from\n",
       "CFA-9000 is reduced, replace impeller or controller.\n",
       "CFA-201\n",
       "Description: Fan controller board communication timeout.\n",
       "Resolution: Reset controller. Check CAN bus connection and terminate signal ends. Replace controller board if failure recurs more than twice daily.\n",
       "CFA-313\n",
       "Description: Fan vibration exceeds limit threshold during ramp-up.\n",
       "Resolution: Inspect fan blades for debris or cracks. Balance impellers or replace affected unit. Use\n",
       "vibration dampeners if repeatedly triggered\n",
       "CFA-404\n",
       "Description: Sensor_E offline - no temperature data for >45 seconds.\n",
       "Resolution: Inspect cabling and sensor port. If connection stable, replace sensor_E and bind via SCADA maintenance utility.\n",
       "Inspection and Maintenance Interval\n",
       "Inspect CFA-9000 every 1,500 turbine hours or monthly in high-temperature regions. Replace individual fans after 10,000 hours or when vibration threshold is exceeded.\n",
       "Step-by-Step Field Technician Procedure\n",
       "1. Access turbine SCADA and disable CFA-9000 module through cooling subsystem controls. Verify module shutdown via indicator LED.\n",
       "2. Power off auxiliary cooling bus and lockout. Confirm voltage zero across fan terminals with multimeter before opening cabinet.\n",
       "3. Open nacelle top panel and locate CFA-9000 mounting frame. Use safety harness and elevated service platform for access.\n",
       "4. Visually inspect the four axial fans for debris, corrosion, or cracked blades. Photograph any damage before continuing.\n",
       "5. Disconnect PWM signal and power cables for each fan, as well as the CAN controller connection.\n",
       "Tag all connections clearly.\n",
       "6. Remove faulty fan(s) by unscrewing mounting bracket bolts. Fans are hot-swappable, but full power-off is advised for safety.\n",
       "7. Clean mounting surfaces, apply anti-vibration foam strips if degraded. Install new fan with bolts torqued to 4 Nm.\n",
       "8. Reconnect all power, signal, and CAN lines. Confirm secure and correct connection layout from pre-labeled cables.\n",
       "9. Inspect condensation heater terminals for oxidation. Test heater resistance and replace unit if value > 400-.\n",
       "10. Power on auxiliary cooling system. From SCADA, run fan diagnostic cycle and monitor RPM, temperature, and vibration metrics.\n",
       "11. Check SCADA fan logs for normal PWM ramping and sensor readings < 50-C across all points.\n",
       "Verify CAN controller uptime.\n",
       "12. Update turbine maintenance log with fan serial numbers, install date, measured resistance, and thermal calibration values.\n",
       "13. If applicable, reset thermal error counters and clear any latched CFA-9000 error codes from SCADA system.\n",
       "14. Close nacelle panel, release lockout, and observe cooling system during normal turbine operation for 15 minutes post-repair.</td></tr><tr><td>22</td><td>91020488</td><td>3400g</td><td>Blade deicing control module</td><td>DCM-4500</td><td>CryoBlade DCM-4500 Blade Deicing Control Module\n",
       "Professional Maintenance Manual - Blade Thermal System\n",
       "Component Type: Blade deicing control module\n",
       "EAN: 91020488\n",
       "Compatible Turbine Model: SkyWind S3000 Mountain-Class Turbine\n",
       "Dimensions: 180mm × 140mm\n",
       "Weight: 3400g\n",
       "Sensor Interfaces: sensor_I, sensor_T, sensor_V\n",
       "Stock Location: Norway/Bergen\n",
       "Component Overview\n",
       "The CryoBlade DCM-4500 is a critical electronic control module that governs the thermal deicing of turbine blades in sub-zero conditions.\n",
       "It modulates heating filament activity embedded in blades based on real-time ice detection, ambient temperature, and wind velocity inputs.\n",
       "The system maintains blade balance by cycling heating zones and using predictive icing models.\n",
       "It interfaces with sensors I (icing presence), T (external temperature), and V (wind velocity) to dynamically adjust power draw\n",
       "across blade zones. The DCM-4500 features redundant heating channel control, a thermal load balancer, and anti-condensation pulse mode.\n",
       "Key Failure Symptoms and Observations\n",
       "- Persistent blade icing despite active deicing system\n",
       "- Long heating delay at turbine startup\n",
       "- SCADA errors referencing sensors I, T, or V\n",
       "- Unusual heat signature patterns on blade thermal scan\n",
       "Diagnostic Fault Codes and Remediation Steps\n",
       "DCM-012\n",
       "Description: Heater channel 2 draws >10A over 30 seconds.\n",
       "Resolution: Check for short circuit in blade zone 2 wiring. Replace any frayed conductor or damaged connector block.\n",
       "DCM-033\n",
       "Description: Sensor T temperature drift exceeds ±3°C in 5 minutes.\n",
       "Resolution: Inspect sensor location for thermal shadowing. Replace sensor if internal drift test fails in diagnostics.\n",
       "DCM-048\n",
       "Description: Sensor 1 reports ice presence while heater active >10 minutes.\n",
       "Resolution: Run heater test mode. If ice not cleared, inspect filament resistance and continuity.\n",
       "Replace blade heater section.\n",
       "DCM-061\n",
       "Description: Communication fault with Sensor V.\n",
       "Resolution: Check wiring from nacelle to hub. Replace twisted pair if continuity fails. Restart module to re-establish sync.\n",
       "DCM-084\n",
       "Description: Overtemperature on deicing controller mainboard >95°C.\n",
       "Resolution: Inspect cooling fan and thermal paste. Check nacelle vent flow. Shut down controller if passive cooling fails.\n",
       "DCM-109\n",
       "Description: Zone activation delay >15s vs target.\n",
       "Resolution: Confirm system voltage stability. Recalibrate thermal switches. Replace relay if sluggish behavior persists.\n",
       "DCM-210\n",
       "Description: Anti-condensation pulse mode active for >60 minutes.\n",
       "Resolution: Review SCADA humidity and blade surface conditions. Override if unnecessary. Inspect pulse modulation board.\n",
       "Preventive Maintenance and Replacement Schedule\n",
       "Inspect the DCM-4500 quarterly and after each severe weather event. Replace after 20,000 operational hours or upon thermal degradation above 90°C.\n",
       "Detailed Maintenance and Swap Instructions\n",
       "1. Shut down turbine and isolate DCM-4500 from SCADA controller interface.\n",
       "2. Use lockout tagout procedures at the electrical cabinet and confirm safe access with voltmeter.\n",
       "3. Open access panel near root of blade. Locate DCM-4500 inside sealed junction enclosure.\n",
       "4. Disconnect all sensor inputs (I, T, V) and mark wiring using labeled ties. Photograph connections.\n",
       "5. Unfasten module using 6 mm hex driver. Carefully remove unit and avoid jarring internal PCB\n",
       "6. Inspect ventilation slits, board edges, and cable glands for corrosion or debris. Clean with ESD-safe tools.\n",
       "7. Install replacement DCM-4500 with vibration-resistant mounts. Torque bolts to 3 Nm.\n",
       "8. Reconnect all sensors and verify pin alignment and signal shielding. Test continuity with multimeter.\n",
       "9. Reconnect SCADA interface. Confirm handshake and initialization of module via LED indicator sequence.\n",
       "10. Run blade-deicing system diagnostics from SCADA and check status of all three sensors.\n",
       "11. Simulate icing scenario using freeze spray or SCADA override. Validate heater activity and zone cycling.\n",
       "12. Log sensor readings, ambient temperature, and activation timing. Photograph SCADA status\n",
       "screen.\n",
       "13. Monitor heating cycle for 15 minutes and verify normal deactivation and cooldown.\n",
       "14. Close junction enclosure and re-secure turbine SCADA interface. Clear error codes and log service.</td></tr><tr><td>23</td><td>56423367</td><td>14980g</td><td>Pitch motor actuator</td><td>DynoTorque PMA-540</td><td>DynoTorque PMA-540 Pitch Motor Actuator\n",
       "Professional Maintenance Manual - Pitch Actuation System\n",
       "Component Type: Pitch motor actuator\n",
       "EAN: 56423367\n",
       "Compatible Turbine Model: Aquilae V800 SmartBlade Platform\n",
       "Dimensions: 442mm - 297mm\n",
       "Weight: 14980g\n",
       "Sensor Inputs: sensor_C, sensor_G, sensor_F\n",
       "Inventory Location: Europe/Madrid\n",
       "Component Overview\n",
       "The DynoTorque PMA-540 is a high-torque electromechanical actuator responsible for adjusting the blade pitch on utility-scale turbines.\n",
       "It features a brushless DC motor encased in a waterproof and dust-resistant IP68 housing, with torque transmission via a dual-stage planetary gear system.\n",
       "Integrated position sensors ensure pitch accuracy within -0.25- across a full rotation cycle. The unit is monitored in real-time through three redundant sensor channels\n",
       "(sensor_C, sensor_G, and sensor_F), providing both position and motor health diagnostics. The actuator interfaces with the Blade Control Module (BCM) and responds to command sequences issued by the turbine controller up to 50 times per second.\n",
       "Operational Anomalies & Pre-Failure Indicators\n",
       "- Audible clicking or grinding during pitch movement\n",
       "- SCADA fault messages tied to actuator position errors\n",
       "- Blade fails to reach full feather position under test\n",
       "- Sudden torque spikes or motor overheating alarms\n",
       "- Delays exceeding 1.5s in command-to-movement intervals\n",
       "Diagnostic Fault Codes and Resolution Paths\n",
       "PMA-042\n",
       "Description: Torque deviation beyond threshold during pitch cycle. Likely gear resistance or partial jamming.\n",
       "Resolution: Verify gear lubricant levels and inspect gearbox for metal shavings. Flush and re-grease using DT-Lube 80 if signs of abrasion are found.\n",
       "PMA-105\n",
       "Description: Sensor G signal mismatch - position feedback inconsistency exceeds 0.5\n",
       "Resolution: Run a triple-sensor calibration using the BladeControl SyncTool. Replace sensor_G if calibration fails or offset persists.\n",
       "PMA-231\n",
       "Description: Overheating; internal coil temperature exceeded 95-C under low-load conditions.\n",
       "Resolution: Check ventilation routes and ambient temperature. Review turbine operation logs for signs of excessive pitch cycling due to gusting winds.\n",
       "PMA-309\n",
       "Description: Voltage drop detected during motor startup sequence. Possible capacitor drain or cable degradation.\n",
       "Resolution: Inspect the high-voltage leads between power distribution unit and motor terminals.\n",
       "Replace any discolored or brittle insulation.\n",
       "PMA-511\n",
       "Description: Unexpected directional reversal signal detected. Safety interlock engaged.\n",
       "Resolution: Check firmware for logic reversal bug (firmware <1.09). Reflash controller and validate direction mapping via diagnostic movement test.\n",
       "PMA-777\n",
       "Description: Sensor F heartbeat timeout. No telemetry received for 30+ seconds.\n",
       "Resolution: Check sensor cabling. If intact, replace sensor_F module and rebind it in the\n",
       "BladeControl firmware configuration menu.\n",
       "Routine Inspection & Lifecycle Guidance\n",
       "Perform function tests every 1,000 turbine hours. Mandatory full inspection after 7,500 hours or 2 years, whichever comes first.\n",
       "Replace unit after 15,000 hours of cumulative runtime, or if sensor variance persists beyond 0.5- over three cycles.\n",
       "Step-by-Step Certified Replacement Protocol\n",
       "1. From the SCADA interface, initiate rotor lock and enable maintenance override on all three blades.\n",
       "2. Confirm pitch position is fixed at 0- (feathered position). Verify lock with mechanical stoppers inside the hub.\n",
       "3. Disconnect all power feeds to the actuator cabinet. Wait 5 minutes before touching internal components.\n",
       "4. Access the rotor hub via the central service hatch. Use confined-space PPE and low-voltage certified gloves.\n",
       "5. Locate the DynoTorque PMA-540 motor unit within blade pitch assembly chamber. It is identifiable by the silver data plate and cable bundle from the base.\n",
       "6. Disconnect the motor power leads and label them clearly. Unplug sensor_C, sensor_G, and sensor_F lines from the hub terminal board.\n",
       "7. Use a digital torque wrench to loosen the 6 hex bolts securing the actuator casing (recommended torque removal: 160 Nm).\n",
       "8. Gently slide the actuator outward from the gear shaft. Use a winch or crane system if the unit weight exceeds 15 kg for safety compliance.\n",
       "9. Visually inspect the splined gear interface for wear, corrosion, or misalignment. Clean using certified turbine grease cleaner.\n",
       "10. Install the replacement PMA-540 by reversing the removal steps. Apply Loctite 243 to bolt threads and torque bolts to 180 Nm.\n",
       "11. Reconnect all sensor leads using anti-vibration clips and dielectric gel to prevent oxidation.\n",
       "12. Re-enable the control circuit and perform manual pitch test cycle via SCADA. Record RPM, torque, and directional response.\n",
       "13. Validate synchronization of all three sensors using BladeControl Diagnostic Suite. Ensure sensor variance < 0.2- over full rotation.\n",
       "14. Update asset tracking logs, firmware register list, and attach photographic documentation of installation and wiring.</td></tr><tr><td>24</td><td>38917763</td><td>28900g</td><td>Yaw drive system</td><td>Yaw drive system</td><td>GyroTrak YD-8000 Yaw Drive System\n",
       "Professional Maintenance Manual - Yaw Drive Subsystem\n",
       "Component Type: Yaw drive system\n",
       "EAN: 38917763\n",
       "Compatible Turbine Model: StormRay T950 Arctic-Class Wind Turbine\n",
       "Dimensions: 720mm - 488mm\n",
       "Weight: 28900g\n",
       "Sensor Interfaces: sensor_J, sensor_M, sensor_B, sensor_L\n",
       "Stock Location: Europe/Copenhagen\n",
       "Component Overview\n",
       "The GyroTrak YD-8000 is a high-torque planetary yaw drive system responsible for orienting the nacelle to face optimal wind direction.\n",
       "Engineered for arctic-class turbines, it includes a dual-motor configuration with mechanical load-sharing gears, an integrated self-locking brake system,\n",
       "and a closed-loop yaw control interface linked via four redundant sensors (sensor J, sensor M,\n",
       "sensor B, and sensor L).\n",
       "It offers up to 280 kWm peak torque and maintains directional accuracy within ±.5- in turbulent wind.\n",
       "The gearbox is filled with cold-climate synthetic grease and features internal heating coils for de-icing.\n",
       "Diagnostic Symptoms and Field Anomalies\n",
       "- Nacelle fails to track changing wind direction\n",
       "- Grinding or clunking sound from yaw mechanism\n",
       "- Brake engagement delays or brake slip under wind pressure\n",
       "- Thermal warning alarms during freezing temperatures\n",
       "- Spurious or uncontrolled nacelle rotations logged by SCADA\n",
       "Critical Error Codes and Resolution Instructions\n",
       "YAW-002\n",
       "Description: Yaw alignment exceeds tolerance - nacelle misaligned by more than 4- for over 60 seconds.\n",
       "Resolution: Check signal integrity of sensor_M and confirm yaw command signals. Inspect slewring for frozen sections or bearing resistance.\n",
       "YAW-103\n",
       "Description: Yaw motor overcurrent event detected during rotation initiation.\n",
       "Resolution: Test drive motors under no-load. Check for mechanical blockage in gear track. Review torque limiter calibration and motor brake release timing.\n",
       "YAW-228\n",
       "Description: Yaw brake failure - brake did not engage within 3 seconds after rotation stop.\n",
       "Resolution: Inspect hydraulic or electric brake actuator depending on configuration. Replace coil or re-pressurize brake line if fluid leak is found.\n",
       "YAW-319\n",
       "Description: Sensor _L failure - invalid angle telemetry or frozen data point.\n",
       "Resolution: Inspect cable shielding and waterproof seals. Replace sensor_L and reconfigure with SCADA calibration utility (firmware - 3.1).\n",
       "YAW-402\n",
       "Description: Yaw system thermal warning - gearbox lubricant below -25°C.\n",
       "Resolution: Check gearbox heater coil circuit and replace thermostat fuse if triggered. Preheat gearbox for 30 min before test rotation.\n",
       "YAW-501\n",
       "Description: Unexpected yaw movement detected - nacelle rotated without command.\n",
       "Resolution: Check for spurious signals on yaw control bus. Replace the yaw logic control board if internal watchdog faults persist.\n",
       "YAW-777\n",
       "Description: Sensor B fails redundancy check - data mismatch across axis inputs.\n",
       "Resolution: Recalibrate all yaw sensors using the GyroSync module. Replace sensor_B if variance remains >2.5- after recalibration.\n",
       "Preventative Inspection and Service Interval\n",
       "Conduct yaw functionality tests every 1,500 turbine hours. Grease top slewing ring interface every 6 months.\n",
       "Replace full YD-8000 drive assembly every 18,000 hours or after 3 high-force fault shutdown events.\n",
       "Step-by-Step Certified Maintenance Protocol\n",
       "1. Activate turbine lockout and ensure nacelle is immobilized using yaw lock pins. Confirm via SCADA lock indicator and manual inspection.\n",
       "2. Power down yaw system via control cabinet isolation switch. Confirm capacitor drain using voltmeter at yaw motor terminals.\n",
       "3. Access yaw gearbox bay through nacelle floor panel using safety harness and descent-rated platform.\n",
       "4. Document current state: photo each sensor cable, motor terminal, and gearbox label. Scan barcode into turbine maintenance database.\n",
       "5. Disconnect sensor cables from sensor J, sensor M, sensor B, and sensor L. Clean contact surfaces with isopropyl and dry with compressed air.\n",
       "6. Loosen motor terminal blocks using insulated hex key. Ensure that all phase wires are labeled and not under tension.\n",
       "7. Using overhead crane, support yaw motor casing while loosening the 8 x M16 bolts securing motor to the gearbox interface.\n",
       "8. Slide motor outward. Use inspection lamp to check gear mesh for debris or cold welding signs.\n",
       "Clean with lint-free cloth and cold-rated grease solvent.\n",
       "9. If replacing the full YD-8000 assembly, use the 4 anchor bolts on base plate to free the gearbox from the nacelle frame. Remove with lift winch.\n",
       "10. Install new unit in reverse order. Use thread locker (blue) on motor bolts and torque to 320 Nm.\n",
       "Align yaw gear teeth using paint-marked reference tooth.\n",
       "11. Reconnect all sensor lines with vibration isolators and dielectric grease. Secure to cable trays to prevent abrasion over time.\n",
       "12. Power on yaw system, run a full SCADA yaw test. Validate brake function, torque curve profile, and positional accuracy <1.5- deviation.\n",
       "13. Record ambient temperature, yaw angle, brake engagement delay, and gearbox temperature after operation in maintenance log.\n",
       "14. Seal access panel, reset SCADA lockout, and verify yaw controller logs are clear of any residual error messages.</td></tr><tr><td>25</td><td>48192934</td><td>19700g</td><td>Hydraulic pitch controller</td><td>HPC-620</td><td>HydroFlow HPC-620 Hydraulic Pitch Controller\n",
       "Professional Maintenance Manual - Hydraulic Pitch Control System\n",
       "Component Type: Hydraulic pitch controller\n",
       "EAN: 48192934\n",
       "Compatible Turbine Model: CycloneRidge V920 Seawind-Class Turbine\n",
       "Dimensions: 505mm - 330mm\n",
       "Weight: 19700g\n",
       "Sensor Interfaces: sensor_F, sensor_l, sensor_D\n",
       "Stock Location: USA/Houston\n",
       "Component Overview\n",
       "The HydroFlow HPC-620 is a precision hydraulic controller responsible for blade pitch actuation in seawind-class turbines.\n",
       "It governs hydraulic pressure flow to blade-mounted actuators, converting electronic signals into precise fluid control.\n",
       "It features an onboard microcontroller, dual-redundant solenoid valves, and PID-controlled pressure loops monitored by\n",
       "sensor_F, sensor_I, and sensor_D. Designed to withstand salt spray and high-vibration environments, the HPC-620 is sealed\n",
       "to IP67 standards, with internal filters and a high-speed pressure relief module to prevent mechanical shock during gust conditions.\n",
       "Operational Fault Symptoms and Early Indicators\n",
       "- Inconsistent blade pitch angle changes\n",
       "- Hydraulic fluid leaks inside nacelle cabinet\n",
       "- Unusual valve clicking or pressure oscillation during blade positioning\n",
       "- Pitch fails to return to feathered position after shutdown\n",
       "- High pressure alarms or fluid temperature warnings in SCADA\n",
       "Common Fault Codes and Corrective Actions\n",
       "HPC-010\n",
       "Description: Pressure deviation exceeded 15 bar over command setpoint.\n",
       "Resolution: Check hydraulic fluid level and quality. Replace filters if clogged. Recalibrate PID pressure controller using SCADA diagnostics.\n",
       "HPC-044\n",
       "Description: Solenoid A fails to respond to control signal - no actuation detected.\n",
       "Resolution: Measure coil resistance; replace solenoid if <8-. Confirm 24V signal from controller output. Inspect wiring and corrosion.\n",
       "HPC-113\n",
       "Description: Sensor _1 drift > 3 bar over 60 seconds.\n",
       "Resolution: Recalibrate sensor_1 from control panel. Replace if drift persists. Check for trapped air in hydraulic loop near sensor port.\n",
       "HPC-209\n",
       "Description: Hydraulic return line blockage suspected - flow reading from sensor_D inconsistent.\n",
       "Resolution: Inspect return line for kinks, frozen fluid, or collapsed hose. Flush system and bleed air after correction.\n",
       "HPC-310\n",
       "Description: Controller internal temperature > 85°C for more than 3 minutes.\n",
       "Resolution: Check cabinet ventilation and ambient temperature. Clean internal fan filters. Replace thermal paste on controller heat sink if dried.\n",
       "HPC-408\n",
       "Description: Uncommanded pressure spike detected - potential stuck valve or delayed\n",
       "decompression.\n",
       "Resolution: Cycle the valve manifold using override mode. If condition persists, inspect valve springs and seals. Replace faulty valve block.\n",
       "HPC-603\n",
       "Description: Sensor F communication loss > 45 seconds.\n",
       "Resolution: Verify connector seating and cable integrity. Replace sensor if no signal on oscilloscope ping test. Rebind sensor in software.\n",
       "Maintenance Schedule and Service Intervals\n",
       "Inspect controller every 2,000 hours for pressure stability, fluid temperature, and valve wear.\n",
       "Replace full unit after 16,000 hours or if more than three valve or pressure-related fault codes occur in a 90-day window.\n",
       "Step-by-Step Certified Service Procedure\n",
       "1. From the SCADA interface, disable turbine pitch control and activate hydraulic service mode to relieve system pressure.\n",
       "2. Verify pressure bleed-off by checking system gauge falls to 0 bar. Use manual bleed valve if residual pressure remains.\n",
       "3. Isolate the HPC-620 control unit via power isolation switch in the nacelle control bay. Confirm capacitor bleed before proceeding.\n",
       "4. Disconnect signal connectors for sensor_F, sensor_I, and sensor_D. Inspect for oil ingress or pin damage. Clean and dry connectors.\n",
       "5. Detach the four M10 bolts securing the controller to the mounting frame. Use sling support to avoid torque stress on piping.\n",
       "6. Slowly unscrew hydraulic supply and return fittings with absorbent pads ready to catch residual fluid. Cap open lines immediately.\n",
       "7. Inspect hydraulic manifold block for signs of leakage, cracking, or pressure plate wear.\n",
       "Photograph for records before cleaning.\n",
       "8. Install new controller, aligning pipe threads carefully and using PTFE tape rated for hydraulic fluid (ISO 52). Torque bolts to 90 Nm.\n",
       "9. Reconnect sensor lines and power terminal. Secure cables with anti-vibration clamps and route through designated nacelle channels.\n",
       "10. Restore power and initiate SCADA-controlled pressurization sequence. Watch for leaks at all junctions for 5 minutes at full operating pressure.\n",
       "11. Run valve cycling test. Validate solenoid actuation, pressure control accuracy -2 bar, and response delay under 250 ms.\n",
       "12. Check and log firmware version, operating pressure, and controller cycle count from the diagnostic panel.\n",
       "13. Update turbine asset system, noting the controller serial, install date, sensor configuration, and attach visual inspection files.\n",
       "14. Replace fluid reservoir filters if not changed in last 2,000 hours. Top off hydraulic oil and validate fluid temperature < 60-C under load.</td></tr><tr><td>26</td><td>60120291</td><td>7400g</td><td>Hydraulic pitch actuator</td><td>HPA-220</td><td>PitchMax HPA-220 Hydraulic Pitch Actuator\n",
       "Professional Maintenance Manual - Blade Pitch Control System\n",
       "Component Type: Hydraulic pitch actuator\n",
       "EAN: 60120291\n",
       "Compatible Turbine Model: StormWind SX2 Offshore-Class Turbine\n",
       "Dimensions: 260mm × 230mm\n",
       "Weight: 7400g\n",
       "Sensor Interfaces: sensor_P, sensor_H, sensor_L\n",
       "Stock Location: Sweden/Göteborg\n",
       "Component Overview\n",
       "The PitchMax HPA-220 is a high-force hydraulic actuator responsible for adjusting the blade pitch angle in response to wind conditions,\n",
       "load demands, and braking protocols. It enables fine-grain aerodynamic control and supports rapid feathering during emergency shutdowns.\n",
       "Each actuator contains a dual-chamber piston system with redundant pressure paths and dynamic seal self-lubrication.\n",
       "It integrates with sensors P (pitch angle), H (hydraulic pressure), and L (fluid level) for closed-loop control.\n",
       "Warning Signs and Performance Deviation\n",
       "- Delayed or jerky blade response to pitch commands\n",
       "- Recurrent pressure drop errors in SCADA\n",
       "- Feathering during normal operation without override\n",
       "- Visible hydraulic fluid leaks near hub actuator housing\n",
       "Error Code Index and Diagnostic Actions\n",
       "HPA-002\n",
       "Description: Sensor _P reports non-responsive pitch angle >3s during wind shift.\n",
       "Resolution: Check for hydraulic fluid obstruction or actuator stiction. Verify linkage with blade pitch shaft.\n",
       "HPA-017\n",
       "Description: Hydraulic pressure drop below 60 bar for >10s.\n",
       "Resolution: Inspect for fluid leaks, worn seals, or damaged accumulator. Recharge hydraulic system and monitor pressure.\n",
       "HPA-033\n",
       "Description: Sensor _L reports low fluid volume <25%.\n",
       "Resolution: Top off reservoir with OEM-approved hydraulic fluid. Inspect for slow leaks or tank pressure loss.\n",
       "HPA-051\n",
       "Description: Actuator cycle time exceeds 5s per 15° change.\n",
       "Resolution: Bleed air from hydraulic lines. Inspect actuator cylinder for internal friction or contamination.\n",
       "HPA-066\n",
       "Description: Hydraulic return temperature >85°C sustained.\n",
       "Resolution: Check cooler flow and pump rate. Flush and replace fluid if discolored or foamy.\n",
       "HPA-089\n",
       "Description: Inconsistent pitch angle across blades >2° spread.\n",
       "Resolution: Compare sensor P readings across all actuators. Recalibrate and inspect mounting tolerance.\n",
       "HPA-104\n",
       "Description: Unexpected feather command triggered mid-cycle.\n",
       "Resolution: Inspect SCADA control logic and emergency override input. Log event and test fail-safe protocol.\n",
       "Preventive Maintenance Strategy\n",
       "Inspect after 6,000 hours or following a hydraulic fault. Replace after 18,000 hours or if actuation exceeds cycle thresholds or leaks persist.\n",
       "Removal and Reinstallation Protocol\n",
       "1. Shut down turbine and engage rotor lock. Verify pitch system is depressurized using SCADA interface.\n",
       "2. Isolate hydraulic lines leading to the actuator. Label each line according to flow direction and chamber port.\n",
       "3. Use spill containment below actuator to catch any residual fluid during disconnection.\n",
       "4. Unbolt actuator mounting brackets using hydraulic-rated wrenches. Support actuator with lift assist.\n",
       "5. Disconnect sensor wiring for P, H, and L. Ensure clean disconnection and label with tags.\n",
       "6. Inspect actuator shaft, seals, and housing for visible damage, wear rings, or contamination.\n",
       "7. Install replacement PitchMax HPA-220, ensuring alignment with blade pitch input shaft.\n",
       "8. Reconnect hydraulic lines with torque rating of 40 Nm. Check all O-rings and fitting conditions.\n",
       "9. Reconnect sensor wiring, shielded from EMI sources. Confirm tightness and waterproof seals.\n",
       "10. Refill hydraulic system and purge air using SCADA-driven bleed mode. Monitor reservoir level and pressure.\n",
       "11. Run pitch cycle test: adjust from 0° to 90° and back. Confirm consistent timing and response curve.\n",
       "12. Compare pitch angle telemetry across all three blades for synchronization accuracy.\n",
       "13. Record actuator serial, install timestamp, and all test data. Photograph actuator if visible through hub port.\n",
       "14. Re-enable turbine yaw control and release rotor lock. Log maintenance operation and clear all system alerts.</td></tr><tr><td>27</td><td>30117620</td><td>220g</td><td>Rotational velocity optical sensor</td><td>ORS-200</td><td>SpinTrak ORS-200 Optical RPM Sensor\n",
       "Professional Maintenance Manual - Rotational Sensing System\n",
       "Component Type: Rotational velocity optical sensor\n",
       "EAN: 30117620\n",
       "Compatible Turbine Model: VoltAir V250 Onshore-Class Turbine\n",
       "Dimensions: 90mm × 60mm\n",
       "Weight: 220g\n",
       "Sensor Interfaces: sensor_Q, sensor_Y\n",
       "Stock Location: Germany/Hamburg\n",
       "Component Overview\n",
       "The SpinTrak ORS-200 is a precision optical sensor used to monitor turbine shaft rotational speed (RPM) with micron-level accuracy.\n",
       "It functions by counting light interruptions as a reflective marker passes across the sensor's optical window. It is designed for low-maintenance\n",
       "installations in high-vibration, dusty environments, with integrated self-cleaning optics and redundant\n",
       "photo-diode arrays.\n",
       "The sensor provides real-time RPM telemetry every 100 ms to the turbine controller and backup logic layer, aiding yaw response optimization and\n",
       "overspeed protection triggers. It connects via dual leads to sensors Q and Y for synchronization and feedback loop validation.\n",
       "Symptoms of Degradation or Fault\n",
       "- RPM spikes or dropouts in SCADA interface\n",
       "- Sudden overspeed or underspeed alerts\n",
       "- Out-of-sync readings between paired sensors\n",
       "- Optical signal strength or refresh interval instability\n",
       "Sensor Error Codes and Troubleshooting Actions\n",
       "ORS-005\n",
       "Description: Sensor_Q optical window obscured - light signal strength below threshold.\n",
       "Resolution: Clean sensor with isopropyl alcohol and verify alignment with marker. Replace lens if scratches are present.\n",
       "ORS-021\n",
       "Description: Inconsistent RPM readings >5% variance within 5 seconds.\n",
       "Resolution: Check for shaft vibration or mechanical looseness. Reseat mounting bracket and recalibrate baseline RPM from SCADA.\n",
       "ORS-034\n",
       "Description: Sensor_Y not synchronized with Sensor_Q\n",
       "Resolution: Inspect sync cable and pins. Reset sensor IDs in diagnostics. Replace Sensor_Y if drift exceeds 2 samples/sec.\n",
       "ORS-058\n",
       "Description: Oversedd alert - signal persists above 120% nominal RPM.\n",
       "Resolution: Confirm turbine controller overspeed protection is active. Validate with tachometer. If real, initiate brake.\n",
       "ORS-072\n",
       "Description: Signal dropout >3s - no optical pulses received.\n",
       "Resolution: Inspect wiring harness and shield continuity. Confirm marker is not dislodged. Replace sensor if pulses don't return.\n",
       "ORS-088\n",
       "Description: Redundant photodiode failure detected.\n",
       "Resolution: Sensor is still operational in fallback mode. Replace sensor at next scheduled\n",
       "maintenance window.\n",
       "ORS-099\n",
       "Description: Erratic telemetry refresh intervals from sensor Q.\n",
       "Resolution: Check for SCADA communication lag or faulty buffer. Inspect sensor clock crystal and replace if drift persists.\n",
       "Service Interval Guidelines\n",
       "Inspect every 3,000 turbine hours or immediately after any overspeed event. Replace sensor every 12,000 hours or if photodiode faults occur.\n",
       "Installation and Calibration Procedure\n",
       "1. Shut down turbine and confirm zero shaft rotation using main SCADA controller.\n",
       "2. Access sensor mounting zone near main shaft bearing housing with protective gloves and safety eyewear.\n",
       "3. Disable power to sensor bus from diagnostic panel. Confirm LEDs on ORS-200 are fully off.\n",
       "4. Disconnect signal cables from Sensor Q and Sensor Y. Label terminals for reinstallation.\n",
       "5. Remove mounting screws with precision driver and carefully detach the sensor from bracket.\n",
       "6. Inspect optical window under bright light for any dust, debris, or lens damage. Clean gently with ethanol swab.\n",
       "7. Install new ORS-200 sensor in same orientation. Tighten screws to 1.8 Nm torque. Align window with marker path.\n",
       "8. Reconnect Sensor Q and Sensor Y leads firmly. Ensure no pin misalignment or looseness.\n",
       "9. Enable power from diagnostics panel. Verify LED initialization sequence (green blink, then solid).\n",
       "10. Run RPM test sequence from SCADA: match reported RPM with manual tachometer reading within 1%\n",
       "11. Check telemetry refresh every 100 ms. Validate signal amplitude and sync between sensors Q\n",
       "and Y.\n",
       "12. Record part number, install date, and photo of mounting for logbook. Save calibration results.\n",
       "13. Monitor turbine acceleration and max RPM over 15-minute post-install cycle for stability\n",
       "14. File on-site report and re-enable turbine SCADA alarms. Tag old sensor for diagnostic return if failed.</td></tr><tr><td>28</td><td>85334441</td><td>7963g</td><td>Brake</td><td>StormHold Dynamic Brake XBR-71</td><td>Maintenance Guide: StormHold Dynamic Brake XBR-71\n",
       "Part Type: Brake\n",
       "EAN: 85334441\n",
       "Compatible Turbine: Ventis R620 High-Speed Rotor Series\n",
       "Location in Turbine: Rotor Hub - Central Brake Assembly\n",
       "Linked Sensors: sensor_A\n",
       "Part ID: SH-XBR71\n",
       "Weight: 7963g\n",
       "Dimensions: 1885mm x 312mm\n",
       "Stock Location: America/Detroit\n",
       "Component Overview\n",
       "The StormHold Dynamic Brake XBR-71 is a high-friction hydraulic brake caliper designed for extreme wind conditions.\n",
       "It provides emergency stop capabilities and acts as the primary mechanical resistance during pitch failure scenarios.\n",
       "This model integrates a single sensor feedback loop (sensor_A) and operates via closed-loop hydraulic feedback to SCADA.\n",
       "Common Symptoms and Troubleshooting Triggers\n",
       "- Audible scraping sounds during turbine slowdown.\n",
       "- Reduced braking efficiency during high wind conditions.\n",
       "- Sudden SCADA alarm linked to pressure or temperature.\n",
       "- Brake caliper overheating warning within 2 minutes of activation.\n",
       "- Visual leakage or misting of hydraulic fluid near the rotor.\n",
       "Error Codes & Corrective Actions\n",
       "BRK-700:\n",
       "Description: Brake pad pressure imbalance detected. This may cause uneven rotor deceleration, especially during emergency stops.\n",
       "Fix: Check the hydraulic actuator lines for leaks or obstructions. Calibrate the pressure regulator using the VentiDiag Toolkit v4.3.\n",
       "BRK-721:\n",
       "Description: Thermal threshold exceeded on brake disc. Possible overuse or ventilation failure.\n",
       "Fix: Inspect the disc for glazing or discoloration. Allow to cool and check thermal sensor calibration.\n",
       "If warping is visible, replace the disc.\n",
       "BRK-788:\n",
       "Description: Signal dropout from sensor_A linked to brake control. This affects real-time feedback to the SCADA system.\n",
       "Fix: Test sensor A wiring continuity using a multimeter. Replace with a shielded cable if exposed to RF interference from inverter.\n",
       "Maintenance and Replacement Interval\n",
       "Perform brake pad inspection every 2,000 operating hours or during any rotor deceleration anomaly.\n",
       "Full replacement is advised every 8,000-10,000 hours depending on terrain and wind profile (desert, offshore, etc.).\n",
       "Always replace if rotor imbalance is detected or caliper wear exceeds 2mm.\n",
       "Step-by-Step Maintenance Procedure\n",
       "1. Ensure the wind turbine is fully stopped and locked out. Apply mechanical locking pins to the rotor shaft and engage yaw locking system.\n",
       "2. Verify SCADA reports the brake system in safe mode. Redundant confirmation via local control panel is required.\n",
       "3. Open the nacelle hatch and access the brake chamber behind the main rotor\n",
       "hub. This chamber may require scaffolding or extension platform.\n",
       "4. Remove the outer nacelle casing (approx. 12 bolts). Use appropriate safety harness if working at height over 40m.\n",
       "5. Visually inspect the brake pads through the inspection port. Look for scoring, excessive wear, or hydraulic fluid misting.\n",
       "6. Disconnect the hydraulic feed line using two adjustable wrenches. Place an oil catch container to collect expelled fluid (can exceed 2L).\n",
       "7. Remove the sensor A feedback line from its socket and secure it safely out of the workspace.\n",
       "8. Unbolt the caliper mount using an M18 hex bit. The assembly is heavy-use a support arm or crane hook to hold the component during removal.\n",
       "9. Once detached, examine the piston actuator for rubber seal integrity and spring compression. Replace as needed.\n",
       "10. Install the new StormHold Dynamic Brake XBR-71 caliper. Align bolt holes and torque all mounting bolts to 250 Nm as per technical spec sheet.\n",
       "11. Reconnect the hydraulic line and refill the fluid reservoir with certified brake fluid ISO VG 46. Use air-bleed screws to remove bubbles.\n",
       "12. Reattach sensor_A to the feedback terminal and verify signal continuity via SCADA live test.\n",
       "13. Replace the nacelle casing and bolt tightly. Confirm torque settings are within range on all fasteners.\n",
       "14. Remove safety locks and re-enable turbine systems. Perform a manual test brake cycle and observe for vibration, noise, or lag.\n",
       "15. Update maintenance logs with serial number, install date, and technician ID.\n",
       "Log pre- and post-maintenance test data for audit.</td></tr><tr><td>29</td><td>99827361</td><td>1180g</td><td>Grid synchronization relay</td><td>GSR-850</td><td>SyncLogic GSR-850 Grid Synchronization Relay\n",
       "Professional Maintenance Manual - Grid Interface Subsystem\n",
       "Component Type: Grid synchronization relay\n",
       "EAN: 99827361\n",
       "Compatible Turbine Model: SkyPulse V600 Onshore Series\n",
       "Dimensions: 185mm - 122mm\n",
       "Weight: 1180g\n",
       "Sensor Interfaces: sensor_A, sensor_M, sensor_G\n",
       "Stock Location: Germany/Frankfurt\n",
       "Component Overview\n",
       "The SyncLogic GSR-850 is a high-precision relay module used for synchronizing turbine-generated power with the external grid.\n",
       "It monitors phase, frequency, and voltage conditions via integrated digital signal processors and interfaces with primary turbine\n",
       "controllers through redundant sensor inputs (sensor_A, sensor_M, sensor_G).\n",
       "The GSR-850 ensures seamless grid connection under IEC 61400-21 compliance, supports real-time blackout detection, and initiates\n",
       "island-mode disengagement if anomalies occur. Equipped with EMI shielding and overvoltage protection, the module is certified for\n",
       "lightning-prone areas and includes built-in event logging via RS-485 telemetry.\n",
       "Early Fault Indicators and Operational Symptoms\n",
       "- Delays in grid synchronization despite stable turbine output\n",
       "- Relay fails to engage during low-load startup\n",
       "- SCADA logs show phase mismatch or voltage instability\n",
       "- Abnormal relay click sequences or failure to trip\n",
       "- Event logs full or missing recent synchronization records\n",
       "Diagnostic Error Codes and Engineering Resolutions\n",
       "GSR-001\n",
       "Description: Phase mismatch exceeds 10- on grid vs generator input.\n",
       "Resolution: Adjust generator phase angle via SCADA PID tuning. If error persists, inspect sensor_M alignment and recalibrate zero crossing logic.\n",
       "GSR-019\n",
       "Description: Voltage disparity exceeds 8% between grid and generator L1-L3.\n",
       "Resolution: Check transformer tap settings. Review inverter output conditioning and test for neutral\n",
       "shift. Replace relay if internal filter cap is degraded.\n",
       "GSR-056\n",
       "Description: Frequency drift detected during synchronization window.\n",
       "Resolution: Review governor response lag. Increase governor gain if underdamped. Confirm GSR firmware - 2.1 to prevent known timing bug.\n",
       "GSR-111\n",
       "Description: Sensor_G offline for >20s.\n",
       "Resolution: Inspect telemetry cable. Replace sensor if signal continuity test fails. Re-bind device in relay firmware using maintenance port.\n",
       "GSR-205\n",
       "Description: Relay trip time exceeds 75ms threshold.\n",
       "Resolution: Measure coil response using diagnostic mode. Replace mechanical contactor if wear exceeds 15ms lag during pulse test.\n",
       "GSR-322\n",
       "Description: Event log buffer full - no new grid events recorded.\n",
       "Resolution: Download and clear event log via RS-485 console. Upgrade firmware to GSR-850v2.3+ for auto-purge support.\n",
       "GSR-808\n",
       "Description: Unknown synchronization fault - system fallback triggered.\n",
       "Resolution: Conduct full sensor diagnostics, firmware hash check, and reboot sequence. Replace GSP if fault recurs within 24 hours.\n",
       "Inspection and Replacement Schedule\n",
       "Inspect every 3,000 hours or quarterly, whichever comes first. Replace relay module every 12,000 hours or after three critical sync faults within 90 days.\n",
       "Step-by-Step Certified Replacement Procedure\n",
       "1. Disable turbine grid connection via SCADA. Confirm isolation using visual indicator at breaker panel.\n",
       "2. Power down relay circuit and lockout-tagout AC input breaker. Confirm no voltage at input terminals.\n",
       "3. Remove GSR-850 front panel using precision screwdriver. Disconnect RS-485 port and three sensor inputs.\n",
       "4. Label all connectors clearly for reassembly. Use camera to capture layout before disconnection.\n",
       "5. Loosen DIN rail latch and slide relay module outward. Handle using ESD precautions - avoid PCB contact.\n",
       "6. Inspect relay casing for soot, arc marks, or impact damage. Check for corrosion near input terminals.\n",
       "7. Install new GSR-850 by seating on DIN rail and engaging latch. Verify no pin bending or debris inside socket.\n",
       "8. Reconnect all input and telemetry cables. Tighten terminal screws to spec (1.2 Nm). Route cables through EMI shield path.\n",
       "9. Power up relay. Watch for boot sequence: LED blink pattern green-green-orange. If red LED persists, abort and recheck sensor alignment.\n",
       "10. Run synchronization simulation from SCADA. Validate phase match - 2-, voltage deviation - 3%, and relay response < 50 ms.\n",
       "11. Access relay RS-485 port and download system event log. Archive log file with timestamp and serial number.\n",
       "12. Check and record firmware version. Upgrade if < v2.3 using maintenance USB port and vendor tool.\n",
       "13. Update asset tracker with GSR serial number, install date, SCADA sync test results, and operator initials.\n",
       "14. Seal relay panel, reset breaker, and re-enable turbine grid connection. Observe first 10 mins of operation for sync stability.</td></tr><tr><td>30</td><td>77630941</td><td>620g</td><td>Tri-axis vibration sensor</td><td>VibeGuard TVS-950</td><td>VibeGuard TVS-950 Tower Vibration Sensor\n",
       "Professional Maintenance Manual - Tower Structural Monitoring\n",
       "Component Type: Tri-axis vibration sensor\n",
       "EAN: 77630941\n",
       "Compatible Turbine Model: ZephyrTech ZT1000 Coastal-Class Turbine\n",
       "Dimensions: 75mm x 75mm\n",
       "Weight: 620g\n",
       "Sensor Interfaces: sensor_Z, sensor_X, sensor_Y\n",
       "Stock Location: Portugal/Porto\n",
       "Component Overview\n",
       "The VibeGuard TVS-950 is a tri-axis MEMS-based vibration sensor installed mid-tower to monitor lateral, vertical, and torsional vibrations of the wind turbine structure.\n",
       "It captures tower resonance profiles and harmonic peaks to detect foundation shifts, loose bolts, and structural fatigue.\n",
       "This sensor outputs real-time telemetry to SCADA and anomaly detection systems, helping prevent long-term mechanical degradation.\n",
       "It interfaces with sensors Z (vertical), X (lateral), and Y (torsional) using shielded cabling and provides both raw vibration signature and FFT summaries.\n",
       "Vibration Anomalies and Failure Indicators\n",
       "- Abnormal SCADA vibration levels in idle or operating mode\n",
       "- FFT frequency anomalies not matching tower resonance\n",
       "- Signal loss or thermal fault warnings from sensor interface\n",
       "- Structural inspection triggers based on peak force readings\n",
       "Error Codes and Troubleshooting Guidelines\n",
       "TVS-001\n",
       "Description: Vertical vibration (Z) exceeds 0.6g RMS for >10s.\n",
       "Resolution: Inspect tower base and anchor bolts. Check for nearby seismic or storm activity. Tighten structural joints.\n",
       "TVS-016\n",
       "Description: Sensor _X signal flatlined - no motion detected.\n",
       "Resolution: Verify sensor mounting bracket is tight. Check cable connection. Replace sensor if MEMS axis is non-responsive.\n",
       "TVS-029\n",
       "Description: Harmonic peak detected at 12 Hz - outside known resonance profile.\n",
       "Resolution: Inspect nacelle yaw system and tower top for oscillation source. Check blade balance and gearbox mounts.\n",
       "TVS-044\n",
       "Description: Sensor _Y FFT checksum mismatch.\n",
       "Resolution: Restart SCADA FFT module. Replace sensor if checksum mismatch persists after diagnostics reboot.\n",
       "TVS-057\n",
       "Description: Temperature on sensor PCB >80°C sustained.\n",
       "Resolution: Inspect cooling airflow or proximity to electrical heater source. Relocate sensor if ambient exceeds spec.\n",
       "TVS-072\n",
       "Description: Cable impedance out of range - potential shielding failure.\n",
       "Resolution: Replace signal cable from sensor to controller. Check for physical damage or improper grounding.\n",
       "TVS-093\n",
       "Description: Sensor 7 reports peak >2.5g - suspected structural event.\n",
       "Resolution: Trigger full structural inspection. Check SCADA for simultaneous brake/yaw events. Log impact and sensor history.\n",
       "Recommended Inspection Interval\n",
       "Check sensor condition and FFT profile quarterly, especially after storms. Replace after 15,000 hours or if g-force anomalies are recorded.\n",
       "Professional Replacement Procedure\n",
       "1. Disable sensor power supply from SCADA diagnostics panel.\n",
       "2. Access mid-tower service platform. Confirm lockout tagout safety procedure is active.\n",
       "3. Locate vibration sensor housing. Use PPE due to possible oil residue or confined space.\n",
       "4. Disconnect signal cabling for axes Z, X, and Y. Label connectors for reassembly.\n",
       "5. Remove mounting screws and detach sensor from baseplate. Inspect for corrosion or crack near mounts.\n",
       "6. Visually inspect sensor body for deformation. Gently clean with antistatic cloth.\n",
       "7. Install new VibeGuard TVS-950. Use Loctite threadlocker and torque to 1.5 Nm.\n",
       "8. Reconnect signal cables. Use EMI shielding sleeves if available. Confirm snug fit.\n",
       "9. Re-enable power and verify initialization LEDs blink correctly.\n",
       "10. Run vibration baseline scan from SCADA and verify RMS values in idle mode.\n",
       "11. Compare SCADA FFT data with historical signature for sensor X, Y, and Z axes.\n",
       "12. Document sensor serial, mount location, and install time. Upload to digital maintenance log.\n",
       "13. Check torque on all tower bolts in vicinity. Monitor vibration for 2-hour post-swap period.\n",
       "14. Log replacement in SCADA ticket and archive vibration plot image if available.</td></tr><tr><td>31</td><td>55319844</td><td>15300g</td><td>lgbt power converter module</td><td>PCM-4400</td><td>VoltBridge PCM-4400 IGBT Power Converter\n",
       "Professional Maintenance Manual - Inverter & Conversion Subsystem\n",
       "Component Type: lgbt power converter module\n",
       "EAN: 55319844\n",
       "Compatible Turbine Model: TyphoonEdge E730 Offshore-Class Turbine\n",
       "Dimensions: 620mm - 340mm\n",
       "Weight: 15300g\n",
       "Sensor Interfaces: sensor_K, sensor_R, sensor_V\n",
       "Stock Location: South Korea/Busan\n",
       "Component Overview\n",
       "The VoltBridge PCM-4400 is an insulated gate bipolar transistor (IGBT) power converter used to regulate DC-to-AC inversion in offshore wind turbines.\n",
       "It enables dynamic reactive power compensation, grid code compliance, and precise frequency/voltage regulation for turbines operating under volatile wind conditions.\n",
       "Equipped with redundant cooling channels, laminated busbars, and onboard gate drivers, the PCM-4400 interfaces with sensors K, R, and V to monitor gate voltage stability,\n",
       "thermal load, and switching efficiency. The module supports soft-start sequencing and overcurrent self-protection routines. Designed for saltwater exposure and EMI resistance,\n",
       "its conformal-coated PCB and titanium-enforced heat sink deliver exceptional offshore performance.\n",
       "Electrical Fault Indicators and Thermal Signs\n",
       "- Abnormal voltage ripple under load\n",
       "- Unexpected turbine soft-start failure\n",
       "- SCADA warnings of gate faults or IGBT errors\n",
       "- Overtemperature alarms at cooling loop entry points\n",
       "- CAN bus reset events or driver timeouts in logs\n",
       "Advanced Diagnostic Codes and Solutions\n",
       "PCM-009\n",
       "Description: IGBT gate fault - gate voltage out of spec on channel 2.\n",
       "Resolution: Inspect gate driver board for burn marks or cold joints. Replace gate driver module and re-run firing sequence verification.\n",
       "PCM-041\n",
       "Description: DC-link voltage ripple exceeds 12% at 80% load.\n",
       "Resolution: Check for degraded capacitors or faulty smoothing inductors. Replace any capacitor with ESR > 2-. Verify filter stage wiring.\n",
       "PCM-112\n",
       "Description: Sensor R thermal overshoot - 90-C reached in under 2 minutes.\n",
       "Resolution: Flush liquid cooling system and check pump RPM. Refill coolant if below line. Replace sensor. R if still unstable.\n",
       "PCM-220\n",
       "Description: Soft-start abort - undervoltage condition during ramp-up.\n",
       "Resolution: Test grid-side voltage supply for sag during startup. Recheck pre-charge resistor path and inverter pre-bias settings.\n",
       "PCM-309\n",
       "Description: Sensor V drift > 1.5V under identical PWM conditions.\n",
       "Resolution: Recalibrate sensor_V from diagnostic panel. If drift remains, inspect for loose solder joints or input-stage signal noise.\n",
       "PCM-413\n",
       "Description: Overcurrent event - shutdown triggered to prevent latch-up.\n",
       "Resolution: Download fault log, Inspect power path for shorts or burned traces, Replace module if\n",
       "IGBT integrity is compromised.\n",
       "PCM-650\n",
       "Description: CAN communication fault - no data from gate driver controller.\n",
       "Resolution: Inspect CAN cabling and shield continuity. Re-flash firmware if no recovery after reset.\n",
       "Replace gate driver board if CRC errors persist.\n",
       "Service Interval and Inspection Requirements\n",
       "Inspect every 2,000 hours for temperature drift, gate timing errors, and CAN stability. Replace PCM-4400 every 15,000 hours or after 2 soft-start aborts in a 90-day period.\n",
       "Step-by-Step Power Converter Maintenance Protocol\n",
       "1. Isolate PCM-4400 system via main power switch and verify absence of voltage across terminals using certified multimeter.\n",
       "2. Remove turbine converter panel rear cover with insulated tools. Activate ESD protection and secure grounding strap.\n",
       "3. Disconnect sensor K, sensor R, and sensor V from module interface. Label connections and verify plug integrity.\n",
       "4. Unbolt top and side mounting brackets with torque-rated socket wrench. Prepare hoist or lift for safe handling of 15kg+ unit.\n",
       "5. Gently pull PCM-4400 out of rack slot, checking for snagged wires or residual thermal compound.\n",
       "Clean contact rails after removal.\n",
       "6. Inspect old module: check for capacitor bulge, PCB discoloration, corroded terminals, and foreign debris near IGBT blocks.\n",
       "7. Install new PCM-4400 in reverse order. Apply fresh thermal paste to heat sink if direct-contact model. Slide into position securely.\n",
       "8. Reconnect all sensors and ensure firm plug engagement. Torque terminal bolts to 2.8 Nm using calibrated wrench.\n",
       "9. Refill and purge converter cooling loop if drained. Monitor pump flow rate and absence of air bubbles in sight glass.\n",
       "10. Re-enable power, observe soft-start sequence from SCADA, and monitor voltage, current, and ripple readings.\n",
       "11. Run full gate driver test from diagnostic console. Validate gate signals are within -15V range and switching events are symmetric.\n",
       "12. Download post-install converter log. Check for residual faults. Archive logs and photograph serial plate for turbine records.\n",
       "13. Update digital maintenance tracker with install date, coolant batch ID, firmware version, and technician initials.\n",
       "14. Seal converter panel, reset lockout tag, and observe turbine behavior during next grid synchronization cycle.</td></tr><tr><td>32</td><td>78904420</td><td>5312g</td><td>Capacitor board</td><td>CBX-880</td><td>VoltEdge CBX-880 Capacitor Board\n",
       "Professional Maintenance Manual - Wind Turbine Power Component\n",
       "Component Type: Capacitor board\n",
       "EAN: 78904420\n",
       "Compatible Turbine Model: NordFlux NF780 Continental Onshore Series\n",
       "Dimensions: 302mm - 217mm\n",
       "Weight: 5312g\n",
       "Sensor Inputs: sensor_B, sensor\n",
       "Inventory Location: Europe/Frankfurt\n",
       "Component Overview\n",
       "The VoltEdge CBX-880 is a high-capacity, multi-phase capacitor board used in turbine power regulation systems.\n",
       "It stabilizes voltage during load transitions, stores temporary charge to prevent power dips, and plays a vital role\n",
       "in protecting sensitive electronics from surges. This board is mounted in the power conversion cabinet and directly\n",
       "interfaces with both the inverter and generator control systems. Heat-resistant up to 125-C, the CBX-860 includes an\n",
       "internal fault detection circuit and over-voltage self-discharge protection.\n",
       "Symptoms and Warning Signs\n",
       "- SCADA alert: capacitor bank undervoltage\n",
       "- Audible buzzing or arcing from cabinet\n",
       "- Sudden turbine shutoff during gusts\n",
       "- Telemetry errors or sensor_B dropouts\n",
       "- IR scan shows hotspots around CBX-880 board\n",
       "Known Error Codes and Corrective Measures\n",
       "CAP-001\n",
       "Description: Voltage retention failure detected - capacitor fails to maintain charge for required duration.\n",
       "Resolution: Test all capacitor banks using a capacitance meter. Replace any unit registering <90% of rated value. Discharge capacitors fully before handling.\n",
       "CAP-017\n",
       "Description: Thermal overrun: onboard temperature exceeded 125°C under load.\n",
       "Resolution: Check for blocked airflow in the power cabinet. Inspect cooling fan function. Apply thermal paste to heatsink if needed.\n",
       "CAP-102\n",
       "Description: Voltage ripple exceeds 8% on output rail - likely capacitor degradation or circuit board trace failure.\n",
       "Resolution: Use oscilloscope to measure ripple at test point TP4. Replace capacitor board if ripple exceeds tolerance despite clean power input.\n",
       "CAP-210\n",
       "Description: Sensor E signal dropout - board health telemetry not received for 60+ seconds.\n",
       "Resolution: Reconnect or replace the sensor_E telemetry cable. Confirm telemetry circuit is enabled via firmware (V2.4.5+).\n",
       "CAP-404\n",
       "Description: Control loop instability - phase mismatch in capacitor discharge observed during turbine ramp-down.\n",
       "Resolution: Synchronize capacitor timing using onboard DIP switch set. Calibrate discharge curve using PowerTune software module.\n",
       "Maintenance Schedule & Conditions\n",
       "Routine inspection every 2,000 hours or quarterly (whichever comes first). Mandatory replacement after 16,000 hours or if failure rate exceeds 0.4 events per 100 hours.\n",
       "Test capacitor charge hold every 6 months using calibrated capacitance diagnostic tool.\n",
       "Step-by-Step Professional Maintenance Procedure\n",
       "1. Shut down turbine and engage full grid isolation using the turbine-s master cutoff switch.\n",
       "2. Wait minimum 10 minutes to allow capacitors to self-discharge. Use a multimeter to confirm voltage across terminals is 0V.\n",
       "3. Open the power conversion cabinet using torque key and locate the CBX-980 board - typically third from the bottom.\n",
       "4. Disconnect high-voltage power leads using insulated tools. Mark each wire with numbered tags to prevent reconnection errors.\n",
       "5. Detach the two signal cables (sensor_B and sensor_E). Avoid pulling by the cable body - use connector grips only.\n",
       "6. Unscrew the four M4 screws holding the board to its aluminum standoff brackets. Support the board with your free hand while loosening.\n",
       "7. Carefully slide the board out and inspect solder joints, capacitor integrity (look for bulging or leaks), and discoloration around diodes.\n",
       "8. Compare the replacement board-s part number, firmware version, and batch code. Ensure full compatibility (rev 2.1 or later required).\n",
       "9. Insert the new board into the same slot, ensuring full seating into the backplane connector. Listen for the click to confirm alignment.\n",
       "10. Secure the board with screws, reattach power and signal connectors, and double-check grounding continuity to chassis.\n",
       "11. Close the cabinet, re-enable turbine systems, and initiate diagnostic boot. Validate telemetry\n",
       "feed from sensor E is active.\n",
       "12. Run the NordFlux Capacitor Sync Utility to verify ripple level, temperature, and charge retention under test conditions.\n",
       "13. Log all readings before and after replacement, including voltage ripple % and capacitor bank health scores. Attach photos of the board and wiring.</td></tr><tr><td>33</td><td>67219804</td><td>21550g</td><td>Main inverter module</td><td>IMX-3100</td><td>VoltForge IMX-3100 Main Inverter Unit\n",
       "Professional Maintenance Manual - Power Conversion System\n",
       "Component Type: Main inverter module\n",
       "EAN: 67219804\n",
       "Compatible Turbine Model: TyphoonEdge GX1000 High-Capacity Offshore Turbine\n",
       "Dimensions: 602mm - 345mm\n",
       "Weight: 21550g\n",
       "Sensor Inputs: sensor_A, sensor_H, sensor_E, sensor_K\n",
       "Inventory Location: Asia/Tokyo\n",
       "Component Overview\n",
       "The VoltForge IMX-3100 is a three-phase, grid-tied main inverter designed for offshore turbines with high dynamic load demands.\n",
       "It converts raw AC input from the generator to grid-synchronized AC output with active harmonic suppression and real-time phase correction.\n",
       "The IMX-3100 uses silicon-carbide IGBT transistors and features a triple-redundant signal integrity feedback system connected via sensor_A, sensor_H, sensor_E, and sensor_K.\n",
       "The unit includes fault logging, thermal protection, and self-repair cycling in the event of micro-faults.\n",
       "Designed for high-humidity and salt-air conditions,\n",
       "its internal control board is conformally coated, and the housing meets IP69K pressure wash standards.\n",
       "Common Issues and Detection Patterns\n",
       "- Turbine fails to reconnect to grid after wind lull\n",
       "- Power fluctuation warnings or phase imbalance alerts\n",
       "- Audible clicking or high-frequency inverter whine\n",
       "- Sudden shutdown during ramp-up or ramp-down\n",
       "- Error messages in SCADA tied to sensor _H or sensor _K\n",
       "Error Codes and Resolutions\n",
       "INV-001\n",
       "Description: DC input voltage out of range. Source fluctuation or generator sync loss.\n",
       "Resolution: Check generator RPM stability. If consistent, validate capacitor bank output. Replace pre-regulator if values fall below 420V under load.\n",
       "INV-056\n",
       "Description: Phase A/B mismatch exceeds 8-. Potential sensor drift or timing offset.\n",
       "Resolution: Recalibrate phase reference using InverterDiag PhaseSync Utility. If drift recurs, inspect sensor_H and rewire terminals.\n",
       "INV-104\n",
       "Description: IGBT driver thermal warning - core temperature exceeds 110°C.\n",
       "Resolution: Inspect coolant loop integrity, check thermal paste application. Replace fan or heat pipe module if temperature doesn't fall within 5 minutes.\n",
       "INV-211\n",
       "Description: Sensor_A data timeout - communication failure lasting 30 seconds.\n",
       "Resolution: Use loopback test to confirm cable function. Replace with shielded twisted pair if interference is suspected from nearby contactors.\n",
       "INV-318\n",
       "Description: Harmonic distortion detected above 5% THD at output terminal L3\n",
       "Resolution: Activate harmonic suppression mode. If error persists, test output waveform using oscilloscope and validate grounding path continuity.\n",
       "INV-404\n",
       "Description: Internal EEPROM corruption detected. Fault log invalid or checksum failure.\n",
       "Resolution: Backup config via serial console. Reflash firmware using VoltForge USB utility. Replace controller board if flashing fails twice.\n",
       "INV-509\n",
       "Description. Overcurrent shutdown triggered on all three phases. Load spike or protection trip.\n",
       "Resolution: Review turbine load logs. Isolate affected output legs, inspect contactors and IGBT modules. Replace if arc marks or fuse damage observed.\n",
       "Preventative Maintenance Schedule\n",
       "Inspect visually every 2,000 hours for dust or corrosion. Run phase balance test monthly.\n",
       "Full replacement recommended after 20,000 hours of operation or at first sign of repeated fault cycling within 72 hours.\n",
       "Replacement Procedure - Certified Technician Protocol\n",
       "1. Initiate full turbine shutdown and engage E-Stop mode from SCADA. Confirm complete grid disconnection before accessing the cabinet.\n",
       "2. Wait 15 minutes to allow capacitor discharge. Confirm zero voltage using a CAT IV-rated multimeter across input/output terminals.\n",
       "3. Open the inverter bay cabinet (typically section C1) using a keyed access panel. Use an anti-static strap before touching internal components.\n",
       "4. Visually inspect the IMX-3100 unit for burn marks, bulging capacitors, or debris. Document any visual abnormalities.\n",
       "5. Disconnect the main AC input, output, and DC bus connectors. Use torque-controlled insulated tools. Tag all cables.\n",
       "6. Unplug sensor connectors for sensor A, sensor E, sensor H, and sensor K. Confirm pins are not oxidized or bent.\n",
       "7. Remove the unit from the rack using two personnel. Weight exceeds 21kg; use shoulder harness or crane winch for safety.\n",
       "8. Inspect rear mounting points and rails for corrosion or deformation. Clean using industrial-grade contact cleaner and corrosion inhibitor.\n",
       "9. Install the new IMX-3100 unit, ensuring complete backplane contact and torquing mounting bolts to 65 Nm.\n",
       "10. Reconnect all signal and power cables. Apply dielectric grease on sensor ports. Double-check\n",
       "AC phase alignment: L1, L2, L3.\n",
       "11. Enable power feeds and monitor for soft boot LED pattern (green-yellow-yellow). If fault LED is red, abort and recheck sensor K.\n",
       "12. Run the full inverter diagnostic from SCADA: verify output voltage waveform, THD < 3.5%, and balance across phases.\n",
       "13. Record and log firmware version, hardware revision, and system serial number into the maintenance management system.\n",
       "14. Perform final enclosure seal test using compressed air and verify IP69K compliance on housing gaskets.</td></tr><tr><td>34</td><td>30981244</td><td>390g</td><td>Ultrasonic wind speed and direction sensor</td><td>WindCore ANA-750</td><td>WindCore ANA-750 Ultrasonic Anemometer\n",
       "Professional Maintenance Manual - Wind Monitoring System\n",
       "Component Type: Ultrasonic wind speed and direction sensor\n",
       "EAN: 30981244\n",
       "Compatible Turbine Model: GreenSpire GS250 Rural Turbine Series\n",
       "Dimensions: 65mm x 105mm\n",
       "Weight: 390g\n",
       "Sensor Interfaces: sensor_W, sensor_D\n",
       "Stock Location: Germany/Bremen\n",
       "Component Overview\n",
       "The WindCore ANA-750 is a high-precision ultrasonic anemometer used to measure wind speed and direction at the nacelle.\n",
       "It operates without moving parts, using time-of-flight measurements of ultrasonic pulses between transducers.\n",
       "This sensor offers fast response in all weather conditions and is resilient to icing, dust, and mechanical wear.\n",
       "It integrates with sensor_W (wind speed) and sensor_D (wind direction) channels to provide real-time data to yaw, pitch, and power regulation systems.\n",
       "Common Faults and Operational Warnings\n",
       "- SCADA shows constant wind speed despite visible wind\n",
       "- Sudden gust detection with no wind pattern to match\n",
       "- Wind direction shows significant offset from nacelle orientation\n",
       "- Tilt or echo quality errors in diagnostics panel\n",
       "Diagnostic Codes and Recommended Actions\n",
       "ANA-005\n",
       "Description: Sensor W reports wind speed = 0 for >90 seconds in active wind environment.\n",
       "Resolution: Inspect sensor for contamination or bird nesting. Clean transducers and check SCADA for signal dropouts.\n",
       "ANA-011\n",
       "Description: Direction offset >25° compared to turbine yaw alignment.\n",
       "Resolution: Recalibrate sensor. D: Confirm sensor is level and properly aligned to turbine nose.\n",
       "ANA-026\n",
       "Description: Temperature compensation failure - signal drift at low temperatures.\n",
       "Resolution: Run temperature compensation test. Replace if internal thermal correction fails diagnostic.\n",
       "ANA-034\n",
       "Description: Sensor communication timeout with SCADA bus.\n",
       "Resolution: Verify cabling and connector integrity. Replace cable or power injector if voltage is unstable.\n",
       "ANA-045\n",
       "Description: Ultrasonic echo quality below threshold.\n",
       "Resolution: Clean all transducer heads with alcohol wipe. If problem persists, replace sensor head module.\n",
       "ANA-063\n",
       "Description: Internal tilt detected >10° for over 5 minutes.\n",
       "Resolution: Check sensor mounting plate and bolts. Realign or remount to eliminate tilt.\n",
       "ANA-078\n",
       "Description: Wind speed fluctuation >80% in 3 seconds (false gust detected).\n",
       "Resolution: Check for reflective obstruction near sensor. Apply gust filtering in SCADA settings if terrain-dependent.\n",
       "Service Interval and Calibration Policy\n",
       "Clean transducers every 4 months. Recalibrate direction alignment annually or after SCADA drift alerts. Replace sensor every 4 years or on drift/echo failure.\n",
       "Safe Replacement and Alignment Workflow\n",
       "1. Deactivate turbine yaw movement. Confirm safe roof access and nacelle lockout.\n",
       "2. Climb to nacelle rooftop and secure PPE harness. Identify ANA-750 mount near weather station pole.\n",
       "3. Disconnect signal cable from sensor base. Shield connector with protective cap.\n",
       "4. Unfasten mounting screws (usually 4 mm Allen head) and gently lift sensor off mount.\n",
       "5. Inspect sensor body and transducer heads for contamination, insect ingress, or damage.\n",
       "6. Install new WindCore ANA-750, ensuring exact north-facing alignment for accurate direction data.\n",
       "7. Tighten mounting screws evenly to avoid tilt. Use digital level if needed.\n",
       "8. Reconnect signal cable. Check for corrosion or wear on contacts before securing.\n",
       "9. Return to SCADA interface and check for live wind speed and direction telemetry.\n",
       "10. Run self-test and verify echo quality, internal temperature, and tilt angle are nominal.\n",
       "11. Simulate wind changes (if possible) or wait for natural variance to validate response.\n",
       "12. Log replacement timestamp, sensor serial, and telemetry readings at install.\n",
       "13. Photograph mount position and attach to service record for traceability.\n",
       "14. Re-enable yaw control and monitor 1-hour wind tracking session for anomalies.</td></tr><tr><td>35</td><td>41227053</td><td>16665g</td><td>controller card</td><td>WindSync ControlMaster X-982</td><td>Maintenance Guide: WindSync ControlMaster X-982\n",
       "Part Type: controller card #2\n",
       "EAN: 41227053\n",
       "Compatible Turbine: Stratus T900 Variable Pitch Turbine\n",
       "Location in Turbine: Internal Nacelle Compartment - Slot 2B\n",
       "Involved Sensors: sensor_B, sensor_E, sensor_D\n",
       "Part Code: WSX982-CTRL\n",
       "Weight: 16665g\n",
       "Dimensions: 443mm x 227mm\n",
       "Stock Location: America/Los Angeles\n",
       "Typical Symptoms & Diagnostic Indicators\n",
       "- Red LED blinking in a 2-blink pattern followed by pause.\n",
       "- Nacelle pitch control fails to respond in high winds.\n",
       "- Diagnostic interface shows intermittent connection to sensor_B and sensor_E.\n",
       "- Error logs contain CMX-prefixed faults.\n",
       "- Occasional SCADA 'heartbeat lost' messages for this controller slot.\n",
       "Detailed Error Code Descriptions\n",
       "CMX-101:\n",
       "Description: The system has detected an unstable voltage input on the main board. This can cause\n",
       "frequent rebooting or erratic turbine blade responses during gusts.\n",
       "How to Fix: Check the main bus power connectors for corrosion. Replace fuse F4 on the board and\n",
       "verify voltage stability using a multimeter set to 20V DC.\n",
       "CMX-234:\n",
       "Description: Internal watchdog timer failed to reset the firmware sequence. This can result in partial\n",
       "initialization or non-responsive controller behavior.\n",
       "How to Fix: Connect via USB Debug Port and reflash the firmware using the provided OEM toolkit. If error persists, consider replacing the EEPROM module.\n",
       "CMX-503:\n",
       "Description: Communication timeout between the blade pitch actuator module and the main controller card. Wind pitch adjustments may lag or fail.\n",
       "How to Fix: Inspect the actuator signal cable for wear. Reconnect or replace the cable. If issue remains, replace the card and recalibrate with ActuatorSync.\n",
       "Replacement & Maintenance Timing\n",
       "Recommended replacement after 12,000 operational hours or when showing symptoms of signal instability.\n",
       "Maintenance check every 1,500 hours to verify firmware stability and connection integrity.\n",
       "Step-by-Step Maintenance Instructions\n",
       "1. Ensure the turbine is fully powered down and isolated from the electrical grid. Confirm lock-out tag is applied at the master circuit breaker.\n",
       "2. Use a grounding strap to prevent electrostatic discharge before accessing internal components.\n",
       "3. Open the main nacelle housing and locate the controller tray marked CC2-TRAY. This is typically beneath the SCADA interface block.\n",
       "4. Unplug all sensor connectors (labelled S-A through S-F) from the controller. Mark each with color-coded tape for easy reconnection.\n",
       "5. Use a T25 screwdriver to unscrew all mounting screws along the perimeter of the controller card.\n",
       "6. Gently slide the card out of its PCI-style connector. Examine edge connectors for black marks or corrosion.\n",
       "7. Use a brush and isopropyl alcohol to clean the connector interface and fan blades within the tray.\n",
       "8. Insert the new WindSync ControlMaster X-982' card into the CC2-TRAY, aligning pins perfectly\n",
       "before applying any force.\n",
       "9. Secure with original mounting screws and reconnect all labeled sensor inputs.\n",
       "10. Re-seal the housing and remove any tools or debris from the nacelle workspace\n",
       "11. Power on the turbine and initiate diagnostic mode. Observe LED codes on the controller (expect solid green).\n",
       "12. Launch the WindTurbineConfig utility and run full diagnostic sync, verifying all sensor readings.\n",
       "13. Update the asset management system with the new controller's serial number.</td></tr><tr><td>36</td><td>80452271</td><td>880g</td><td>Yaw position optical encoder</td><td>YawSense ENC-310</td><td>YawSense ENC-310 Rotary Position Encoder\n",
       "Professional Maintenance Manual - Nacelle Orientation Sensing\n",
       "Component Type: Yaw position optical encoder\n",
       "EAN: 80452271\n",
       "Compatible Turbine Model: HeliVolt H1200 Medium-Wind Turbine\n",
       "Dimensions: 110mm × 75mm\n",
       "Weight: 880g\n",
       "Sensor Interfaces: sensor_A, sensor_M\n",
       "Stock Location: Denmark/Odense\n",
       "Component Overview\n",
       "The YawSense ENC-310 is a high-precision rotary position encoder used to track nacelle orientation relative to wind direction.\n",
       "This optical encoder uses a coded disc and dual photodetectors to output angular position in real-time via quadrature signal and serial interface.\n",
       "It is vital for yaw system feedback and storm misalignment correction. The ENC-310 features anti-vibration mounts, sealed bearings, and operates with\n",
       "Sensor_A (angular position) and Sensor_M (motor feedback correlation) to maintain real-time alignment to wind vector.\n",
       "Symptoms Indicating Encoder Issues\n",
       "- Incorrect yaw angle displayed in SCADA\n",
       "- Jerky or reversed nacelle movement\n",
       "- Error codes related to Sensor_A or Sensor_M\n",
       "- Intermittent loss of yaw tracking during rotation\n",
       "Error Code Table and Remedial Actions\n",
       "ENC-003\n",
       "Description: Sensor_A signal noise exceeds 15% jitter.\n",
       "Resolution: Check encoder disc for smudge or damage. Clean or replace disc. Shield signal cable from Elm.\n",
       "ENC-019\n",
       "Description: Yaw offset >5° between Sensor A and Sensor M\n",
       "Resolution: Recalibrate yaw zero point. Replace encoder if drift persists. Check mounting bracket stability.\n",
       "ENC-025\n",
       "Description: No signal from Sensor_M during yaw motion.\n",
       "Resolution: Inspect cable routing and connector integrity. Confirm gear coupling is intact and rotating properly.\n",
       "ENC-047\n",
       "Description: Excessive vibration detected at encoder mount.\n",
       "Resolution: Check anti-vibration gaskets. Torque mount bolts. Inspect for nacelle oscillation.\n",
       "ENC-062\n",
       "Description: Quadrature channel B signal missing.\n",
       "Resolution: Replace encoder output cable or entire unit. Verify channel logic at SCADA diagnostics panel.\n",
       "ENC-073\n",
       "Description: Encoder temperature >70°C sustained for 10 min.\n",
       "Resolution: Check for nearby heat source or airflow blockage. Replace encoder if thermal damage is observed.\n",
       "ENC-091\n",
       "Description: Unexpected direction reversals in yaw telemetry.\n",
       "Resolution: Validate yaw control software version. Replace encoder if directional integrity test fails.\n",
       "Inspection Cycle and Replacement Criteria\n",
       "Inspect every 6,000 turbine hours or after abnormal yaw events. Replace encoder after 18,000 hours or if mechanical play is detected in shaft fit.\n",
       "Replacement and Calibration Instructions\n",
       "1. Disable yaw motor from SCADA. Confirm turbine is parked and rotor is locked.\n",
       "2. Access encoder mount point on nacelle rotation base. Wear fall protection and helmet.\n",
       "3. Disconnect Sensor A and Sensor M signal cables from the encoder. Tag cables clearly for later reconnection.\n",
       "4. Unscrew encoder cover and inspect for dust, oil, or corrosion. Clean gently with air and optical wipes.\n",
       "5. Remove encoder disc and inspect for scratches, delamination, or misalignment. Replace if worn.\n",
       "6. Unbolt encoder housing from mount bracket. Carefully detach without damaging shaft coupling.\n",
       "7. Install new encoder onto shaft with alignment key inserted. Torque bolts to 2.5 Nm and verify fit.\n",
       "8. Reattach signal cables. Check for intact shielding and tight terminals. Avoid crossing wires.\n",
       "9. Reconnect encoder to yaw controller. Reboot module and verify SCADA detects encoder with no error.\n",
       "10. Run yaw test: rotate nacelle ±15° and compare SCADA angle with mechanical markings.\n",
       "11. Confirm directionality, jitter level <5%, and yaw-motor sync with sensor_M telemetry.\n",
       "12. Record encoder serial, install timestamp, and temperature during test cycle.\n",
       "13. Log calibration values and verify yaw alignment is within tolerance. Store photos if needed.\n",
       "14. Enable yaw motor. Monitor 1-hour yaw tracking log to verify stability and correction behavior.</td></tr><tr><td>1</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>2</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>3</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>4</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>5</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>6</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>7</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>8</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>9</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>10</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>11</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>12</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>13</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>14</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>15</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>16</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>17</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr><tr><td>18</td><td>Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
       "</td><td>Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.</td><td>Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
       "</td><td>Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
       "</td><td></td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         19,
         "96356234",
         "4014g",
         "Anemometer",
         "AeroSense VTX-220",
         "AeroSense VTX-220 Precision Anemometer\nProfessional Maintenance Manual - Wind Turbine Component\nPart Type: Anemometer\nEAN: 96356234\nCompatible Turbine: Skylance XR550 Coastal Turbine Platform\nSensors Used: sensor D, sensor C\nDimensions: 364mm - 286mm\nWeight: 4014g\nStock Zone: America/New_York\nComponent Overview\nThe AeroSense VTX-220 is a precision optical-cup anemometer designed to operate in harsh marine and coastal environments.\nIt delivers high-frequency wind speed measurements to the turbine-s main control unit via sensor_D and communicates backup diagnostics via sensor_C.\nEquipped with a UV-resistant ABS housing and dual-bearing stainless steel shaft, it maintains sub-0.1s response latency in gusts up to 180 km/h.\nRecognizing Issues\nOperators may notice discrepancies between actual weather data and turbine logs, delayed SCADA wind alerts, or sudden drops in power generation.\nTypical warning signs include inconsistent wind-speed readings or visible physical obstruction of the rotor cups.\nError Codes & Troubleshooting\nANM-100\nDescription: Low RPM detected despite wind presence. Likely mechanical blockage or bearing failure.\nResolution: Inspect the cup rotor for salt deposits or bird interference. Apply approved cleaning fluid and rotate manually to check bearing friction.\nANM-201\nDescription: Intermittent signal loss on sensor_D channel. Data irregularities exceed 15% over 10 minutes.\nResolution: Replace connector or test with alternate input pin. Ensure waterproof sealing and corrosion-free contact surface.\nANM-310\nDescription: Excessive vibration detected beyond 2.0g, Shaft imbalance or mounting flange fault suspected.\nResolution: Use vibration sensor diagnostics to assess severity. Tighten mounting base, and recalibrate pitch compensation in firmware settings.\nRecommended Maintenance Schedule\nInspect every 2,500 operational hours. Replace every 12,000 hours or if more than two error events occur in a 90-day period.\nPreventative cleaning every 6 months in salt-air environments is strongly advised.\nCertified Maintenance Procedure\n1. Engage turbine maintenance mode from SCADA dashboard. Confirm rotor lock and system isolation.\n2. Access the rooftop nacelle via secured ladder or lift. Wear high-visibility PPE and fall arrest gear rated for 100kg minimum.\n3. Locate the anemometer mast on the rear left corner of the nacelle shell. Document physical condition with photos before removal.\n4. Disconnect the twin sensor cable junction box under the mast. Use an IP67-rated cap to cover open leads during maintenance.\n5. Using a torque wrench, loosen the 3 mounting bolts (13mm) securing the anemometer base. Hold the unit from above to prevent fall.\n6. Gently lift the AeroSense VTX-220 unit upward and inspect the shaft coupling area for corrosion or fatigue.\n7. Clean the mounting flange area with alcohol-based surface cleaner and dry thoroughly using lint-free cloth.\n8. Prepare the replacement unit by verifying serial number, firmware revision, and alignment pin compatibility.\n9. Insert the VTX-220 anemometer onto the flange, align the guiding notch, and secure all bolts to 28 Nm torque.\n10. Reconnect sensor D and sensor C to the color-coded terminal blocks in the junction box. Use dielectric grease on terminals.\n11. Run a full sensor check using the Skylance Diagnostic Utility. Validate RPM, signal variance, and latency under simulated gust input.\n12. Recalibrate the wind-speed baseline against local weather station data. Accept results only if variance < 1.2%.\n13. Document replacement with timestamp, technician ID, and attach photos to maintenance log in the turbine management system."
        ],
        [
         20,
         "50311982",
         "9200g",
         "Spring-applied hydraulic brake caliper",
         "SBC-1300",
         "BrakeTech SBC-1300 Spring-Applied Brake Caliper\nProfessional Maintenance Manual - Rotor Braking System\nComponent Type: Spring-applied hydraulic brake caliper\nEAN: 50311982\nCompatible Turbine Model: AeroVolt AV4000 High-Speed Wind Turbine\nDimensions: 280mm × 240mm\nWeight: 9200g\nSensor Interfaces: sensor_B, sensor_T, sensor_R\nStock Location: Italy/Torino\nComponent Overview\nThe BrakeTech SBC-1300 is a heavy-duty spring-applied, hydraulically released brake caliper used to halt the turbine rotor during maintenance, overspeed events, or emergency stops.\nIt applies immense clamping force through hardened steel pads onto the turbine disc. The caliper uses redundant hydraulic releases and preload springs,\nensuring default engagement during pressure loss. It is integrated with Sensor_B (brake status), Sensor_T (temperature), and Sensor_R (release pressure feedback).\nSymptoms of Brake System Degradation\n- Delayed or ineffective braking in SCADA logs\n- Elevated brake temperatures during standard operation\n- Visible pad wear or oil leakage around caliper base\n- Abnormal noise or rotor drag during idle state\nBrake Fault Codes and Service Instructions\nSBC-007\nDescription: Sensor_B reports brake engaged but rotor speed remains >5 RPM.\nResolution: Check pad wear or oil contamination. Confirm hydraulic release valve integrity and spring tension.\nSBC-021\nDescription: Sensor R pressure below 80 bar while in release mode.\nResolution: Recharge hydraulic accumulator. Inspect for line blockage or degraded fluid viscosity.\nSBC-039\nDescription: Brake temperature exceeds 95°C during braking cycle.\nResolution: Allow cooldown. Check friction material and rotor disc for glazing. Replace if warped.\nSBC-044\nDescription: Pad wear sensor triggered - thickness <3 mm.\nResolution: Replace pads immediately. Inspect caliper piston travel distance and reset wear indicator.\nSBC-066\nDescription: Hydraulic release response delay >1.5s.\nResolution: Bleed hydraulic line and inspect solenoid valve function. Replace if sluggish.\nSBC-082\nDescription: Caliper does not return to full open position.\nResolution: Inspect spring assembly for binding. Lubricate guide pins and test manual release.\nSBC-097\nDescription: Sensor _T drift between paired calipers >12°C.\nResolution: Recalibrate Sensor_Ts. If discrepancy persists, replace affected sensor. Verify thermal coupling.\nMaintenance Frequency and Brake Lifecycle\nInspect every 2,000 turbine hours or after every emergency stop. Replace brake caliper after 12,000 hours or 3 pad replacements.\nCaliper Removal and Safety Replacement Steps\n1. Park turbine, engage rotor lock, and confirm SCADA brake command is disengaged.\n2. Deactivate hydraulic pressure supply and drain residual line pressure safely.\n3. Access brake caliper zone at main shaft rear. Use platform lift or nacelle access hatch.\n4. Inspect for oil leaks, burn marks, or scoring on brake disc. Document any visible anomalies\n5. Disconnect sensor wiring for B, T, and R with ESD protection. Label connectors clearly.\n6. Unbolt caliper from its mount using heavy-duty tools. Use hoist or pulley system to support caliper weight.\n7. Inspect spring pack, return guides, and pistons for corrosion or misalignment. Replace if damaged.\n8. Mount new SBC-1300 caliper in exact orientation. Torque bolts to 85 Nm using cross pattern.\n9. Reconnect hydraulic line and perform leak test at 120 bar. Reattach sensor wiring and shield cable ends.\n10. Initiate SCADA brake release test. Confirm full retraction and zero drag on brake disc.\n11. Perform emergency brake test at low rotor RPM. Verify response time <1.2s and full stop within specs.\n12. Check pad contact area for uniform wear. Log disco temperature with Sensor_T\n13. Log all actions, torque values, and pressure readings. Photograph installation site and serial plate.\n14. Clear brake fault codes in SCADA and monitor first 3 hours of rotor activity for anomalies."
        ],
        [
         21,
         "74182930",
         "8900g",
         "Turbine nacelle cooling fan array",
         "ClimaRotor CFA-9000",
         "ClimaRotor CFA-9000 Nacelle Cooling Fan Array\nProfessional Maintenance Manual - Thermal Regulation System\nComponent Type: Turbine nacelle cooling fan array\nEAN: 74182930\nCompatible Turbine Model: Solaris T680 Low-Noise Turbine\nDimensions: 880mm - 270mm\nWeight: 8900g\nSensor Interfaces: sensor_Q, sensor_T, sensor_E\nStock Location: Spain/Valencia\nComponent Overview\nThe ClimaRotor CFA-9000 is an industrial-grade nacelle cooling fan array, composed of four variable-speed axial fans mounted in a stainless-steel frame.\nIt ensures thermal regulation of inverter cabinets, hydraulic controllers, and sensor blocks.\nThe unit operates under adaptive thermal control via SCADA and reads ambient and component temperatures from sensor_Q, sensor_T, and sensor_E.\nIt features hot-swap fans, integrated anti-condensation heaters, low-noise impellers, and IP56-certified enclosures.\nThe CFA-9000 prevents thermal derating and electronic component degradation in high-temperature environments and is critical to maintaining optimal turbine performance.\nThermal Fault Symptoms and Behavior\n- Excessive inverter cabinet temperatures\n- SCADA alarms related to Sensor Q, Sensor T, or Sensor E\n- Sudden fan speed drop or loud vibration during operation\n- Moisture buildup or fog inside nacelle electrical compartments\n- Error messages from fan controller or PWM signal anomalies\nCritical Error Codes and Technical Resolutions\nCFA-012\nDescription: Fan #2 RPM < 400 during operation at 80% PWM duty cycle.\nResolution: Verify power connection and fan controller output. If fan does not spin freely by hand, replace motor bearing assembly.\nCFA-035\nDescription: Sensor_Q temperature exceeds 55-C, airflow nominal.\nResolution: Check for external air blockage, clogged intake filter, or sensor calibration drift. Replace filter and recalibrate sensor_Q if needed.\nCFA-074\nDescription: Condensation heater fault - coil resistance above 500-.\nResolution: Check heater coil continuity and relay response. Replace heater module if damaged.\nConfirm ambient humidity > 60% before retest.\nCFA-109\nDescription: Sensor T reports thermal runaway - >20-C rise in under 2 minutes.\nResolution: Inspect inverter cabinet ventilation. Ensure internal fans are operational. If airflow from\nCFA-9000 is reduced, replace impeller or controller.\nCFA-201\nDescription: Fan controller board communication timeout.\nResolution: Reset controller. Check CAN bus connection and terminate signal ends. Replace controller board if failure recurs more than twice daily.\nCFA-313\nDescription: Fan vibration exceeds limit threshold during ramp-up.\nResolution: Inspect fan blades for debris or cracks. Balance impellers or replace affected unit. Use\nvibration dampeners if repeatedly triggered\nCFA-404\nDescription: Sensor_E offline - no temperature data for >45 seconds.\nResolution: Inspect cabling and sensor port. If connection stable, replace sensor_E and bind via SCADA maintenance utility.\nInspection and Maintenance Interval\nInspect CFA-9000 every 1,500 turbine hours or monthly in high-temperature regions. Replace individual fans after 10,000 hours or when vibration threshold is exceeded.\nStep-by-Step Field Technician Procedure\n1. Access turbine SCADA and disable CFA-9000 module through cooling subsystem controls. Verify module shutdown via indicator LED.\n2. Power off auxiliary cooling bus and lockout. Confirm voltage zero across fan terminals with multimeter before opening cabinet.\n3. Open nacelle top panel and locate CFA-9000 mounting frame. Use safety harness and elevated service platform for access.\n4. Visually inspect the four axial fans for debris, corrosion, or cracked blades. Photograph any damage before continuing.\n5. Disconnect PWM signal and power cables for each fan, as well as the CAN controller connection.\nTag all connections clearly.\n6. Remove faulty fan(s) by unscrewing mounting bracket bolts. Fans are hot-swappable, but full power-off is advised for safety.\n7. Clean mounting surfaces, apply anti-vibration foam strips if degraded. Install new fan with bolts torqued to 4 Nm.\n8. Reconnect all power, signal, and CAN lines. Confirm secure and correct connection layout from pre-labeled cables.\n9. Inspect condensation heater terminals for oxidation. Test heater resistance and replace unit if value > 400-.\n10. Power on auxiliary cooling system. From SCADA, run fan diagnostic cycle and monitor RPM, temperature, and vibration metrics.\n11. Check SCADA fan logs for normal PWM ramping and sensor readings < 50-C across all points.\nVerify CAN controller uptime.\n12. Update turbine maintenance log with fan serial numbers, install date, measured resistance, and thermal calibration values.\n13. If applicable, reset thermal error counters and clear any latched CFA-9000 error codes from SCADA system.\n14. Close nacelle panel, release lockout, and observe cooling system during normal turbine operation for 15 minutes post-repair."
        ],
        [
         22,
         "91020488",
         "3400g",
         "Blade deicing control module",
         "DCM-4500",
         "CryoBlade DCM-4500 Blade Deicing Control Module\nProfessional Maintenance Manual - Blade Thermal System\nComponent Type: Blade deicing control module\nEAN: 91020488\nCompatible Turbine Model: SkyWind S3000 Mountain-Class Turbine\nDimensions: 180mm × 140mm\nWeight: 3400g\nSensor Interfaces: sensor_I, sensor_T, sensor_V\nStock Location: Norway/Bergen\nComponent Overview\nThe CryoBlade DCM-4500 is a critical electronic control module that governs the thermal deicing of turbine blades in sub-zero conditions.\nIt modulates heating filament activity embedded in blades based on real-time ice detection, ambient temperature, and wind velocity inputs.\nThe system maintains blade balance by cycling heating zones and using predictive icing models.\nIt interfaces with sensors I (icing presence), T (external temperature), and V (wind velocity) to dynamically adjust power draw\nacross blade zones. The DCM-4500 features redundant heating channel control, a thermal load balancer, and anti-condensation pulse mode.\nKey Failure Symptoms and Observations\n- Persistent blade icing despite active deicing system\n- Long heating delay at turbine startup\n- SCADA errors referencing sensors I, T, or V\n- Unusual heat signature patterns on blade thermal scan\nDiagnostic Fault Codes and Remediation Steps\nDCM-012\nDescription: Heater channel 2 draws >10A over 30 seconds.\nResolution: Check for short circuit in blade zone 2 wiring. Replace any frayed conductor or damaged connector block.\nDCM-033\nDescription: Sensor T temperature drift exceeds ±3°C in 5 minutes.\nResolution: Inspect sensor location for thermal shadowing. Replace sensor if internal drift test fails in diagnostics.\nDCM-048\nDescription: Sensor 1 reports ice presence while heater active >10 minutes.\nResolution: Run heater test mode. If ice not cleared, inspect filament resistance and continuity.\nReplace blade heater section.\nDCM-061\nDescription: Communication fault with Sensor V.\nResolution: Check wiring from nacelle to hub. Replace twisted pair if continuity fails. Restart module to re-establish sync.\nDCM-084\nDescription: Overtemperature on deicing controller mainboard >95°C.\nResolution: Inspect cooling fan and thermal paste. Check nacelle vent flow. Shut down controller if passive cooling fails.\nDCM-109\nDescription: Zone activation delay >15s vs target.\nResolution: Confirm system voltage stability. Recalibrate thermal switches. Replace relay if sluggish behavior persists.\nDCM-210\nDescription: Anti-condensation pulse mode active for >60 minutes.\nResolution: Review SCADA humidity and blade surface conditions. Override if unnecessary. Inspect pulse modulation board.\nPreventive Maintenance and Replacement Schedule\nInspect the DCM-4500 quarterly and after each severe weather event. Replace after 20,000 operational hours or upon thermal degradation above 90°C.\nDetailed Maintenance and Swap Instructions\n1. Shut down turbine and isolate DCM-4500 from SCADA controller interface.\n2. Use lockout tagout procedures at the electrical cabinet and confirm safe access with voltmeter.\n3. Open access panel near root of blade. Locate DCM-4500 inside sealed junction enclosure.\n4. Disconnect all sensor inputs (I, T, V) and mark wiring using labeled ties. Photograph connections.\n5. Unfasten module using 6 mm hex driver. Carefully remove unit and avoid jarring internal PCB\n6. Inspect ventilation slits, board edges, and cable glands for corrosion or debris. Clean with ESD-safe tools.\n7. Install replacement DCM-4500 with vibration-resistant mounts. Torque bolts to 3 Nm.\n8. Reconnect all sensors and verify pin alignment and signal shielding. Test continuity with multimeter.\n9. Reconnect SCADA interface. Confirm handshake and initialization of module via LED indicator sequence.\n10. Run blade-deicing system diagnostics from SCADA and check status of all three sensors.\n11. Simulate icing scenario using freeze spray or SCADA override. Validate heater activity and zone cycling.\n12. Log sensor readings, ambient temperature, and activation timing. Photograph SCADA status\nscreen.\n13. Monitor heating cycle for 15 minutes and verify normal deactivation and cooldown.\n14. Close junction enclosure and re-secure turbine SCADA interface. Clear error codes and log service."
        ],
        [
         23,
         "56423367",
         "14980g",
         "Pitch motor actuator",
         "DynoTorque PMA-540",
         "DynoTorque PMA-540 Pitch Motor Actuator\nProfessional Maintenance Manual - Pitch Actuation System\nComponent Type: Pitch motor actuator\nEAN: 56423367\nCompatible Turbine Model: Aquilae V800 SmartBlade Platform\nDimensions: 442mm - 297mm\nWeight: 14980g\nSensor Inputs: sensor_C, sensor_G, sensor_F\nInventory Location: Europe/Madrid\nComponent Overview\nThe DynoTorque PMA-540 is a high-torque electromechanical actuator responsible for adjusting the blade pitch on utility-scale turbines.\nIt features a brushless DC motor encased in a waterproof and dust-resistant IP68 housing, with torque transmission via a dual-stage planetary gear system.\nIntegrated position sensors ensure pitch accuracy within -0.25- across a full rotation cycle. The unit is monitored in real-time through three redundant sensor channels\n(sensor_C, sensor_G, and sensor_F), providing both position and motor health diagnostics. The actuator interfaces with the Blade Control Module (BCM) and responds to command sequences issued by the turbine controller up to 50 times per second.\nOperational Anomalies & Pre-Failure Indicators\n- Audible clicking or grinding during pitch movement\n- SCADA fault messages tied to actuator position errors\n- Blade fails to reach full feather position under test\n- Sudden torque spikes or motor overheating alarms\n- Delays exceeding 1.5s in command-to-movement intervals\nDiagnostic Fault Codes and Resolution Paths\nPMA-042\nDescription: Torque deviation beyond threshold during pitch cycle. Likely gear resistance or partial jamming.\nResolution: Verify gear lubricant levels and inspect gearbox for metal shavings. Flush and re-grease using DT-Lube 80 if signs of abrasion are found.\nPMA-105\nDescription: Sensor G signal mismatch - position feedback inconsistency exceeds 0.5\nResolution: Run a triple-sensor calibration using the BladeControl SyncTool. Replace sensor_G if calibration fails or offset persists.\nPMA-231\nDescription: Overheating; internal coil temperature exceeded 95-C under low-load conditions.\nResolution: Check ventilation routes and ambient temperature. Review turbine operation logs for signs of excessive pitch cycling due to gusting winds.\nPMA-309\nDescription: Voltage drop detected during motor startup sequence. Possible capacitor drain or cable degradation.\nResolution: Inspect the high-voltage leads between power distribution unit and motor terminals.\nReplace any discolored or brittle insulation.\nPMA-511\nDescription: Unexpected directional reversal signal detected. Safety interlock engaged.\nResolution: Check firmware for logic reversal bug (firmware <1.09). Reflash controller and validate direction mapping via diagnostic movement test.\nPMA-777\nDescription: Sensor F heartbeat timeout. No telemetry received for 30+ seconds.\nResolution: Check sensor cabling. If intact, replace sensor_F module and rebind it in the\nBladeControl firmware configuration menu.\nRoutine Inspection & Lifecycle Guidance\nPerform function tests every 1,000 turbine hours. Mandatory full inspection after 7,500 hours or 2 years, whichever comes first.\nReplace unit after 15,000 hours of cumulative runtime, or if sensor variance persists beyond 0.5- over three cycles.\nStep-by-Step Certified Replacement Protocol\n1. From the SCADA interface, initiate rotor lock and enable maintenance override on all three blades.\n2. Confirm pitch position is fixed at 0- (feathered position). Verify lock with mechanical stoppers inside the hub.\n3. Disconnect all power feeds to the actuator cabinet. Wait 5 minutes before touching internal components.\n4. Access the rotor hub via the central service hatch. Use confined-space PPE and low-voltage certified gloves.\n5. Locate the DynoTorque PMA-540 motor unit within blade pitch assembly chamber. It is identifiable by the silver data plate and cable bundle from the base.\n6. Disconnect the motor power leads and label them clearly. Unplug sensor_C, sensor_G, and sensor_F lines from the hub terminal board.\n7. Use a digital torque wrench to loosen the 6 hex bolts securing the actuator casing (recommended torque removal: 160 Nm).\n8. Gently slide the actuator outward from the gear shaft. Use a winch or crane system if the unit weight exceeds 15 kg for safety compliance.\n9. Visually inspect the splined gear interface for wear, corrosion, or misalignment. Clean using certified turbine grease cleaner.\n10. Install the replacement PMA-540 by reversing the removal steps. Apply Loctite 243 to bolt threads and torque bolts to 180 Nm.\n11. Reconnect all sensor leads using anti-vibration clips and dielectric gel to prevent oxidation.\n12. Re-enable the control circuit and perform manual pitch test cycle via SCADA. Record RPM, torque, and directional response.\n13. Validate synchronization of all three sensors using BladeControl Diagnostic Suite. Ensure sensor variance < 0.2- over full rotation.\n14. Update asset tracking logs, firmware register list, and attach photographic documentation of installation and wiring."
        ],
        [
         24,
         "38917763",
         "28900g",
         "Yaw drive system",
         "Yaw drive system",
         "GyroTrak YD-8000 Yaw Drive System\nProfessional Maintenance Manual - Yaw Drive Subsystem\nComponent Type: Yaw drive system\nEAN: 38917763\nCompatible Turbine Model: StormRay T950 Arctic-Class Wind Turbine\nDimensions: 720mm - 488mm\nWeight: 28900g\nSensor Interfaces: sensor_J, sensor_M, sensor_B, sensor_L\nStock Location: Europe/Copenhagen\nComponent Overview\nThe GyroTrak YD-8000 is a high-torque planetary yaw drive system responsible for orienting the nacelle to face optimal wind direction.\nEngineered for arctic-class turbines, it includes a dual-motor configuration with mechanical load-sharing gears, an integrated self-locking brake system,\nand a closed-loop yaw control interface linked via four redundant sensors (sensor J, sensor M,\nsensor B, and sensor L).\nIt offers up to 280 kWm peak torque and maintains directional accuracy within ±.5- in turbulent wind.\nThe gearbox is filled with cold-climate synthetic grease and features internal heating coils for de-icing.\nDiagnostic Symptoms and Field Anomalies\n- Nacelle fails to track changing wind direction\n- Grinding or clunking sound from yaw mechanism\n- Brake engagement delays or brake slip under wind pressure\n- Thermal warning alarms during freezing temperatures\n- Spurious or uncontrolled nacelle rotations logged by SCADA\nCritical Error Codes and Resolution Instructions\nYAW-002\nDescription: Yaw alignment exceeds tolerance - nacelle misaligned by more than 4- for over 60 seconds.\nResolution: Check signal integrity of sensor_M and confirm yaw command signals. Inspect slewring for frozen sections or bearing resistance.\nYAW-103\nDescription: Yaw motor overcurrent event detected during rotation initiation.\nResolution: Test drive motors under no-load. Check for mechanical blockage in gear track. Review torque limiter calibration and motor brake release timing.\nYAW-228\nDescription: Yaw brake failure - brake did not engage within 3 seconds after rotation stop.\nResolution: Inspect hydraulic or electric brake actuator depending on configuration. Replace coil or re-pressurize brake line if fluid leak is found.\nYAW-319\nDescription: Sensor _L failure - invalid angle telemetry or frozen data point.\nResolution: Inspect cable shielding and waterproof seals. Replace sensor_L and reconfigure with SCADA calibration utility (firmware - 3.1).\nYAW-402\nDescription: Yaw system thermal warning - gearbox lubricant below -25°C.\nResolution: Check gearbox heater coil circuit and replace thermostat fuse if triggered. Preheat gearbox for 30 min before test rotation.\nYAW-501\nDescription: Unexpected yaw movement detected - nacelle rotated without command.\nResolution: Check for spurious signals on yaw control bus. Replace the yaw logic control board if internal watchdog faults persist.\nYAW-777\nDescription: Sensor B fails redundancy check - data mismatch across axis inputs.\nResolution: Recalibrate all yaw sensors using the GyroSync module. Replace sensor_B if variance remains >2.5- after recalibration.\nPreventative Inspection and Service Interval\nConduct yaw functionality tests every 1,500 turbine hours. Grease top slewing ring interface every 6 months.\nReplace full YD-8000 drive assembly every 18,000 hours or after 3 high-force fault shutdown events.\nStep-by-Step Certified Maintenance Protocol\n1. Activate turbine lockout and ensure nacelle is immobilized using yaw lock pins. Confirm via SCADA lock indicator and manual inspection.\n2. Power down yaw system via control cabinet isolation switch. Confirm capacitor drain using voltmeter at yaw motor terminals.\n3. Access yaw gearbox bay through nacelle floor panel using safety harness and descent-rated platform.\n4. Document current state: photo each sensor cable, motor terminal, and gearbox label. Scan barcode into turbine maintenance database.\n5. Disconnect sensor cables from sensor J, sensor M, sensor B, and sensor L. Clean contact surfaces with isopropyl and dry with compressed air.\n6. Loosen motor terminal blocks using insulated hex key. Ensure that all phase wires are labeled and not under tension.\n7. Using overhead crane, support yaw motor casing while loosening the 8 x M16 bolts securing motor to the gearbox interface.\n8. Slide motor outward. Use inspection lamp to check gear mesh for debris or cold welding signs.\nClean with lint-free cloth and cold-rated grease solvent.\n9. If replacing the full YD-8000 assembly, use the 4 anchor bolts on base plate to free the gearbox from the nacelle frame. Remove with lift winch.\n10. Install new unit in reverse order. Use thread locker (blue) on motor bolts and torque to 320 Nm.\nAlign yaw gear teeth using paint-marked reference tooth.\n11. Reconnect all sensor lines with vibration isolators and dielectric grease. Secure to cable trays to prevent abrasion over time.\n12. Power on yaw system, run a full SCADA yaw test. Validate brake function, torque curve profile, and positional accuracy <1.5- deviation.\n13. Record ambient temperature, yaw angle, brake engagement delay, and gearbox temperature after operation in maintenance log.\n14. Seal access panel, reset SCADA lockout, and verify yaw controller logs are clear of any residual error messages."
        ],
        [
         25,
         "48192934",
         "19700g",
         "Hydraulic pitch controller",
         "HPC-620",
         "HydroFlow HPC-620 Hydraulic Pitch Controller\nProfessional Maintenance Manual - Hydraulic Pitch Control System\nComponent Type: Hydraulic pitch controller\nEAN: 48192934\nCompatible Turbine Model: CycloneRidge V920 Seawind-Class Turbine\nDimensions: 505mm - 330mm\nWeight: 19700g\nSensor Interfaces: sensor_F, sensor_l, sensor_D\nStock Location: USA/Houston\nComponent Overview\nThe HydroFlow HPC-620 is a precision hydraulic controller responsible for blade pitch actuation in seawind-class turbines.\nIt governs hydraulic pressure flow to blade-mounted actuators, converting electronic signals into precise fluid control.\nIt features an onboard microcontroller, dual-redundant solenoid valves, and PID-controlled pressure loops monitored by\nsensor_F, sensor_I, and sensor_D. Designed to withstand salt spray and high-vibration environments, the HPC-620 is sealed\nto IP67 standards, with internal filters and a high-speed pressure relief module to prevent mechanical shock during gust conditions.\nOperational Fault Symptoms and Early Indicators\n- Inconsistent blade pitch angle changes\n- Hydraulic fluid leaks inside nacelle cabinet\n- Unusual valve clicking or pressure oscillation during blade positioning\n- Pitch fails to return to feathered position after shutdown\n- High pressure alarms or fluid temperature warnings in SCADA\nCommon Fault Codes and Corrective Actions\nHPC-010\nDescription: Pressure deviation exceeded 15 bar over command setpoint.\nResolution: Check hydraulic fluid level and quality. Replace filters if clogged. Recalibrate PID pressure controller using SCADA diagnostics.\nHPC-044\nDescription: Solenoid A fails to respond to control signal - no actuation detected.\nResolution: Measure coil resistance; replace solenoid if <8-. Confirm 24V signal from controller output. Inspect wiring and corrosion.\nHPC-113\nDescription: Sensor _1 drift > 3 bar over 60 seconds.\nResolution: Recalibrate sensor_1 from control panel. Replace if drift persists. Check for trapped air in hydraulic loop near sensor port.\nHPC-209\nDescription: Hydraulic return line blockage suspected - flow reading from sensor_D inconsistent.\nResolution: Inspect return line for kinks, frozen fluid, or collapsed hose. Flush system and bleed air after correction.\nHPC-310\nDescription: Controller internal temperature > 85°C for more than 3 minutes.\nResolution: Check cabinet ventilation and ambient temperature. Clean internal fan filters. Replace thermal paste on controller heat sink if dried.\nHPC-408\nDescription: Uncommanded pressure spike detected - potential stuck valve or delayed\ndecompression.\nResolution: Cycle the valve manifold using override mode. If condition persists, inspect valve springs and seals. Replace faulty valve block.\nHPC-603\nDescription: Sensor F communication loss > 45 seconds.\nResolution: Verify connector seating and cable integrity. Replace sensor if no signal on oscilloscope ping test. Rebind sensor in software.\nMaintenance Schedule and Service Intervals\nInspect controller every 2,000 hours for pressure stability, fluid temperature, and valve wear.\nReplace full unit after 16,000 hours or if more than three valve or pressure-related fault codes occur in a 90-day window.\nStep-by-Step Certified Service Procedure\n1. From the SCADA interface, disable turbine pitch control and activate hydraulic service mode to relieve system pressure.\n2. Verify pressure bleed-off by checking system gauge falls to 0 bar. Use manual bleed valve if residual pressure remains.\n3. Isolate the HPC-620 control unit via power isolation switch in the nacelle control bay. Confirm capacitor bleed before proceeding.\n4. Disconnect signal connectors for sensor_F, sensor_I, and sensor_D. Inspect for oil ingress or pin damage. Clean and dry connectors.\n5. Detach the four M10 bolts securing the controller to the mounting frame. Use sling support to avoid torque stress on piping.\n6. Slowly unscrew hydraulic supply and return fittings with absorbent pads ready to catch residual fluid. Cap open lines immediately.\n7. Inspect hydraulic manifold block for signs of leakage, cracking, or pressure plate wear.\nPhotograph for records before cleaning.\n8. Install new controller, aligning pipe threads carefully and using PTFE tape rated for hydraulic fluid (ISO 52). Torque bolts to 90 Nm.\n9. Reconnect sensor lines and power terminal. Secure cables with anti-vibration clamps and route through designated nacelle channels.\n10. Restore power and initiate SCADA-controlled pressurization sequence. Watch for leaks at all junctions for 5 minutes at full operating pressure.\n11. Run valve cycling test. Validate solenoid actuation, pressure control accuracy -2 bar, and response delay under 250 ms.\n12. Check and log firmware version, operating pressure, and controller cycle count from the diagnostic panel.\n13. Update turbine asset system, noting the controller serial, install date, sensor configuration, and attach visual inspection files.\n14. Replace fluid reservoir filters if not changed in last 2,000 hours. Top off hydraulic oil and validate fluid temperature < 60-C under load."
        ],
        [
         26,
         "60120291",
         "7400g",
         "Hydraulic pitch actuator",
         "HPA-220",
         "PitchMax HPA-220 Hydraulic Pitch Actuator\nProfessional Maintenance Manual - Blade Pitch Control System\nComponent Type: Hydraulic pitch actuator\nEAN: 60120291\nCompatible Turbine Model: StormWind SX2 Offshore-Class Turbine\nDimensions: 260mm × 230mm\nWeight: 7400g\nSensor Interfaces: sensor_P, sensor_H, sensor_L\nStock Location: Sweden/Göteborg\nComponent Overview\nThe PitchMax HPA-220 is a high-force hydraulic actuator responsible for adjusting the blade pitch angle in response to wind conditions,\nload demands, and braking protocols. It enables fine-grain aerodynamic control and supports rapid feathering during emergency shutdowns.\nEach actuator contains a dual-chamber piston system with redundant pressure paths and dynamic seal self-lubrication.\nIt integrates with sensors P (pitch angle), H (hydraulic pressure), and L (fluid level) for closed-loop control.\nWarning Signs and Performance Deviation\n- Delayed or jerky blade response to pitch commands\n- Recurrent pressure drop errors in SCADA\n- Feathering during normal operation without override\n- Visible hydraulic fluid leaks near hub actuator housing\nError Code Index and Diagnostic Actions\nHPA-002\nDescription: Sensor _P reports non-responsive pitch angle >3s during wind shift.\nResolution: Check for hydraulic fluid obstruction or actuator stiction. Verify linkage with blade pitch shaft.\nHPA-017\nDescription: Hydraulic pressure drop below 60 bar for >10s.\nResolution: Inspect for fluid leaks, worn seals, or damaged accumulator. Recharge hydraulic system and monitor pressure.\nHPA-033\nDescription: Sensor _L reports low fluid volume <25%.\nResolution: Top off reservoir with OEM-approved hydraulic fluid. Inspect for slow leaks or tank pressure loss.\nHPA-051\nDescription: Actuator cycle time exceeds 5s per 15° change.\nResolution: Bleed air from hydraulic lines. Inspect actuator cylinder for internal friction or contamination.\nHPA-066\nDescription: Hydraulic return temperature >85°C sustained.\nResolution: Check cooler flow and pump rate. Flush and replace fluid if discolored or foamy.\nHPA-089\nDescription: Inconsistent pitch angle across blades >2° spread.\nResolution: Compare sensor P readings across all actuators. Recalibrate and inspect mounting tolerance.\nHPA-104\nDescription: Unexpected feather command triggered mid-cycle.\nResolution: Inspect SCADA control logic and emergency override input. Log event and test fail-safe protocol.\nPreventive Maintenance Strategy\nInspect after 6,000 hours or following a hydraulic fault. Replace after 18,000 hours or if actuation exceeds cycle thresholds or leaks persist.\nRemoval and Reinstallation Protocol\n1. Shut down turbine and engage rotor lock. Verify pitch system is depressurized using SCADA interface.\n2. Isolate hydraulic lines leading to the actuator. Label each line according to flow direction and chamber port.\n3. Use spill containment below actuator to catch any residual fluid during disconnection.\n4. Unbolt actuator mounting brackets using hydraulic-rated wrenches. Support actuator with lift assist.\n5. Disconnect sensor wiring for P, H, and L. Ensure clean disconnection and label with tags.\n6. Inspect actuator shaft, seals, and housing for visible damage, wear rings, or contamination.\n7. Install replacement PitchMax HPA-220, ensuring alignment with blade pitch input shaft.\n8. Reconnect hydraulic lines with torque rating of 40 Nm. Check all O-rings and fitting conditions.\n9. Reconnect sensor wiring, shielded from EMI sources. Confirm tightness and waterproof seals.\n10. Refill hydraulic system and purge air using SCADA-driven bleed mode. Monitor reservoir level and pressure.\n11. Run pitch cycle test: adjust from 0° to 90° and back. Confirm consistent timing and response curve.\n12. Compare pitch angle telemetry across all three blades for synchronization accuracy.\n13. Record actuator serial, install timestamp, and all test data. Photograph actuator if visible through hub port.\n14. Re-enable turbine yaw control and release rotor lock. Log maintenance operation and clear all system alerts."
        ],
        [
         27,
         "30117620",
         "220g",
         "Rotational velocity optical sensor",
         "ORS-200",
         "SpinTrak ORS-200 Optical RPM Sensor\nProfessional Maintenance Manual - Rotational Sensing System\nComponent Type: Rotational velocity optical sensor\nEAN: 30117620\nCompatible Turbine Model: VoltAir V250 Onshore-Class Turbine\nDimensions: 90mm × 60mm\nWeight: 220g\nSensor Interfaces: sensor_Q, sensor_Y\nStock Location: Germany/Hamburg\nComponent Overview\nThe SpinTrak ORS-200 is a precision optical sensor used to monitor turbine shaft rotational speed (RPM) with micron-level accuracy.\nIt functions by counting light interruptions as a reflective marker passes across the sensor's optical window. It is designed for low-maintenance\ninstallations in high-vibration, dusty environments, with integrated self-cleaning optics and redundant\nphoto-diode arrays.\nThe sensor provides real-time RPM telemetry every 100 ms to the turbine controller and backup logic layer, aiding yaw response optimization and\noverspeed protection triggers. It connects via dual leads to sensors Q and Y for synchronization and feedback loop validation.\nSymptoms of Degradation or Fault\n- RPM spikes or dropouts in SCADA interface\n- Sudden overspeed or underspeed alerts\n- Out-of-sync readings between paired sensors\n- Optical signal strength or refresh interval instability\nSensor Error Codes and Troubleshooting Actions\nORS-005\nDescription: Sensor_Q optical window obscured - light signal strength below threshold.\nResolution: Clean sensor with isopropyl alcohol and verify alignment with marker. Replace lens if scratches are present.\nORS-021\nDescription: Inconsistent RPM readings >5% variance within 5 seconds.\nResolution: Check for shaft vibration or mechanical looseness. Reseat mounting bracket and recalibrate baseline RPM from SCADA.\nORS-034\nDescription: Sensor_Y not synchronized with Sensor_Q\nResolution: Inspect sync cable and pins. Reset sensor IDs in diagnostics. Replace Sensor_Y if drift exceeds 2 samples/sec.\nORS-058\nDescription: Oversedd alert - signal persists above 120% nominal RPM.\nResolution: Confirm turbine controller overspeed protection is active. Validate with tachometer. If real, initiate brake.\nORS-072\nDescription: Signal dropout >3s - no optical pulses received.\nResolution: Inspect wiring harness and shield continuity. Confirm marker is not dislodged. Replace sensor if pulses don't return.\nORS-088\nDescription: Redundant photodiode failure detected.\nResolution: Sensor is still operational in fallback mode. Replace sensor at next scheduled\nmaintenance window.\nORS-099\nDescription: Erratic telemetry refresh intervals from sensor Q.\nResolution: Check for SCADA communication lag or faulty buffer. Inspect sensor clock crystal and replace if drift persists.\nService Interval Guidelines\nInspect every 3,000 turbine hours or immediately after any overspeed event. Replace sensor every 12,000 hours or if photodiode faults occur.\nInstallation and Calibration Procedure\n1. Shut down turbine and confirm zero shaft rotation using main SCADA controller.\n2. Access sensor mounting zone near main shaft bearing housing with protective gloves and safety eyewear.\n3. Disable power to sensor bus from diagnostic panel. Confirm LEDs on ORS-200 are fully off.\n4. Disconnect signal cables from Sensor Q and Sensor Y. Label terminals for reinstallation.\n5. Remove mounting screws with precision driver and carefully detach the sensor from bracket.\n6. Inspect optical window under bright light for any dust, debris, or lens damage. Clean gently with ethanol swab.\n7. Install new ORS-200 sensor in same orientation. Tighten screws to 1.8 Nm torque. Align window with marker path.\n8. Reconnect Sensor Q and Sensor Y leads firmly. Ensure no pin misalignment or looseness.\n9. Enable power from diagnostics panel. Verify LED initialization sequence (green blink, then solid).\n10. Run RPM test sequence from SCADA: match reported RPM with manual tachometer reading within 1%\n11. Check telemetry refresh every 100 ms. Validate signal amplitude and sync between sensors Q\nand Y.\n12. Record part number, install date, and photo of mounting for logbook. Save calibration results.\n13. Monitor turbine acceleration and max RPM over 15-minute post-install cycle for stability\n14. File on-site report and re-enable turbine SCADA alarms. Tag old sensor for diagnostic return if failed."
        ],
        [
         28,
         "85334441",
         "7963g",
         "Brake",
         "StormHold Dynamic Brake XBR-71",
         "Maintenance Guide: StormHold Dynamic Brake XBR-71\nPart Type: Brake\nEAN: 85334441\nCompatible Turbine: Ventis R620 High-Speed Rotor Series\nLocation in Turbine: Rotor Hub - Central Brake Assembly\nLinked Sensors: sensor_A\nPart ID: SH-XBR71\nWeight: 7963g\nDimensions: 1885mm x 312mm\nStock Location: America/Detroit\nComponent Overview\nThe StormHold Dynamic Brake XBR-71 is a high-friction hydraulic brake caliper designed for extreme wind conditions.\nIt provides emergency stop capabilities and acts as the primary mechanical resistance during pitch failure scenarios.\nThis model integrates a single sensor feedback loop (sensor_A) and operates via closed-loop hydraulic feedback to SCADA.\nCommon Symptoms and Troubleshooting Triggers\n- Audible scraping sounds during turbine slowdown.\n- Reduced braking efficiency during high wind conditions.\n- Sudden SCADA alarm linked to pressure or temperature.\n- Brake caliper overheating warning within 2 minutes of activation.\n- Visual leakage or misting of hydraulic fluid near the rotor.\nError Codes & Corrective Actions\nBRK-700:\nDescription: Brake pad pressure imbalance detected. This may cause uneven rotor deceleration, especially during emergency stops.\nFix: Check the hydraulic actuator lines for leaks or obstructions. Calibrate the pressure regulator using the VentiDiag Toolkit v4.3.\nBRK-721:\nDescription: Thermal threshold exceeded on brake disc. Possible overuse or ventilation failure.\nFix: Inspect the disc for glazing or discoloration. Allow to cool and check thermal sensor calibration.\nIf warping is visible, replace the disc.\nBRK-788:\nDescription: Signal dropout from sensor_A linked to brake control. This affects real-time feedback to the SCADA system.\nFix: Test sensor A wiring continuity using a multimeter. Replace with a shielded cable if exposed to RF interference from inverter.\nMaintenance and Replacement Interval\nPerform brake pad inspection every 2,000 operating hours or during any rotor deceleration anomaly.\nFull replacement is advised every 8,000-10,000 hours depending on terrain and wind profile (desert, offshore, etc.).\nAlways replace if rotor imbalance is detected or caliper wear exceeds 2mm.\nStep-by-Step Maintenance Procedure\n1. Ensure the wind turbine is fully stopped and locked out. Apply mechanical locking pins to the rotor shaft and engage yaw locking system.\n2. Verify SCADA reports the brake system in safe mode. Redundant confirmation via local control panel is required.\n3. Open the nacelle hatch and access the brake chamber behind the main rotor\nhub. This chamber may require scaffolding or extension platform.\n4. Remove the outer nacelle casing (approx. 12 bolts). Use appropriate safety harness if working at height over 40m.\n5. Visually inspect the brake pads through the inspection port. Look for scoring, excessive wear, or hydraulic fluid misting.\n6. Disconnect the hydraulic feed line using two adjustable wrenches. Place an oil catch container to collect expelled fluid (can exceed 2L).\n7. Remove the sensor A feedback line from its socket and secure it safely out of the workspace.\n8. Unbolt the caliper mount using an M18 hex bit. The assembly is heavy-use a support arm or crane hook to hold the component during removal.\n9. Once detached, examine the piston actuator for rubber seal integrity and spring compression. Replace as needed.\n10. Install the new StormHold Dynamic Brake XBR-71 caliper. Align bolt holes and torque all mounting bolts to 250 Nm as per technical spec sheet.\n11. Reconnect the hydraulic line and refill the fluid reservoir with certified brake fluid ISO VG 46. Use air-bleed screws to remove bubbles.\n12. Reattach sensor_A to the feedback terminal and verify signal continuity via SCADA live test.\n13. Replace the nacelle casing and bolt tightly. Confirm torque settings are within range on all fasteners.\n14. Remove safety locks and re-enable turbine systems. Perform a manual test brake cycle and observe for vibration, noise, or lag.\n15. Update maintenance logs with serial number, install date, and technician ID.\nLog pre- and post-maintenance test data for audit."
        ],
        [
         29,
         "99827361",
         "1180g",
         "Grid synchronization relay",
         "GSR-850",
         "SyncLogic GSR-850 Grid Synchronization Relay\nProfessional Maintenance Manual - Grid Interface Subsystem\nComponent Type: Grid synchronization relay\nEAN: 99827361\nCompatible Turbine Model: SkyPulse V600 Onshore Series\nDimensions: 185mm - 122mm\nWeight: 1180g\nSensor Interfaces: sensor_A, sensor_M, sensor_G\nStock Location: Germany/Frankfurt\nComponent Overview\nThe SyncLogic GSR-850 is a high-precision relay module used for synchronizing turbine-generated power with the external grid.\nIt monitors phase, frequency, and voltage conditions via integrated digital signal processors and interfaces with primary turbine\ncontrollers through redundant sensor inputs (sensor_A, sensor_M, sensor_G).\nThe GSR-850 ensures seamless grid connection under IEC 61400-21 compliance, supports real-time blackout detection, and initiates\nisland-mode disengagement if anomalies occur. Equipped with EMI shielding and overvoltage protection, the module is certified for\nlightning-prone areas and includes built-in event logging via RS-485 telemetry.\nEarly Fault Indicators and Operational Symptoms\n- Delays in grid synchronization despite stable turbine output\n- Relay fails to engage during low-load startup\n- SCADA logs show phase mismatch or voltage instability\n- Abnormal relay click sequences or failure to trip\n- Event logs full or missing recent synchronization records\nDiagnostic Error Codes and Engineering Resolutions\nGSR-001\nDescription: Phase mismatch exceeds 10- on grid vs generator input.\nResolution: Adjust generator phase angle via SCADA PID tuning. If error persists, inspect sensor_M alignment and recalibrate zero crossing logic.\nGSR-019\nDescription: Voltage disparity exceeds 8% between grid and generator L1-L3.\nResolution: Check transformer tap settings. Review inverter output conditioning and test for neutral\nshift. Replace relay if internal filter cap is degraded.\nGSR-056\nDescription: Frequency drift detected during synchronization window.\nResolution: Review governor response lag. Increase governor gain if underdamped. Confirm GSR firmware - 2.1 to prevent known timing bug.\nGSR-111\nDescription: Sensor_G offline for >20s.\nResolution: Inspect telemetry cable. Replace sensor if signal continuity test fails. Re-bind device in relay firmware using maintenance port.\nGSR-205\nDescription: Relay trip time exceeds 75ms threshold.\nResolution: Measure coil response using diagnostic mode. Replace mechanical contactor if wear exceeds 15ms lag during pulse test.\nGSR-322\nDescription: Event log buffer full - no new grid events recorded.\nResolution: Download and clear event log via RS-485 console. Upgrade firmware to GSR-850v2.3+ for auto-purge support.\nGSR-808\nDescription: Unknown synchronization fault - system fallback triggered.\nResolution: Conduct full sensor diagnostics, firmware hash check, and reboot sequence. Replace GSP if fault recurs within 24 hours.\nInspection and Replacement Schedule\nInspect every 3,000 hours or quarterly, whichever comes first. Replace relay module every 12,000 hours or after three critical sync faults within 90 days.\nStep-by-Step Certified Replacement Procedure\n1. Disable turbine grid connection via SCADA. Confirm isolation using visual indicator at breaker panel.\n2. Power down relay circuit and lockout-tagout AC input breaker. Confirm no voltage at input terminals.\n3. Remove GSR-850 front panel using precision screwdriver. Disconnect RS-485 port and three sensor inputs.\n4. Label all connectors clearly for reassembly. Use camera to capture layout before disconnection.\n5. Loosen DIN rail latch and slide relay module outward. Handle using ESD precautions - avoid PCB contact.\n6. Inspect relay casing for soot, arc marks, or impact damage. Check for corrosion near input terminals.\n7. Install new GSR-850 by seating on DIN rail and engaging latch. Verify no pin bending or debris inside socket.\n8. Reconnect all input and telemetry cables. Tighten terminal screws to spec (1.2 Nm). Route cables through EMI shield path.\n9. Power up relay. Watch for boot sequence: LED blink pattern green-green-orange. If red LED persists, abort and recheck sensor alignment.\n10. Run synchronization simulation from SCADA. Validate phase match - 2-, voltage deviation - 3%, and relay response < 50 ms.\n11. Access relay RS-485 port and download system event log. Archive log file with timestamp and serial number.\n12. Check and record firmware version. Upgrade if < v2.3 using maintenance USB port and vendor tool.\n13. Update asset tracker with GSR serial number, install date, SCADA sync test results, and operator initials.\n14. Seal relay panel, reset breaker, and re-enable turbine grid connection. Observe first 10 mins of operation for sync stability."
        ],
        [
         30,
         "77630941",
         "620g",
         "Tri-axis vibration sensor",
         "VibeGuard TVS-950",
         "VibeGuard TVS-950 Tower Vibration Sensor\nProfessional Maintenance Manual - Tower Structural Monitoring\nComponent Type: Tri-axis vibration sensor\nEAN: 77630941\nCompatible Turbine Model: ZephyrTech ZT1000 Coastal-Class Turbine\nDimensions: 75mm x 75mm\nWeight: 620g\nSensor Interfaces: sensor_Z, sensor_X, sensor_Y\nStock Location: Portugal/Porto\nComponent Overview\nThe VibeGuard TVS-950 is a tri-axis MEMS-based vibration sensor installed mid-tower to monitor lateral, vertical, and torsional vibrations of the wind turbine structure.\nIt captures tower resonance profiles and harmonic peaks to detect foundation shifts, loose bolts, and structural fatigue.\nThis sensor outputs real-time telemetry to SCADA and anomaly detection systems, helping prevent long-term mechanical degradation.\nIt interfaces with sensors Z (vertical), X (lateral), and Y (torsional) using shielded cabling and provides both raw vibration signature and FFT summaries.\nVibration Anomalies and Failure Indicators\n- Abnormal SCADA vibration levels in idle or operating mode\n- FFT frequency anomalies not matching tower resonance\n- Signal loss or thermal fault warnings from sensor interface\n- Structural inspection triggers based on peak force readings\nError Codes and Troubleshooting Guidelines\nTVS-001\nDescription: Vertical vibration (Z) exceeds 0.6g RMS for >10s.\nResolution: Inspect tower base and anchor bolts. Check for nearby seismic or storm activity. Tighten structural joints.\nTVS-016\nDescription: Sensor _X signal flatlined - no motion detected.\nResolution: Verify sensor mounting bracket is tight. Check cable connection. Replace sensor if MEMS axis is non-responsive.\nTVS-029\nDescription: Harmonic peak detected at 12 Hz - outside known resonance profile.\nResolution: Inspect nacelle yaw system and tower top for oscillation source. Check blade balance and gearbox mounts.\nTVS-044\nDescription: Sensor _Y FFT checksum mismatch.\nResolution: Restart SCADA FFT module. Replace sensor if checksum mismatch persists after diagnostics reboot.\nTVS-057\nDescription: Temperature on sensor PCB >80°C sustained.\nResolution: Inspect cooling airflow or proximity to electrical heater source. Relocate sensor if ambient exceeds spec.\nTVS-072\nDescription: Cable impedance out of range - potential shielding failure.\nResolution: Replace signal cable from sensor to controller. Check for physical damage or improper grounding.\nTVS-093\nDescription: Sensor 7 reports peak >2.5g - suspected structural event.\nResolution: Trigger full structural inspection. Check SCADA for simultaneous brake/yaw events. Log impact and sensor history.\nRecommended Inspection Interval\nCheck sensor condition and FFT profile quarterly, especially after storms. Replace after 15,000 hours or if g-force anomalies are recorded.\nProfessional Replacement Procedure\n1. Disable sensor power supply from SCADA diagnostics panel.\n2. Access mid-tower service platform. Confirm lockout tagout safety procedure is active.\n3. Locate vibration sensor housing. Use PPE due to possible oil residue or confined space.\n4. Disconnect signal cabling for axes Z, X, and Y. Label connectors for reassembly.\n5. Remove mounting screws and detach sensor from baseplate. Inspect for corrosion or crack near mounts.\n6. Visually inspect sensor body for deformation. Gently clean with antistatic cloth.\n7. Install new VibeGuard TVS-950. Use Loctite threadlocker and torque to 1.5 Nm.\n8. Reconnect signal cables. Use EMI shielding sleeves if available. Confirm snug fit.\n9. Re-enable power and verify initialization LEDs blink correctly.\n10. Run vibration baseline scan from SCADA and verify RMS values in idle mode.\n11. Compare SCADA FFT data with historical signature for sensor X, Y, and Z axes.\n12. Document sensor serial, mount location, and install time. Upload to digital maintenance log.\n13. Check torque on all tower bolts in vicinity. Monitor vibration for 2-hour post-swap period.\n14. Log replacement in SCADA ticket and archive vibration plot image if available."
        ],
        [
         31,
         "55319844",
         "15300g",
         "lgbt power converter module",
         "PCM-4400",
         "VoltBridge PCM-4400 IGBT Power Converter\nProfessional Maintenance Manual - Inverter & Conversion Subsystem\nComponent Type: lgbt power converter module\nEAN: 55319844\nCompatible Turbine Model: TyphoonEdge E730 Offshore-Class Turbine\nDimensions: 620mm - 340mm\nWeight: 15300g\nSensor Interfaces: sensor_K, sensor_R, sensor_V\nStock Location: South Korea/Busan\nComponent Overview\nThe VoltBridge PCM-4400 is an insulated gate bipolar transistor (IGBT) power converter used to regulate DC-to-AC inversion in offshore wind turbines.\nIt enables dynamic reactive power compensation, grid code compliance, and precise frequency/voltage regulation for turbines operating under volatile wind conditions.\nEquipped with redundant cooling channels, laminated busbars, and onboard gate drivers, the PCM-4400 interfaces with sensors K, R, and V to monitor gate voltage stability,\nthermal load, and switching efficiency. The module supports soft-start sequencing and overcurrent self-protection routines. Designed for saltwater exposure and EMI resistance,\nits conformal-coated PCB and titanium-enforced heat sink deliver exceptional offshore performance.\nElectrical Fault Indicators and Thermal Signs\n- Abnormal voltage ripple under load\n- Unexpected turbine soft-start failure\n- SCADA warnings of gate faults or IGBT errors\n- Overtemperature alarms at cooling loop entry points\n- CAN bus reset events or driver timeouts in logs\nAdvanced Diagnostic Codes and Solutions\nPCM-009\nDescription: IGBT gate fault - gate voltage out of spec on channel 2.\nResolution: Inspect gate driver board for burn marks or cold joints. Replace gate driver module and re-run firing sequence verification.\nPCM-041\nDescription: DC-link voltage ripple exceeds 12% at 80% load.\nResolution: Check for degraded capacitors or faulty smoothing inductors. Replace any capacitor with ESR > 2-. Verify filter stage wiring.\nPCM-112\nDescription: Sensor R thermal overshoot - 90-C reached in under 2 minutes.\nResolution: Flush liquid cooling system and check pump RPM. Refill coolant if below line. Replace sensor. R if still unstable.\nPCM-220\nDescription: Soft-start abort - undervoltage condition during ramp-up.\nResolution: Test grid-side voltage supply for sag during startup. Recheck pre-charge resistor path and inverter pre-bias settings.\nPCM-309\nDescription: Sensor V drift > 1.5V under identical PWM conditions.\nResolution: Recalibrate sensor_V from diagnostic panel. If drift remains, inspect for loose solder joints or input-stage signal noise.\nPCM-413\nDescription: Overcurrent event - shutdown triggered to prevent latch-up.\nResolution: Download fault log, Inspect power path for shorts or burned traces, Replace module if\nIGBT integrity is compromised.\nPCM-650\nDescription: CAN communication fault - no data from gate driver controller.\nResolution: Inspect CAN cabling and shield continuity. Re-flash firmware if no recovery after reset.\nReplace gate driver board if CRC errors persist.\nService Interval and Inspection Requirements\nInspect every 2,000 hours for temperature drift, gate timing errors, and CAN stability. Replace PCM-4400 every 15,000 hours or after 2 soft-start aborts in a 90-day period.\nStep-by-Step Power Converter Maintenance Protocol\n1. Isolate PCM-4400 system via main power switch and verify absence of voltage across terminals using certified multimeter.\n2. Remove turbine converter panel rear cover with insulated tools. Activate ESD protection and secure grounding strap.\n3. Disconnect sensor K, sensor R, and sensor V from module interface. Label connections and verify plug integrity.\n4. Unbolt top and side mounting brackets with torque-rated socket wrench. Prepare hoist or lift for safe handling of 15kg+ unit.\n5. Gently pull PCM-4400 out of rack slot, checking for snagged wires or residual thermal compound.\nClean contact rails after removal.\n6. Inspect old module: check for capacitor bulge, PCB discoloration, corroded terminals, and foreign debris near IGBT blocks.\n7. Install new PCM-4400 in reverse order. Apply fresh thermal paste to heat sink if direct-contact model. Slide into position securely.\n8. Reconnect all sensors and ensure firm plug engagement. Torque terminal bolts to 2.8 Nm using calibrated wrench.\n9. Refill and purge converter cooling loop if drained. Monitor pump flow rate and absence of air bubbles in sight glass.\n10. Re-enable power, observe soft-start sequence from SCADA, and monitor voltage, current, and ripple readings.\n11. Run full gate driver test from diagnostic console. Validate gate signals are within -15V range and switching events are symmetric.\n12. Download post-install converter log. Check for residual faults. Archive logs and photograph serial plate for turbine records.\n13. Update digital maintenance tracker with install date, coolant batch ID, firmware version, and technician initials.\n14. Seal converter panel, reset lockout tag, and observe turbine behavior during next grid synchronization cycle."
        ],
        [
         32,
         "78904420",
         "5312g",
         "Capacitor board",
         "CBX-880",
         "VoltEdge CBX-880 Capacitor Board\nProfessional Maintenance Manual - Wind Turbine Power Component\nComponent Type: Capacitor board\nEAN: 78904420\nCompatible Turbine Model: NordFlux NF780 Continental Onshore Series\nDimensions: 302mm - 217mm\nWeight: 5312g\nSensor Inputs: sensor_B, sensor\nInventory Location: Europe/Frankfurt\nComponent Overview\nThe VoltEdge CBX-880 is a high-capacity, multi-phase capacitor board used in turbine power regulation systems.\nIt stabilizes voltage during load transitions, stores temporary charge to prevent power dips, and plays a vital role\nin protecting sensitive electronics from surges. This board is mounted in the power conversion cabinet and directly\ninterfaces with both the inverter and generator control systems. Heat-resistant up to 125-C, the CBX-860 includes an\ninternal fault detection circuit and over-voltage self-discharge protection.\nSymptoms and Warning Signs\n- SCADA alert: capacitor bank undervoltage\n- Audible buzzing or arcing from cabinet\n- Sudden turbine shutoff during gusts\n- Telemetry errors or sensor_B dropouts\n- IR scan shows hotspots around CBX-880 board\nKnown Error Codes and Corrective Measures\nCAP-001\nDescription: Voltage retention failure detected - capacitor fails to maintain charge for required duration.\nResolution: Test all capacitor banks using a capacitance meter. Replace any unit registering <90% of rated value. Discharge capacitors fully before handling.\nCAP-017\nDescription: Thermal overrun: onboard temperature exceeded 125°C under load.\nResolution: Check for blocked airflow in the power cabinet. Inspect cooling fan function. Apply thermal paste to heatsink if needed.\nCAP-102\nDescription: Voltage ripple exceeds 8% on output rail - likely capacitor degradation or circuit board trace failure.\nResolution: Use oscilloscope to measure ripple at test point TP4. Replace capacitor board if ripple exceeds tolerance despite clean power input.\nCAP-210\nDescription: Sensor E signal dropout - board health telemetry not received for 60+ seconds.\nResolution: Reconnect or replace the sensor_E telemetry cable. Confirm telemetry circuit is enabled via firmware (V2.4.5+).\nCAP-404\nDescription: Control loop instability - phase mismatch in capacitor discharge observed during turbine ramp-down.\nResolution: Synchronize capacitor timing using onboard DIP switch set. Calibrate discharge curve using PowerTune software module.\nMaintenance Schedule & Conditions\nRoutine inspection every 2,000 hours or quarterly (whichever comes first). Mandatory replacement after 16,000 hours or if failure rate exceeds 0.4 events per 100 hours.\nTest capacitor charge hold every 6 months using calibrated capacitance diagnostic tool.\nStep-by-Step Professional Maintenance Procedure\n1. Shut down turbine and engage full grid isolation using the turbine-s master cutoff switch.\n2. Wait minimum 10 minutes to allow capacitors to self-discharge. Use a multimeter to confirm voltage across terminals is 0V.\n3. Open the power conversion cabinet using torque key and locate the CBX-980 board - typically third from the bottom.\n4. Disconnect high-voltage power leads using insulated tools. Mark each wire with numbered tags to prevent reconnection errors.\n5. Detach the two signal cables (sensor_B and sensor_E). Avoid pulling by the cable body - use connector grips only.\n6. Unscrew the four M4 screws holding the board to its aluminum standoff brackets. Support the board with your free hand while loosening.\n7. Carefully slide the board out and inspect solder joints, capacitor integrity (look for bulging or leaks), and discoloration around diodes.\n8. Compare the replacement board-s part number, firmware version, and batch code. Ensure full compatibility (rev 2.1 or later required).\n9. Insert the new board into the same slot, ensuring full seating into the backplane connector. Listen for the click to confirm alignment.\n10. Secure the board with screws, reattach power and signal connectors, and double-check grounding continuity to chassis.\n11. Close the cabinet, re-enable turbine systems, and initiate diagnostic boot. Validate telemetry\nfeed from sensor E is active.\n12. Run the NordFlux Capacitor Sync Utility to verify ripple level, temperature, and charge retention under test conditions.\n13. Log all readings before and after replacement, including voltage ripple % and capacitor bank health scores. Attach photos of the board and wiring."
        ],
        [
         33,
         "67219804",
         "21550g",
         "Main inverter module",
         "IMX-3100",
         "VoltForge IMX-3100 Main Inverter Unit\nProfessional Maintenance Manual - Power Conversion System\nComponent Type: Main inverter module\nEAN: 67219804\nCompatible Turbine Model: TyphoonEdge GX1000 High-Capacity Offshore Turbine\nDimensions: 602mm - 345mm\nWeight: 21550g\nSensor Inputs: sensor_A, sensor_H, sensor_E, sensor_K\nInventory Location: Asia/Tokyo\nComponent Overview\nThe VoltForge IMX-3100 is a three-phase, grid-tied main inverter designed for offshore turbines with high dynamic load demands.\nIt converts raw AC input from the generator to grid-synchronized AC output with active harmonic suppression and real-time phase correction.\nThe IMX-3100 uses silicon-carbide IGBT transistors and features a triple-redundant signal integrity feedback system connected via sensor_A, sensor_H, sensor_E, and sensor_K.\nThe unit includes fault logging, thermal protection, and self-repair cycling in the event of micro-faults.\nDesigned for high-humidity and salt-air conditions,\nits internal control board is conformally coated, and the housing meets IP69K pressure wash standards.\nCommon Issues and Detection Patterns\n- Turbine fails to reconnect to grid after wind lull\n- Power fluctuation warnings or phase imbalance alerts\n- Audible clicking or high-frequency inverter whine\n- Sudden shutdown during ramp-up or ramp-down\n- Error messages in SCADA tied to sensor _H or sensor _K\nError Codes and Resolutions\nINV-001\nDescription: DC input voltage out of range. Source fluctuation or generator sync loss.\nResolution: Check generator RPM stability. If consistent, validate capacitor bank output. Replace pre-regulator if values fall below 420V under load.\nINV-056\nDescription: Phase A/B mismatch exceeds 8-. Potential sensor drift or timing offset.\nResolution: Recalibrate phase reference using InverterDiag PhaseSync Utility. If drift recurs, inspect sensor_H and rewire terminals.\nINV-104\nDescription: IGBT driver thermal warning - core temperature exceeds 110°C.\nResolution: Inspect coolant loop integrity, check thermal paste application. Replace fan or heat pipe module if temperature doesn't fall within 5 minutes.\nINV-211\nDescription: Sensor_A data timeout - communication failure lasting 30 seconds.\nResolution: Use loopback test to confirm cable function. Replace with shielded twisted pair if interference is suspected from nearby contactors.\nINV-318\nDescription: Harmonic distortion detected above 5% THD at output terminal L3\nResolution: Activate harmonic suppression mode. If error persists, test output waveform using oscilloscope and validate grounding path continuity.\nINV-404\nDescription: Internal EEPROM corruption detected. Fault log invalid or checksum failure.\nResolution: Backup config via serial console. Reflash firmware using VoltForge USB utility. Replace controller board if flashing fails twice.\nINV-509\nDescription. Overcurrent shutdown triggered on all three phases. Load spike or protection trip.\nResolution: Review turbine load logs. Isolate affected output legs, inspect contactors and IGBT modules. Replace if arc marks or fuse damage observed.\nPreventative Maintenance Schedule\nInspect visually every 2,000 hours for dust or corrosion. Run phase balance test monthly.\nFull replacement recommended after 20,000 hours of operation or at first sign of repeated fault cycling within 72 hours.\nReplacement Procedure - Certified Technician Protocol\n1. Initiate full turbine shutdown and engage E-Stop mode from SCADA. Confirm complete grid disconnection before accessing the cabinet.\n2. Wait 15 minutes to allow capacitor discharge. Confirm zero voltage using a CAT IV-rated multimeter across input/output terminals.\n3. Open the inverter bay cabinet (typically section C1) using a keyed access panel. Use an anti-static strap before touching internal components.\n4. Visually inspect the IMX-3100 unit for burn marks, bulging capacitors, or debris. Document any visual abnormalities.\n5. Disconnect the main AC input, output, and DC bus connectors. Use torque-controlled insulated tools. Tag all cables.\n6. Unplug sensor connectors for sensor A, sensor E, sensor H, and sensor K. Confirm pins are not oxidized or bent.\n7. Remove the unit from the rack using two personnel. Weight exceeds 21kg; use shoulder harness or crane winch for safety.\n8. Inspect rear mounting points and rails for corrosion or deformation. Clean using industrial-grade contact cleaner and corrosion inhibitor.\n9. Install the new IMX-3100 unit, ensuring complete backplane contact and torquing mounting bolts to 65 Nm.\n10. Reconnect all signal and power cables. Apply dielectric grease on sensor ports. Double-check\nAC phase alignment: L1, L2, L3.\n11. Enable power feeds and monitor for soft boot LED pattern (green-yellow-yellow). If fault LED is red, abort and recheck sensor K.\n12. Run the full inverter diagnostic from SCADA: verify output voltage waveform, THD < 3.5%, and balance across phases.\n13. Record and log firmware version, hardware revision, and system serial number into the maintenance management system.\n14. Perform final enclosure seal test using compressed air and verify IP69K compliance on housing gaskets."
        ],
        [
         34,
         "30981244",
         "390g",
         "Ultrasonic wind speed and direction sensor",
         "WindCore ANA-750",
         "WindCore ANA-750 Ultrasonic Anemometer\nProfessional Maintenance Manual - Wind Monitoring System\nComponent Type: Ultrasonic wind speed and direction sensor\nEAN: 30981244\nCompatible Turbine Model: GreenSpire GS250 Rural Turbine Series\nDimensions: 65mm x 105mm\nWeight: 390g\nSensor Interfaces: sensor_W, sensor_D\nStock Location: Germany/Bremen\nComponent Overview\nThe WindCore ANA-750 is a high-precision ultrasonic anemometer used to measure wind speed and direction at the nacelle.\nIt operates without moving parts, using time-of-flight measurements of ultrasonic pulses between transducers.\nThis sensor offers fast response in all weather conditions and is resilient to icing, dust, and mechanical wear.\nIt integrates with sensor_W (wind speed) and sensor_D (wind direction) channels to provide real-time data to yaw, pitch, and power regulation systems.\nCommon Faults and Operational Warnings\n- SCADA shows constant wind speed despite visible wind\n- Sudden gust detection with no wind pattern to match\n- Wind direction shows significant offset from nacelle orientation\n- Tilt or echo quality errors in diagnostics panel\nDiagnostic Codes and Recommended Actions\nANA-005\nDescription: Sensor W reports wind speed = 0 for >90 seconds in active wind environment.\nResolution: Inspect sensor for contamination or bird nesting. Clean transducers and check SCADA for signal dropouts.\nANA-011\nDescription: Direction offset >25° compared to turbine yaw alignment.\nResolution: Recalibrate sensor. D: Confirm sensor is level and properly aligned to turbine nose.\nANA-026\nDescription: Temperature compensation failure - signal drift at low temperatures.\nResolution: Run temperature compensation test. Replace if internal thermal correction fails diagnostic.\nANA-034\nDescription: Sensor communication timeout with SCADA bus.\nResolution: Verify cabling and connector integrity. Replace cable or power injector if voltage is unstable.\nANA-045\nDescription: Ultrasonic echo quality below threshold.\nResolution: Clean all transducer heads with alcohol wipe. If problem persists, replace sensor head module.\nANA-063\nDescription: Internal tilt detected >10° for over 5 minutes.\nResolution: Check sensor mounting plate and bolts. Realign or remount to eliminate tilt.\nANA-078\nDescription: Wind speed fluctuation >80% in 3 seconds (false gust detected).\nResolution: Check for reflective obstruction near sensor. Apply gust filtering in SCADA settings if terrain-dependent.\nService Interval and Calibration Policy\nClean transducers every 4 months. Recalibrate direction alignment annually or after SCADA drift alerts. Replace sensor every 4 years or on drift/echo failure.\nSafe Replacement and Alignment Workflow\n1. Deactivate turbine yaw movement. Confirm safe roof access and nacelle lockout.\n2. Climb to nacelle rooftop and secure PPE harness. Identify ANA-750 mount near weather station pole.\n3. Disconnect signal cable from sensor base. Shield connector with protective cap.\n4. Unfasten mounting screws (usually 4 mm Allen head) and gently lift sensor off mount.\n5. Inspect sensor body and transducer heads for contamination, insect ingress, or damage.\n6. Install new WindCore ANA-750, ensuring exact north-facing alignment for accurate direction data.\n7. Tighten mounting screws evenly to avoid tilt. Use digital level if needed.\n8. Reconnect signal cable. Check for corrosion or wear on contacts before securing.\n9. Return to SCADA interface and check for live wind speed and direction telemetry.\n10. Run self-test and verify echo quality, internal temperature, and tilt angle are nominal.\n11. Simulate wind changes (if possible) or wait for natural variance to validate response.\n12. Log replacement timestamp, sensor serial, and telemetry readings at install.\n13. Photograph mount position and attach to service record for traceability.\n14. Re-enable yaw control and monitor 1-hour wind tracking session for anomalies."
        ],
        [
         35,
         "41227053",
         "16665g",
         "controller card",
         "WindSync ControlMaster X-982",
         "Maintenance Guide: WindSync ControlMaster X-982\nPart Type: controller card #2\nEAN: 41227053\nCompatible Turbine: Stratus T900 Variable Pitch Turbine\nLocation in Turbine: Internal Nacelle Compartment - Slot 2B\nInvolved Sensors: sensor_B, sensor_E, sensor_D\nPart Code: WSX982-CTRL\nWeight: 16665g\nDimensions: 443mm x 227mm\nStock Location: America/Los Angeles\nTypical Symptoms & Diagnostic Indicators\n- Red LED blinking in a 2-blink pattern followed by pause.\n- Nacelle pitch control fails to respond in high winds.\n- Diagnostic interface shows intermittent connection to sensor_B and sensor_E.\n- Error logs contain CMX-prefixed faults.\n- Occasional SCADA 'heartbeat lost' messages for this controller slot.\nDetailed Error Code Descriptions\nCMX-101:\nDescription: The system has detected an unstable voltage input on the main board. This can cause\nfrequent rebooting or erratic turbine blade responses during gusts.\nHow to Fix: Check the main bus power connectors for corrosion. Replace fuse F4 on the board and\nverify voltage stability using a multimeter set to 20V DC.\nCMX-234:\nDescription: Internal watchdog timer failed to reset the firmware sequence. This can result in partial\ninitialization or non-responsive controller behavior.\nHow to Fix: Connect via USB Debug Port and reflash the firmware using the provided OEM toolkit. If error persists, consider replacing the EEPROM module.\nCMX-503:\nDescription: Communication timeout between the blade pitch actuator module and the main controller card. Wind pitch adjustments may lag or fail.\nHow to Fix: Inspect the actuator signal cable for wear. Reconnect or replace the cable. If issue remains, replace the card and recalibrate with ActuatorSync.\nReplacement & Maintenance Timing\nRecommended replacement after 12,000 operational hours or when showing symptoms of signal instability.\nMaintenance check every 1,500 hours to verify firmware stability and connection integrity.\nStep-by-Step Maintenance Instructions\n1. Ensure the turbine is fully powered down and isolated from the electrical grid. Confirm lock-out tag is applied at the master circuit breaker.\n2. Use a grounding strap to prevent electrostatic discharge before accessing internal components.\n3. Open the main nacelle housing and locate the controller tray marked CC2-TRAY. This is typically beneath the SCADA interface block.\n4. Unplug all sensor connectors (labelled S-A through S-F) from the controller. Mark each with color-coded tape for easy reconnection.\n5. Use a T25 screwdriver to unscrew all mounting screws along the perimeter of the controller card.\n6. Gently slide the card out of its PCI-style connector. Examine edge connectors for black marks or corrosion.\n7. Use a brush and isopropyl alcohol to clean the connector interface and fan blades within the tray.\n8. Insert the new WindSync ControlMaster X-982' card into the CC2-TRAY, aligning pins perfectly\nbefore applying any force.\n9. Secure with original mounting screws and reconnect all labeled sensor inputs.\n10. Re-seal the housing and remove any tools or debris from the nacelle workspace\n11. Power on the turbine and initiate diagnostic mode. Observe LED codes on the controller (expect solid green).\n12. Launch the WindTurbineConfig utility and run full diagnostic sync, verifying all sensor readings.\n13. Update the asset management system with the new controller's serial number."
        ],
        [
         36,
         "80452271",
         "880g",
         "Yaw position optical encoder",
         "YawSense ENC-310",
         "YawSense ENC-310 Rotary Position Encoder\nProfessional Maintenance Manual - Nacelle Orientation Sensing\nComponent Type: Yaw position optical encoder\nEAN: 80452271\nCompatible Turbine Model: HeliVolt H1200 Medium-Wind Turbine\nDimensions: 110mm × 75mm\nWeight: 880g\nSensor Interfaces: sensor_A, sensor_M\nStock Location: Denmark/Odense\nComponent Overview\nThe YawSense ENC-310 is a high-precision rotary position encoder used to track nacelle orientation relative to wind direction.\nThis optical encoder uses a coded disc and dual photodetectors to output angular position in real-time via quadrature signal and serial interface.\nIt is vital for yaw system feedback and storm misalignment correction. The ENC-310 features anti-vibration mounts, sealed bearings, and operates with\nSensor_A (angular position) and Sensor_M (motor feedback correlation) to maintain real-time alignment to wind vector.\nSymptoms Indicating Encoder Issues\n- Incorrect yaw angle displayed in SCADA\n- Jerky or reversed nacelle movement\n- Error codes related to Sensor_A or Sensor_M\n- Intermittent loss of yaw tracking during rotation\nError Code Table and Remedial Actions\nENC-003\nDescription: Sensor_A signal noise exceeds 15% jitter.\nResolution: Check encoder disc for smudge or damage. Clean or replace disc. Shield signal cable from Elm.\nENC-019\nDescription: Yaw offset >5° between Sensor A and Sensor M\nResolution: Recalibrate yaw zero point. Replace encoder if drift persists. Check mounting bracket stability.\nENC-025\nDescription: No signal from Sensor_M during yaw motion.\nResolution: Inspect cable routing and connector integrity. Confirm gear coupling is intact and rotating properly.\nENC-047\nDescription: Excessive vibration detected at encoder mount.\nResolution: Check anti-vibration gaskets. Torque mount bolts. Inspect for nacelle oscillation.\nENC-062\nDescription: Quadrature channel B signal missing.\nResolution: Replace encoder output cable or entire unit. Verify channel logic at SCADA diagnostics panel.\nENC-073\nDescription: Encoder temperature >70°C sustained for 10 min.\nResolution: Check for nearby heat source or airflow blockage. Replace encoder if thermal damage is observed.\nENC-091\nDescription: Unexpected direction reversals in yaw telemetry.\nResolution: Validate yaw control software version. Replace encoder if directional integrity test fails.\nInspection Cycle and Replacement Criteria\nInspect every 6,000 turbine hours or after abnormal yaw events. Replace encoder after 18,000 hours or if mechanical play is detected in shaft fit.\nReplacement and Calibration Instructions\n1. Disable yaw motor from SCADA. Confirm turbine is parked and rotor is locked.\n2. Access encoder mount point on nacelle rotation base. Wear fall protection and helmet.\n3. Disconnect Sensor A and Sensor M signal cables from the encoder. Tag cables clearly for later reconnection.\n4. Unscrew encoder cover and inspect for dust, oil, or corrosion. Clean gently with air and optical wipes.\n5. Remove encoder disc and inspect for scratches, delamination, or misalignment. Replace if worn.\n6. Unbolt encoder housing from mount bracket. Carefully detach without damaging shaft coupling.\n7. Install new encoder onto shaft with alignment key inserted. Torque bolts to 2.5 Nm and verify fit.\n8. Reattach signal cables. Check for intact shielding and tight terminals. Avoid crossing wires.\n9. Reconnect encoder to yaw controller. Reboot module and verify SCADA detects encoder with no error.\n10. Run yaw test: rotate nacelle ±15° and compare SCADA angle with mechanical markings.\n11. Confirm directionality, jitter level <5%, and yaw-motor sync with sensor_M telemetry.\n12. Record encoder serial, install timestamp, and temperature during test cycle.\n13. Log calibration values and verify yaw alignment is within tolerance. Store photos if needed.\n14. Enable yaw motor. Monitor 1-hour yaw tracking log to verify stability and correction behavior."
        ],
        [
         1,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         2,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         3,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         4,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         5,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         6,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         7,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         8,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         9,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         10,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         11,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         12,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         13,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         14,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         15,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         16,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         17,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ],
        [
         18,
         "Please provide the text you want me to extract the EAN from. I need the text to be able to find and return the EAN.\n",
         "Please provide the text you want me to extract the component weight from. I need the text to be able to answer your request.",
         "Please provide the text you want me to extract the component type from. I need the text to be able to answer your request.\n",
         "Please provide the text you want me to extract the component name from. I need the text to be able to answer your request.\n",
         ""
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "id",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "EAN",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "weight",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "component_type",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "component_name",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "full_guide",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 52
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "EAN",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "weight",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "component_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "component_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "full_guide",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM turbine_maintenance_guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "485d17d8-caf6-4c39-b76a-05aa901270e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2. Creating the Vector Search endpoint\n",
    "\n",
    "Let's create a new Vector search endpoint. You can also use the [UI under Compute](#/setting/clusters/vector-search) to directly create your endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c53199e-8cd3-4e52-b667-5e6954b92bbd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the Vector Search endpoint"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for endpoint to be ready, this can take a few min... {'name': 'e2eai_vs_endpoint', 'creator': 'kemalcan@berkeley.edu', 'creation_timestamp': 1760640677691, 'last_updated_timestamp': 1760640677691, 'endpoint_type': 'STANDARD', 'last_updated_user': 'kemalcan@berkeley.edu', 'id': 'a4f09294-ebd9-4b6b-9e2d-f1fae9c3a7be', 'endpoint_status': {'state': 'PROVISIONING'}, 'num_indexes': 0}\nWaiting for endpoint to be ready, this can take a few min... {'name': 'e2eai_vs_endpoint', 'creator': 'kemalcan@berkeley.edu', 'creation_timestamp': 1760640677691, 'last_updated_timestamp': 1760640677691, 'endpoint_type': 'STANDARD', 'last_updated_user': 'kemalcan@berkeley.edu', 'id': 'a4f09294-ebd9-4b6b-9e2d-f1fae9c3a7be', 'endpoint_status': {'state': 'PROVISIONING'}, 'num_indexes': 0}\nWaiting for endpoint to be ready, this can take a few min... {'name': 'e2eai_vs_endpoint', 'creator': 'kemalcan@berkeley.edu', 'creation_timestamp': 1760640677691, 'last_updated_timestamp': 1760640677691, 'endpoint_type': 'STANDARD', 'last_updated_user': 'kemalcan@berkeley.edu', 'id': 'a4f09294-ebd9-4b6b-9e2d-f1fae9c3a7be', 'endpoint_status': {'state': 'PROVISIONING'}, 'num_indexes': 0}\nWaiting for endpoint to be ready, this can take a few min... {'name': 'e2eai_vs_endpoint', 'creator': 'kemalcan@berkeley.edu', 'creation_timestamp': 1760640677691, 'last_updated_timestamp': 1760640677691, 'endpoint_type': 'STANDARD', 'last_updated_user': 'kemalcan@berkeley.edu', 'id': 'a4f09294-ebd9-4b6b-9e2d-f1fae9c3a7be', 'endpoint_status': {'state': 'PROVISIONING'}, 'num_indexes': 0}\nEndpoint named e2eai_vs_endpoint is ready.\n"
     ]
    }
   ],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "vsc = VectorSearchClient(disable_notice=True) #Whether to disable authentication notice messages. Default is False.\n",
    "\n",
    "if not endpoint_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME):\n",
    "    vsc.create_endpoint(name=VECTOR_SEARCH_ENDPOINT_NAME, \n",
    "                        endpoint_type=\"STANDARD\")\n",
    "\n",
    "wait_for_vs_endpoint_to_be_ready(vsc, \n",
    "                                 VECTOR_SEARCH_ENDPOINT_NAME)\n",
    "\n",
    "\n",
    "print(f\"Endpoint named {VECTOR_SEARCH_ENDPOINT_NAME} is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "f2cb98b0-7524-4527-8ea4-c1ae678ffda7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 2.3 Creating the Vector Search Index\n",
    "\n",
    "<img src=\"https://github.com/databricks-demos/dbdemos-resources/blob/main/images/index_creation.gif?raw=true\" width=\"600px\" style=\"float: right; margin-left: 10px\">\n",
    "\n",
    "You can view your endpoint on the [Vector Search Endpoints UI](#/setting/clusters/vector-search). Click on the endpoint name to see all indexes that are served by the endpoint.\n",
    "\n",
    "All we now have to do is to as Databricks to create the index on top of our table. The Delta Table will automatically be synched with the index.\n",
    "\n",
    "\n",
    "Again, you can do that using your Unity Catalog UI, and selecting the turbine_maintenance_guide table in your Unity Catalog, and click on add a vector search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75fa4683-3aec-42ed-ae97-83d56bbd5790",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating the VS from the maintenance table"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating index main.e2eai_iot_turbine.turbine_maintenance_guide_vs_index on endpoint e2eai_vs_endpoint...\n"
     ]
    }
   ],
   "source": [
    "import databricks.sdk.service.catalog as c\n",
    "\n",
    "# Where we want to store our index\n",
    "vs_index_fullname = f\"{catalog}.{db}.turbine_maintenance_guide_vs_index\"\n",
    "\n",
    "if not index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname):\n",
    "  print(f\"Creating index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  \n",
    "  index = vsc.create_delta_sync_index(\n",
    "    endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    source_table_name=f\"{catalog}.{db}.turbine_maintenance_guide\",\n",
    "    index_name=vs_index_fullname,\n",
    "    pipeline_type=\"TRIGGERED\", # Must be CONTINUOUS or TRIGGERED.\n",
    "    primary_key='id',\n",
    "    embedding_source_column=\"full_guide\",\n",
    "    #embedding_vector_column = you don't need this because we are using a model. You'll need this if you are using precomputed embeddings.\n",
    "    embedding_model_endpoint_name=\"databricks-gte-large-en\"\n",
    "  )\n",
    "else:\n",
    "  print(f\"Grabbing existing index {vs_index_fullname} on endpoint {VECTOR_SEARCH_ENDPOINT_NAME}...\")\n",
    "  index = vsc.get_index(VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "\n",
    "\n",
    "  # You don’t need to specify embedding_dimension in this mode; it’s derived from the model. For managed endpoints like GTE, that’s 1024; for a custom model, it’s whatever your model outputs. \n",
    "\n",
    "  # If you use precomputed embeddings (embedding_vector_column)\n",
    "  # You must explicitly set embedding_dimension=... to match the length of your stored vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d95969b7-5740-481c-93e0-fb3bbbf481b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>index_name</th><th>exists</th></tr></thead><tbody><tr><td>main.e2eai_iot_turbine.turbine_maintenance_guide_vs_index</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "main.e2eai_iot_turbine.turbine_maintenance_guide_vs_index",
         true
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "index_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "exists",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "exists = index_exists(vsc, VECTOR_SEARCH_ENDPOINT_NAME, vs_index_fullname)\n",
    "display(spark.createDataFrame([(vs_index_fullname, exists)], [\"index_name\", \"exists\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "isMarkdownSandbox": true,
     "nuid": "01310d1f-28fb-4aeb-8f84-b11ccb15eab9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.4 Create our tool\n",
    "Below, we utilize the _VECTOR\\_SEARCH_ SQL function from Databricks to easily set up our maintenance reports retriever function. Our agent will utilize this function in the subsequent steps!\n",
    "\n",
    "The [vector_search()](https://docs.databricks.com/aws/en/sql/language-manual/functions/vector_search) function is a SQL AI function that queries a Mosaic AI Vector Search index directly from SQL. It performs similarity search (and optionally hybrid keyword+vector search) against an index and returns the top matching rows with selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95418fdd-74b7-43e7-95ed-540d1818ff40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP FUNCTION IF EXISTS turbine_maintenance_guide_retriever\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE FUNCTION \n",
    "turbine_maintenance_guide_retriever(question STRING COMMENT 'Question to be answered from the turbine maintenance guides.')\n",
    "RETURNS ARRAY<STRING>\n",
    "LANGUAGE SQL\n",
    "COMMENT 'This tool searches / retrieves the wind turbine maintenance guide for a given question'\n",
    "RETURN (\n",
    "  SELECT collect_list(full_guide) \n",
    "  FROM VECTOR_SEARCH(index => '{catalog}.{schema}.turbine_maintenance_guide_vs_index', query => question, num_results => 1) ) \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6e4463-6510-4768-bc33-33881c12c171",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "-- understanding collect_list() function\n",
    "--SELECT collect_list(col) FROM VALUES (1), (2), (NULL), (1) AS tab(col);\n",
    " \n",
    "-- returns [1,2,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdbce0ab-1b1b-4026-8a44-a959be479ab6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>reports</th></tr></thead><tbody><tr><td>List(VibeGuard TVS-950 Tower Vibration Sensor\n",
       "Professional Maintenance Manual - Tower Structural Monitoring\n",
       "Component Type: Tri-axis vibration sensor\n",
       "EAN: 77630941\n",
       "Compatible Turbine Model: ZephyrTech ZT1000 Coastal-Class Turbine\n",
       "Dimensions: 75mm x 75mm\n",
       "Weight: 620g\n",
       "Sensor Interfaces: sensor_Z, sensor_X, sensor_Y\n",
       "Stock Location: Portugal/Porto\n",
       "Component Overview\n",
       "The VibeGuard TVS-950 is a tri-axis MEMS-based vibration sensor installed mid-tower to monitor lateral, vertical, and torsional vibrations of the wind turbine structure.\n",
       "It captures tower resonance profiles and harmonic peaks to detect foundation shifts, loose bolts, and structural fatigue.\n",
       "This sensor outputs real-time telemetry to SCADA and anomaly detection systems, helping prevent long-term mechanical degradation.\n",
       "It interfaces with sensors Z (vertical), X (lateral), and Y (torsional) using shielded cabling and provides both raw vibration signature and FFT summaries.\n",
       "Vibration Anomalies and Failure Indicators\n",
       "- Abnormal SCADA vibration levels in idle or operating mode\n",
       "- FFT frequency anomalies not matching tower resonance\n",
       "- Signal loss or thermal fault warnings from sensor interface\n",
       "- Structural inspection triggers based on peak force readings\n",
       "Error Codes and Troubleshooting Guidelines\n",
       "TVS-001\n",
       "Description: Vertical vibration (Z) exceeds 0.6g RMS for >10s.\n",
       "Resolution: Inspect tower base and anchor bolts. Check for nearby seismic or storm activity. Tighten structural joints.\n",
       "TVS-016\n",
       "Description: Sensor _X signal flatlined - no motion detected.\n",
       "Resolution: Verify sensor mounting bracket is tight. Check cable connection. Replace sensor if MEMS axis is non-responsive.\n",
       "TVS-029\n",
       "Description: Harmonic peak detected at 12 Hz - outside known resonance profile.\n",
       "Resolution: Inspect nacelle yaw system and tower top for oscillation source. Check blade balance and gearbox mounts.\n",
       "TVS-044\n",
       "Description: Sensor _Y FFT checksum mismatch.\n",
       "Resolution: Restart SCADA FFT module. Replace sensor if checksum mismatch persists after diagnostics reboot.\n",
       "TVS-057\n",
       "Description: Temperature on sensor PCB >80°C sustained.\n",
       "Resolution: Inspect cooling airflow or proximity to electrical heater source. Relocate sensor if ambient exceeds spec.\n",
       "TVS-072\n",
       "Description: Cable impedance out of range - potential shielding failure.\n",
       "Resolution: Replace signal cable from sensor to controller. Check for physical damage or improper grounding.\n",
       "TVS-093\n",
       "Description: Sensor 7 reports peak >2.5g - suspected structural event.\n",
       "Resolution: Trigger full structural inspection. Check SCADA for simultaneous brake/yaw events. Log impact and sensor history.\n",
       "Recommended Inspection Interval\n",
       "Check sensor condition and FFT profile quarterly, especially after storms. Replace after 15,000 hours or if g-force anomalies are recorded.\n",
       "Professional Replacement Procedure\n",
       "1. Disable sensor power supply from SCADA diagnostics panel.\n",
       "2. Access mid-tower service platform. Confirm lockout tagout safety procedure is active.\n",
       "3. Locate vibration sensor housing. Use PPE due to possible oil residue or confined space.\n",
       "4. Disconnect signal cabling for axes Z, X, and Y. Label connectors for reassembly.\n",
       "5. Remove mounting screws and detach sensor from baseplate. Inspect for corrosion or crack near mounts.\n",
       "6. Visually inspect sensor body for deformation. Gently clean with antistatic cloth.\n",
       "7. Install new VibeGuard TVS-950. Use Loctite threadlocker and torque to 1.5 Nm.\n",
       "8. Reconnect signal cables. Use EMI shielding sleeves if available. Confirm snug fit.\n",
       "9. Re-enable power and verify initialization LEDs blink correctly.\n",
       "10. Run vibration baseline scan from SCADA and verify RMS values in idle mode.\n",
       "11. Compare SCADA FFT data with historical signature for sensor X, Y, and Z axes.\n",
       "12. Document sensor serial, mount location, and install time. Upload to digital maintenance log.\n",
       "13. Check torque on all tower bolts in vicinity. Monitor vibration for 2-hour post-swap period.\n",
       "14. Log replacement in SCADA ticket and archive vibration plot image if available.)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         [
          "VibeGuard TVS-950 Tower Vibration Sensor\nProfessional Maintenance Manual - Tower Structural Monitoring\nComponent Type: Tri-axis vibration sensor\nEAN: 77630941\nCompatible Turbine Model: ZephyrTech ZT1000 Coastal-Class Turbine\nDimensions: 75mm x 75mm\nWeight: 620g\nSensor Interfaces: sensor_Z, sensor_X, sensor_Y\nStock Location: Portugal/Porto\nComponent Overview\nThe VibeGuard TVS-950 is a tri-axis MEMS-based vibration sensor installed mid-tower to monitor lateral, vertical, and torsional vibrations of the wind turbine structure.\nIt captures tower resonance profiles and harmonic peaks to detect foundation shifts, loose bolts, and structural fatigue.\nThis sensor outputs real-time telemetry to SCADA and anomaly detection systems, helping prevent long-term mechanical degradation.\nIt interfaces with sensors Z (vertical), X (lateral), and Y (torsional) using shielded cabling and provides both raw vibration signature and FFT summaries.\nVibration Anomalies and Failure Indicators\n- Abnormal SCADA vibration levels in idle or operating mode\n- FFT frequency anomalies not matching tower resonance\n- Signal loss or thermal fault warnings from sensor interface\n- Structural inspection triggers based on peak force readings\nError Codes and Troubleshooting Guidelines\nTVS-001\nDescription: Vertical vibration (Z) exceeds 0.6g RMS for >10s.\nResolution: Inspect tower base and anchor bolts. Check for nearby seismic or storm activity. Tighten structural joints.\nTVS-016\nDescription: Sensor _X signal flatlined - no motion detected.\nResolution: Verify sensor mounting bracket is tight. Check cable connection. Replace sensor if MEMS axis is non-responsive.\nTVS-029\nDescription: Harmonic peak detected at 12 Hz - outside known resonance profile.\nResolution: Inspect nacelle yaw system and tower top for oscillation source. Check blade balance and gearbox mounts.\nTVS-044\nDescription: Sensor _Y FFT checksum mismatch.\nResolution: Restart SCADA FFT module. Replace sensor if checksum mismatch persists after diagnostics reboot.\nTVS-057\nDescription: Temperature on sensor PCB >80°C sustained.\nResolution: Inspect cooling airflow or proximity to electrical heater source. Relocate sensor if ambient exceeds spec.\nTVS-072\nDescription: Cable impedance out of range - potential shielding failure.\nResolution: Replace signal cable from sensor to controller. Check for physical damage or improper grounding.\nTVS-093\nDescription: Sensor 7 reports peak >2.5g - suspected structural event.\nResolution: Trigger full structural inspection. Check SCADA for simultaneous brake/yaw events. Log impact and sensor history.\nRecommended Inspection Interval\nCheck sensor condition and FFT profile quarterly, especially after storms. Replace after 15,000 hours or if g-force anomalies are recorded.\nProfessional Replacement Procedure\n1. Disable sensor power supply from SCADA diagnostics panel.\n2. Access mid-tower service platform. Confirm lockout tagout safety procedure is active.\n3. Locate vibration sensor housing. Use PPE due to possible oil residue or confined space.\n4. Disconnect signal cabling for axes Z, X, and Y. Label connectors for reassembly.\n5. Remove mounting screws and detach sensor from baseplate. Inspect for corrosion or crack near mounts.\n6. Visually inspect sensor body for deformation. Gently clean with antistatic cloth.\n7. Install new VibeGuard TVS-950. Use Loctite threadlocker and torque to 1.5 Nm.\n8. Reconnect signal cables. Use EMI shielding sleeves if available. Confirm snug fit.\n9. Re-enable power and verify initialization LEDs blink correctly.\n10. Run vibration baseline scan from SCADA and verify RMS values in idle mode.\n11. Compare SCADA FFT data with historical signature for sensor X, Y, and Z axes.\n12. Document sensor serial, mount location, and install time. Upload to digital maintenance log.\n13. Check torque on all tower bolts in vicinity. Monitor vibration for 2-hour post-swap period.\n14. Log replacement in SCADA ticket and archive vibration plot image if available."
         ]
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "reports",
            "nullable": true,
            "type": {
             "containsNull": true,
             "elementType": "string",
             "type": "array"
            }
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 20
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "reports",
         "type": "{\"containsNull\":true,\"elementType\":\"string\",\"type\":\"array\"}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql \n",
    "-- Let's test the tool we created\n",
    "SELECT turbine_maintenance_guide_retriever('The VibeGuard TVS-950 is giving me an error code TVS-001.') AS reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a6c9473-c398-4a3c-89ee-b109c1823267",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Exploring Mosaic AI Tools in Unity Catalog\n",
    "\n",
    "Our tools are ready! \n",
    "\n",
    "You can now view the UC function tools in Catalog Explorer. Click **Catalog** in the sidebar. In the Catalog Explorer, navigate to your catalog and schema. \n",
    "\n",
    "The UC function tools appears under **Functions**. \n",
    "\n",
    "<img src=\"https://github.com/Datastohne/demo/blob/main/Screenshot%202024-09-18%20at%2016.24.24.png?raw=true\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e98df57-c2ef-4aa5-acf1-4f637f760c70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## What’s next: test your Agents with Databricks Playground\n",
    "\n",
    "Now that we have our AI Tools ready and registered in Unity Catalog, we can compose them into an agent system that generates maintenance work orders using the Mosaic AI agent framework.\n",
    "\n",
    "Open the [05.2-agent-creation-guide]($./05.2-agent-creation-guide) notebook to create and deploy the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33ed3bbb-788c-41b4-86ab-f8d5803010db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>turbine_id</th><th>hourly_timestamp</th><th>avg_energy</th><th>std_sensor_A</th><th>std_sensor_B</th><th>std_sensor_C</th><th>std_sensor_D</th><th>std_sensor_E</th><th>std_sensor_F</th><th>location</th><th>model</th><th>state</th><th>abnormal_sensor</th></tr></thead><tbody><tr><td>004a641f-e9e5-9fff-d421-1bf88319420b</td><td>2024-01-16T17:00:00.000Z</td><td>0.18897920400916973</td><td>0.9644652043128558</td><td>2.6558386572409103</td><td>3.4528106013576214</td><td>2.485158752607405</td><td>2.2884032468369284</td><td>4.702138990110717</td><td>Tupelo</td><td>EpicWind</td><td>America/Chicago</td><td>sensor_F</td></tr><tr><td>004a641f-e9e5-9fff-d421-1bf88319420b</td><td>2024-01-16T18:00:00.000Z</td><td>0.19212257629921775</td><td>1.0681855556261903</td><td>2.3848184303882847</td><td>3.303412042721332</td><td>2.172251292324001</td><td>2.342593019596896</td><td>4.870875418724548</td><td>Tupelo</td><td>EpicWind</td><td>America/Chicago</td><td>sensor_F</td></tr><tr><td>004a641f-e9e5-9fff-d421-1bf88319420b</td><td>2024-01-16T19:00:00.000Z</td><td>0.1735634457450677</td><td>1.1420887720146298</td><td>2.062708699095104</td><td>3.019329663712003</td><td>2.339552044868049</td><td>2.7306978700770164</td><td>4.237196637787606</td><td>Tupelo</td><td>EpicWind</td><td>America/Chicago</td><td>sensor_F</td></tr><tr><td>004a641f-e9e5-9fff-d421-1bf88319420b</td><td>2024-01-16T20:00:00.000Z</td><td>0.10343409262714734</td><td>1.0498727154061804</td><td>2.2192165091594975</td><td>3.246726138931612</td><td>2.3204665834317817</td><td>2.662700177613455</td><td>4.289404582190178</td><td>Tupelo</td><td>EpicWind</td><td>America/Chicago</td><td>sensor_F</td></tr><tr><td>004a641f-e9e5-9fff-d421-1bf88319420b</td><td>2024-01-16T21:00:00.000Z</td><td>0.15481243527493338</td><td>1.0325552090494656</td><td>2.142101655549623</td><td>2.7298423212662217</td><td>2.3597486817214515</td><td>2.761466398058171</td><td>4.588788770497015</td><td>Tupelo</td><td>EpicWind</td><td>America/Chicago</td><td>sensor_F</td></tr><tr><td>004a641f-e9e5-9fff-d421-1bf88319420b</td><td>2024-01-16T22:00:00.000Z</td><td>0.0847723255024208</td><td>1.0021697211227565</td><td>2.0968943765292085</td><td>2.9215472587753415</td><td>2.477840322666964</td><td>2.9466029618007314</td><td>4.357159925464822</td><td>Tupelo</td><td>EpicWind</td><td>America/Chicago</td><td>sensor_F</td></tr><tr><td>004a641f-e9e5-9fff-d421-1bf88319420b</td><td>2024-01-16T23:00:00.000Z</td><td>0.074818609038791</td><td>1.058048335093487</td><td>2.4852932716249665</td><td>2.8927160852893152</td><td>2.1567050955955853</td><td>2.2120358529793696</td><td>5.614526027139428</td><td>Tupelo</td><td>EpicWind</td><td>America/Chicago</td><td>sensor_F</td></tr><tr><td>00f27248-1f4f-e174-432c-53bd2a9158df</td><td>2024-01-16T17:00:00.000Z</td><td>0.12839653721057284</td><td>1.0656088831997519</td><td>1.9263319253102174</td><td>3.3330563526547747</td><td>2.2300401961414615</td><td>2.354626086386649</td><td>1.8913049031607982</td><td>Crystal Lake</td><td>EpicWind</td><td>America/Chicago</td><td>ok</td></tr><tr><td>00f27248-1f4f-e174-432c-53bd2a9158df</td><td>2024-01-16T18:00:00.000Z</td><td>0.8542245491303897</td><td>1.080309777815946</td><td>1.9618452098136365</td><td>2.9717426105145472</td><td>2.306627597988137</td><td>2.5166973688595817</td><td>1.980452948870913</td><td>Crystal Lake</td><td>EpicWind</td><td>America/Chicago</td><td>ok</td></tr><tr><td>00f27248-1f4f-e174-432c-53bd2a9158df</td><td>2024-01-16T19:00:00.000Z</td><td>0.4915535666395597</td><td>1.0646332592567707</td><td>2.2186746553400307</td><td>3.3459438407963433</td><td>2.2847856939507167</td><td>2.5560343320959498</td><td>1.9519204325253467</td><td>Crystal Lake</td><td>EpicWind</td><td>America/Chicago</td><td>ok</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "004a641f-e9e5-9fff-d421-1bf88319420b",
         "2024-01-16T17:00:00.000Z",
         0.18897920400916973,
         0.9644652043128558,
         2.6558386572409103,
         3.4528106013576214,
         2.485158752607405,
         2.2884032468369284,
         4.702138990110717,
         "Tupelo",
         "EpicWind",
         "America/Chicago",
         "sensor_F"
        ],
        [
         "004a641f-e9e5-9fff-d421-1bf88319420b",
         "2024-01-16T18:00:00.000Z",
         0.19212257629921775,
         1.0681855556261903,
         2.3848184303882847,
         3.303412042721332,
         2.172251292324001,
         2.342593019596896,
         4.870875418724548,
         "Tupelo",
         "EpicWind",
         "America/Chicago",
         "sensor_F"
        ],
        [
         "004a641f-e9e5-9fff-d421-1bf88319420b",
         "2024-01-16T19:00:00.000Z",
         0.1735634457450677,
         1.1420887720146298,
         2.062708699095104,
         3.019329663712003,
         2.339552044868049,
         2.7306978700770164,
         4.237196637787606,
         "Tupelo",
         "EpicWind",
         "America/Chicago",
         "sensor_F"
        ],
        [
         "004a641f-e9e5-9fff-d421-1bf88319420b",
         "2024-01-16T20:00:00.000Z",
         0.10343409262714734,
         1.0498727154061804,
         2.2192165091594975,
         3.246726138931612,
         2.3204665834317817,
         2.662700177613455,
         4.289404582190178,
         "Tupelo",
         "EpicWind",
         "America/Chicago",
         "sensor_F"
        ],
        [
         "004a641f-e9e5-9fff-d421-1bf88319420b",
         "2024-01-16T21:00:00.000Z",
         0.15481243527493338,
         1.0325552090494656,
         2.142101655549623,
         2.7298423212662217,
         2.3597486817214515,
         2.761466398058171,
         4.588788770497015,
         "Tupelo",
         "EpicWind",
         "America/Chicago",
         "sensor_F"
        ],
        [
         "004a641f-e9e5-9fff-d421-1bf88319420b",
         "2024-01-16T22:00:00.000Z",
         0.0847723255024208,
         1.0021697211227565,
         2.0968943765292085,
         2.9215472587753415,
         2.477840322666964,
         2.9466029618007314,
         4.357159925464822,
         "Tupelo",
         "EpicWind",
         "America/Chicago",
         "sensor_F"
        ],
        [
         "004a641f-e9e5-9fff-d421-1bf88319420b",
         "2024-01-16T23:00:00.000Z",
         0.074818609038791,
         1.058048335093487,
         2.4852932716249665,
         2.8927160852893152,
         2.1567050955955853,
         2.2120358529793696,
         5.614526027139428,
         "Tupelo",
         "EpicWind",
         "America/Chicago",
         "sensor_F"
        ],
        [
         "00f27248-1f4f-e174-432c-53bd2a9158df",
         "2024-01-16T17:00:00.000Z",
         0.12839653721057284,
         1.0656088831997519,
         1.9263319253102174,
         3.3330563526547747,
         2.2300401961414615,
         2.354626086386649,
         1.8913049031607982,
         "Crystal Lake",
         "EpicWind",
         "America/Chicago",
         "ok"
        ],
        [
         "00f27248-1f4f-e174-432c-53bd2a9158df",
         "2024-01-16T18:00:00.000Z",
         0.8542245491303897,
         1.080309777815946,
         1.9618452098136365,
         2.9717426105145472,
         2.306627597988137,
         2.5166973688595817,
         1.980452948870913,
         "Crystal Lake",
         "EpicWind",
         "America/Chicago",
         "ok"
        ],
        [
         "00f27248-1f4f-e174-432c-53bd2a9158df",
         "2024-01-16T19:00:00.000Z",
         0.4915535666395597,
         1.0646332592567707,
         2.2186746553400307,
         3.3459438407963433,
         2.2847856939507167,
         2.5560343320959498,
         1.9519204325253467,
         "Crystal Lake",
         "EpicWind",
         "America/Chicago",
         "ok"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "turbine_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "hourly_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "avg_energy",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_A",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_B",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_C",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_D",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_E",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "std_sensor_F",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "model",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "state",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "abnormal_sensor",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.sql(f\"SELECT * FROM main.e2eai_iot_turbine.turbine_hourly_features LIMIT 10\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7947322644040234,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "05.1-ai-tools",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
