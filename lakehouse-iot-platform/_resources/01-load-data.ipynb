{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f5ef106-8e7f-4f93-b07d-4850f8a88e31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install databricks-sdk==0.40.0 mlflow==2.22.0 numpy==1.26.4 pandas==2.2.3\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45a86a5c-72ec-4aeb-be16-b38d29fb984f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0dd88cb-07ff-4541-99a8-aab16c8ff29e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.dropdown(\"reset_all_data\", \"false\", [\"true\", \"false\"], \"Reset all data\")\n",
    "# dbutils.widgets.text(\"min_dbr_version\", \"12.2\", \"Min required DBR version\")\n",
    "dbutils.widgets.text(\"min_dbr_version\", \"15.4.10\", \"Min required DBR version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27c304db-6edc-4f99-9bbd-fb1ee329a5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import collections\n",
    "import os\n",
    "\n",
    "\n",
    "class DBDemos():\n",
    "  @staticmethod\n",
    "  def setup_schema(catalog, db, reset_all_data, volume_name = None):\n",
    "    if reset_all_data:\n",
    "      print(f'clearing up volume named `{catalog}`.`{db}`.`{volume_name}`')\n",
    "      try:\n",
    "        spark.sql(f\"DROP VOLUME IF EXISTS `{catalog}`.`{db}`.`{volume_name}`\")\n",
    "        spark.sql(f\"DROP SCHEMA IF EXISTS `{catalog}`.`{db}` CASCADE\")\n",
    "      except Exception as e:\n",
    "        print(f'catalog `{catalog}` or schema `{db}` do not exist.  Skipping data reset')\n",
    "\n",
    "    def use_and_create_db(catalog, dbName, cloud_storage_path = None):\n",
    "      print(f\"USE CATALOG `{catalog}`\")\n",
    "      spark.sql(f\"USE CATALOG `{catalog}`\")\n",
    "      spark.sql(f\"\"\"create database if not exists `{dbName}` \"\"\")\n",
    "\n",
    "    assert catalog not in ['hive_metastore', 'spark_catalog'], \"This demo only support Unity. Please change your catalog name.\"\n",
    "    #If the catalog is defined, we force it to the given value and throw exception if not.\n",
    "    current_catalog = spark.sql(\"select current_catalog()\").collect()[0]['current_catalog()']\n",
    "    if current_catalog != catalog:\n",
    "      catalogs = [r['catalog'] for r in spark.sql(\"SHOW CATALOGS\").collect()]\n",
    "      if catalog not in catalogs:\n",
    "        spark.sql(f\"CREATE CATALOG IF NOT EXISTS `{catalog}`\")\n",
    "        if catalog == 'dbdemos':\n",
    "          spark.sql(f\"ALTER CATALOG `{catalog}` OWNER TO `account users`\")\n",
    "    use_and_create_db(catalog, db)\n",
    "\n",
    "    if catalog == 'dbdemos':\n",
    "      try:\n",
    "        spark.sql(f\"GRANT CREATE, USAGE on DATABASE `{catalog}`.`{db}` TO `account users`\")\n",
    "        spark.sql(f\"ALTER SCHEMA `{catalog}`.`{db}` OWNER TO `account users`\")\n",
    "        for t in spark.sql(f'SHOW TABLES in {catalog}.{db}').collect():\n",
    "          try:\n",
    "            spark.sql(f'GRANT ALL PRIVILEGES ON TABLE {catalog}.{db}.{t[\"tableName\"]} TO `account users`')\n",
    "            spark.sql(f'ALTER TABLE {catalog}.{db}.{t[\"tableName\"]} OWNER TO `account users`')\n",
    "          except Exception as e:\n",
    "            if \"NOT_IMPLEMENTED.TRANSFER_MATERIALIZED_VIEW_OWNERSHIP\" not in str(e) and \"STREAMING_TABLE_OPERATION_NOT_ALLOWED.UNSUPPORTED_OPERATION\" not in str(e) :\n",
    "              print(f'WARN: Couldn t set table {catalog}.{db}.{t[\"tableName\"]} owner to account users, error: {e}')\n",
    "      except Exception as e:\n",
    "        print(\"Couldn't grant access to the schema to all users:\"+str(e))    \n",
    "\n",
    "    print(f\"using catalog.database `{catalog}`.`{db}`\")\n",
    "    spark.sql(f\"\"\"USE `{catalog}`.`{db}`\"\"\")    \n",
    "\n",
    "    if volume_name:\n",
    "      spark.sql(f'CREATE VOLUME IF NOT EXISTS {volume_name};')\n",
    "\n",
    "                     \n",
    "  #Return true if the folder is empty or does not exists\n",
    "  @staticmethod\n",
    "  def is_folder_empty(folder):\n",
    "    try:\n",
    "      return len(dbutils.fs.ls(folder)) == 0\n",
    "    except:\n",
    "      return True\n",
    "    \n",
    "  @staticmethod\n",
    "  def is_any_folder_empty(folders):\n",
    "    return any([DBDemos.is_folder_empty(f) for f in folders])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_model_permission(model_name, permission, principal):\n",
    "    import databricks.sdk.service.catalog as c\n",
    "    sdk_client = databricks.sdk.WorkspaceClient()\n",
    "    return sdk_client.grants.update(c.SecurableType.FUNCTION, model_name, changes=[\n",
    "                              c.PermissionsChange(add=[c.Privilege[permission]], principal=principal)])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_model_endpoint_permission(endpoint_name, permission, group_name):\n",
    "    import databricks.sdk.service.serving as s\n",
    "    sdk_client = databricks.sdk.WorkspaceClient()\n",
    "    ep = sdk_client.serving_endpoints.get(endpoint_name)\n",
    "    return sdk_client.serving_endpoints.set_permissions(serving_endpoint_id=ep.id, access_control_list=[s.ServingEndpointAccessControlRequest(permission_level=s.ServingEndpointPermissionLevel[permission], group_name=group_name)])\n",
    "\n",
    "  @staticmethod\n",
    "  def set_index_permission(index_name, permission, principal):\n",
    "      import databricks.sdk.service.catalog as c\n",
    "      sdk_client = databricks.sdk.WorkspaceClient()\n",
    "      return sdk_client.grants.update(c.SecurableType.TABLE, index_name, changes=[\n",
    "                              c.PermissionsChange(add=[c.Privilege[permission]], principal=principal)])\n",
    "    \n",
    "\n",
    "  @staticmethod\n",
    "  def download_file_from_git(dest, owner, repo, path):\n",
    "    \n",
    "    def download_file(url, destination):\n",
    "      local_filename = url.split('/')[-1]\n",
    "      # NOTE the stream=True parameter below\n",
    "      with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        print('saving '+destination+'/'+local_filename)\n",
    "        with open(destination+'/'+local_filename, 'wb') as f:\n",
    "          for chunk in r.iter_content(chunk_size=8192): \n",
    "            # If you have chunk encoded response uncomment if\n",
    "            # and set chunk_size parameter to None.\n",
    "            #if chunk: \n",
    "            f.write(chunk)\n",
    "      return local_filename\n",
    "\n",
    "    if not os.path.exists(dest):\n",
    "      os.makedirs(dest)\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    files = requests.get(f'https://api.github.com/repos/{owner}/{repo}/contents{path}').json()\n",
    "    files = [f['download_url'] for f in files if 'NOTICE' not in f['name']]\n",
    "    \n",
    "    def download_to_dest(url):\n",
    "      try:\n",
    "        #Temporary fix to avoid hitting github limits - Swap github to our S3 bucket to download files\n",
    "        s3url = url.replace(\"https://raw.githubusercontent.com/databricks-demos/dbdemos-dataset/main/\", \"https://dbdemos-dataset.s3.amazonaws.com/\")\n",
    "        download_file(s3url, dest)\n",
    "      except:\n",
    "        download_file(url, dest)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "      collections.deque(executor.map(download_to_dest, files))\n",
    "         \n",
    "\n",
    "  #force the experiment to the field demos one. Required to launch as a batch\n",
    "  @staticmethod\n",
    "  def init_experiment_for_batch(demo_name, experiment_name):\n",
    "    import mlflow\n",
    "    #You can programatically get a PAT token with the following\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    w = WorkspaceClient()\n",
    "    xp_root_path = f\"/Shared/dbdemos/experiments/{demo_name}\"\n",
    "    try:\n",
    "      r = w.workspace.mkdirs(path=xp_root_path)\n",
    "    except Exception as e:\n",
    "      print(f\"ERROR: couldn't create a folder for the experiment under {xp_root_path} - please create the folder manually or  skip this init (used for job only: {e})\")\n",
    "      raise e\n",
    "    xp = f\"{xp_root_path}/{experiment_name}\"\n",
    "    print(f\"Using common experiment under {xp}\")\n",
    "    mlflow.set_experiment(xp)\n",
    "    DBDemos.set_experiment_permission(xp)\n",
    "    return mlflow.get_experiment_by_name(xp)\n",
    "\n",
    "  @staticmethod\n",
    "  def set_experiment_permission(experiment_path):\n",
    "    from databricks.sdk import WorkspaceClient\n",
    "    from databricks.sdk.service import iam\n",
    "    w = WorkspaceClient()\n",
    "    try:\n",
    "      status = w.workspace.get_status(experiment_path)\n",
    "      w.permissions.set(\"experiments\", request_object_id=status.object_id,  access_control_list=[\n",
    "                            iam.AccessControlRequest(group_name=\"users\", permission_level=iam.PermissionLevel.CAN_MANAGE)])    \n",
    "    except Exception as e:\n",
    "      print(f\"error setting up shared experiment {experiment_path} permission: {e}\")\n",
    "\n",
    "    print(f\"Experiment on {experiment_path} was set public\")\n",
    "\n",
    "\n",
    "  @staticmethod\n",
    "  def get_active_streams(start_with = \"\"):\n",
    "    return [s for s in spark.streams.active if len(start_with) == 0 or (s.name is not None and s.name.startswith(start_with))]\n",
    "\n",
    "  @staticmethod\n",
    "  def stop_all_streams_asynch(start_with = \"\", sleep_time=0):\n",
    "    import threading\n",
    "    def stop_streams():\n",
    "        DBDemos.stop_all_streams(start_with=start_with, sleep_time=sleep_time)\n",
    "\n",
    "    thread = threading.Thread(target=stop_streams)\n",
    "    thread.start()\n",
    "\n",
    "  @staticmethod\n",
    "  def stop_all_streams(start_with = \"\", sleep_time=0):\n",
    "    import time\n",
    "    time.sleep(sleep_time)\n",
    "    streams = DBDemos.get_active_streams(start_with)\n",
    "    if len(streams) > 0:\n",
    "      print(f\"Stopping {len(streams)} streams\")\n",
    "      for s in streams:\n",
    "          try:\n",
    "              s.stop()\n",
    "          except:\n",
    "              pass\n",
    "      print(f\"All stream stopped {'' if len(start_with) == 0 else f'(starting with: {start_with}.)'}\")\n",
    "\n",
    "  @staticmethod\n",
    "  def wait_for_all_stream(start = \"\"):\n",
    "    import time\n",
    "    actives = DBDemos.get_active_streams(start)\n",
    "    if len(actives) > 0:\n",
    "      print(f\"{len(actives)} streams still active, waiting... ({[s.name for s in actives]})\")\n",
    "    while len(actives) > 0:\n",
    "      spark.streams.awaitAnyTermination()\n",
    "      time.sleep(1)\n",
    "      actives = DBDemos.get_active_streams(start)\n",
    "    print(\"All streams completed.\")\n",
    "\n",
    "  @staticmethod\n",
    "  def get_last_experiment(demo_name, experiment_path = \"/Shared/dbdemos/experiments/\"):\n",
    "    import requests\n",
    "    import re\n",
    "    from datetime import datetime\n",
    "    #TODO: waiting for https://github.com/databricks/databricks-sdk-py/issues/509 to use the python sdk instead\n",
    "    base_url =dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "    token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Content-Type\": \"application/json\"}\n",
    "    r = requests.get(base_url+\"/api/2.0/workspace/list\", params={'path': f\"{experiment_path}/{demo_name}\"}, headers=headers).json()\n",
    "    if 'objects' not in r:\n",
    "      raise Exception(f\"No experiment available for this demo. Please re-run the previous notebook with the AutoML run. - {r}\")\n",
    "    xps = [f for f in r['objects'] if f['object_type'] == 'MLFLOW_EXPERIMENT' and 'automl' in f['path']]\n",
    "    xps = [x for x in xps if re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})', x['path'])]\n",
    "    sorted_xp = sorted(xps, key=lambda f: f['path'], reverse = True)\n",
    "    if len(sorted_xp) == 0:\n",
    "      raise Exception(f\"No experiment available for this demo. Please re-run the previous notebook with the AutoML run. - {r}\")\n",
    "\n",
    "    last_xp = sorted_xp[0]\n",
    "\n",
    "    # Search for the date pattern in the input string\n",
    "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}:\\d{2}:\\d{2})', last_xp['path'])\n",
    "\n",
    "    if match:\n",
    "        date_str = match.group(1)  # Extract the matched date string\n",
    "        date = datetime.strptime(date_str, '%Y-%m-%d_%H:%M:%S')  # Convert to a datetime object\n",
    "        # Calculate the difference in days from the current date\n",
    "        days_difference = (datetime.now() - date).days\n",
    "        if days_difference > 30:\n",
    "            raise Exception(f\"It looks like the last experiment {last_xp} is too old ({days_difference} days). Please re-run the previous notebook to make sure you have the latest version. Delete the experiment folder if needed to clear history.\")\n",
    "    else:\n",
    "        raise Exception(f\"Invalid experiment format or no experiment available. Please re-run the previous notebook. {last_xp['path']}\")\n",
    "    return last_xp\n",
    "  \n",
    "\n",
    "  @staticmethod\n",
    "  def wait_for_table(table_name, timeout_duration=120):\n",
    "    import time\n",
    "    i = 0\n",
    "    while not spark.catalog.tableExists(table_name) or spark.table(table_name).count() == 0:\n",
    "      time.sleep(1)\n",
    "      if i > timeout_duration:\n",
    "        raise Exception(f\"couldn't find table {table_name} or table is empty. Do you have data being generated to be consumed?\")\n",
    "      i += 1\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c166122-0418-4092-ac73-eda4c2843818",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reset_all_data = dbutils.widgets.get(\"reset_all_data\") == \"true\"\n",
    "DBDemos.setup_schema(catalog, db, reset_all_data, volume_name)\n",
    "folder = f\"/Volumes/{catalog}/{db}/{volume_name}\"\n",
    "\n",
    "data_exists = False\n",
    "try:\n",
    "  dbutils.fs.ls(folder)\n",
    "  dbutils.fs.ls(folder+\"/historical_turbine_status\")\n",
    "  dbutils.fs.ls(folder+\"/parts\")\n",
    "  dbutils.fs.ls(folder+\"/turbine\")\n",
    "  dbutils.fs.ls(folder+\"/incoming_data\")\n",
    "  dbutils.fs.ls(folder+\"/maintenance_guide\")\n",
    "  data_exists = True\n",
    "  print(\"data already exists\")\n",
    "except Exception as e:\n",
    "  print(f\"folder doesn't exists, generating the data...\")\n",
    "\n",
    "\n",
    "def cleanup_folder(path):\n",
    "  #Cleanup to have something nicer\n",
    "  for f in dbutils.fs.ls(path):\n",
    "    if f.name.startswith('_committed') or f.name.startswith('_started') or f.name.startswith('_SUCCESS') :\n",
    "      dbutils.fs.rm(f.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354d81b8-9e09-4949-b973-6fd9e706683c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data_downloaded = True\n",
    "if not data_exists:\n",
    "    try:\n",
    "        DBDemos.download_file_from_git(folder+'/historical_turbine_status', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/historical_turbine_status\")\n",
    "        DBDemos.download_file_from_git(folder+'/parts', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/parts\")\n",
    "        DBDemos.download_file_from_git(folder+'/turbine', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/turbine\")\n",
    "        DBDemos.download_file_from_git(folder+'/incoming_data', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/incoming_data\")\n",
    "        DBDemos.download_file_from_git(folder+'/maintenance_guide', \"databricks-demos\", \"dbdemos-dataset\", \"/manufacturing/lakehouse-iot-turbine/maintenance_guide\")\n",
    "        spark.sql(\"CREATE TABLE IF NOT EXISTS turbine_power_prediction ( hour INT, min FLOAT, max FLOAT, prediction FLOAT);\")\n",
    "        spark.sql(\"DELETE FROM turbine_power_prediction\")\n",
    "        spark.sql(\"insert into turbine_power_prediction values (0, 377, 397, 391), (1, 393, 423, 412), (2, 399, 455, 426), (3, 391, 445, 404), (4, 345, 394, 365), (5, 235, 340, 276), (6, 144, 275, 195), (7, 93, 175, 133), (8, 45, 105, 76), (9, 55, 125, 95), (10, 35, 99, 77), (11, 14, 79, 44)\")\n",
    "    except Exception as e: \n",
    "        data_downloaded = False\n",
    "        print(f\"Error trying to download the file from the repo: {str(e)}. Will generate the data instead...\")    \n",
    "        raise e"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01-load-data",
   "widgets": {
    "min_dbr_version": {
     "currentValue": "15.4.10",
     "nuid": "db211836-22b9-43aa-9026-a8185f04abf3",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "15.4.10",
      "label": "Min required DBR version",
      "name": "min_dbr_version",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "15.4.10",
      "label": "Min required DBR version",
      "name": "min_dbr_version",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "reset_all_data": {
     "currentValue": "false",
     "nuid": "e7586573-3bea-426e-86e5-ff1e9c9e1569",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "false",
      "label": "Reset all data",
      "name": "reset_all_data",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "true",
        "false"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "false",
      "label": "Reset all data",
      "name": "reset_all_data",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "true",
        "false"
       ]
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
